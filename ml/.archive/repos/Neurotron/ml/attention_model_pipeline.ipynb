{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Model Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from TrainValTensorBoard import TrainValTensorBoard\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal\n",
    "\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['HDF5_USE_FILE_LOCKING'] = 'FALSE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.13.1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Datasets and Clean\n",
    "In this configuration the relevant data set should be loaded from the same folder as the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/nfs/2018/j/jcruz-y-/neurotron_datasets/joined/joined_data_106979_24-Oct-19_17:31_jose_all_1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data consists of timestamps from the two hardware devices and a diff between them. When the two hardware data streams were stitched together an effor was made to minimize this diff, but the driver configuration did not easily permit eliminating it. This information is included to understand the accuracy of the data, but will not be used during the training.\n",
    "\n",
    "The time data is followed by the 8 channels from the Myo, this data will be used as input features.\n",
    "\n",
    "This is followed by the 63 positional points from the Leap cameras. These will be used as labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Leap timestamp</th>\n",
       "      <th>timestamp diff</th>\n",
       "      <th>emg timestamp</th>\n",
       "      <th>ch1</th>\n",
       "      <th>ch2</th>\n",
       "      <th>ch3</th>\n",
       "      <th>ch4</th>\n",
       "      <th>ch5</th>\n",
       "      <th>ch6</th>\n",
       "      <th>ch7</th>\n",
       "      <th>ch8</th>\n",
       "      <th>Wrist x</th>\n",
       "      <th>Wrist y</th>\n",
       "      <th>Wrist z</th>\n",
       "      <th>Thumb Proximal x</th>\n",
       "      <th>Thumb Proximal y</th>\n",
       "      <th>Thumb Proximal z</th>\n",
       "      <th>Thumb Intermediate x</th>\n",
       "      <th>Thumb Intermediate y</th>\n",
       "      <th>Thumb Intermediate z</th>\n",
       "      <th>Thumb Distal x</th>\n",
       "      <th>Thumb Distal y</th>\n",
       "      <th>Thumb Distal z</th>\n",
       "      <th>Thumb Tip x</th>\n",
       "      <th>Thumb Tip y</th>\n",
       "      <th>Thumb Tip z</th>\n",
       "      <th>Index Proximal x</th>\n",
       "      <th>Index Proximal y</th>\n",
       "      <th>Index Proximal z</th>\n",
       "      <th>Index Intermediate x</th>\n",
       "      <th>Index Intermediate y</th>\n",
       "      <th>Index Intermediate z</th>\n",
       "      <th>Index Distal x</th>\n",
       "      <th>Index Distal y</th>\n",
       "      <th>Index Distal z</th>\n",
       "      <th>Index Tip x</th>\n",
       "      <th>Index Tip y</th>\n",
       "      <th>Index Tip z</th>\n",
       "      <th>Middle Proximal x</th>\n",
       "      <th>Middle Proximal y</th>\n",
       "      <th>Middle Proximal z</th>\n",
       "      <th>Middle Intermediate x</th>\n",
       "      <th>Middle Intermediate y</th>\n",
       "      <th>Middle Intermediate z</th>\n",
       "      <th>Middle Distal x</th>\n",
       "      <th>Middle Distal y</th>\n",
       "      <th>Middle Distal z</th>\n",
       "      <th>Middle Tip x</th>\n",
       "      <th>Middle Tip y</th>\n",
       "      <th>Middle Tip z</th>\n",
       "      <th>Ring Proximal x</th>\n",
       "      <th>Ring Proximal y</th>\n",
       "      <th>Ring Proximal z</th>\n",
       "      <th>Ring Intermediate x</th>\n",
       "      <th>Ring Intermediate y</th>\n",
       "      <th>Ring Intermediate z</th>\n",
       "      <th>Ring Distal x</th>\n",
       "      <th>Ring Distal y</th>\n",
       "      <th>Ring Distal z</th>\n",
       "      <th>Ring Tip x</th>\n",
       "      <th>Ring Tip y</th>\n",
       "      <th>Ring Tip z</th>\n",
       "      <th>Pinky Proximal x</th>\n",
       "      <th>Pinky Proximal y</th>\n",
       "      <th>Pinky Proximal z</th>\n",
       "      <th>Pinky Intermediate x</th>\n",
       "      <th>Pinky Intermediate y</th>\n",
       "      <th>Pinky Intermediate z</th>\n",
       "      <th>Pinky Distal x</th>\n",
       "      <th>Pinky Distal y</th>\n",
       "      <th>Pinky Distal z</th>\n",
       "      <th>Pinky Tip x</th>\n",
       "      <th>Pinky Tip y</th>\n",
       "      <th>Pinky Tip z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.571891e+09</td>\n",
       "      <td>0.006226</td>\n",
       "      <td>1.571891e+09</td>\n",
       "      <td>50.0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>396.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>9.262085</td>\n",
       "      <td>-9.118690</td>\n",
       "      <td>47.617905</td>\n",
       "      <td>-18.114399</td>\n",
       "      <td>-8.678528</td>\n",
       "      <td>51.673462</td>\n",
       "      <td>-46.709621</td>\n",
       "      <td>-12.567474</td>\n",
       "      <td>17.094170</td>\n",
       "      <td>-69.877258</td>\n",
       "      <td>-11.163055</td>\n",
       "      <td>-3.096992</td>\n",
       "      <td>-85.138763</td>\n",
       "      <td>-10.990265</td>\n",
       "      <td>-17.690151</td>\n",
       "      <td>-20.603283</td>\n",
       "      <td>15.033867</td>\n",
       "      <td>-19.298096</td>\n",
       "      <td>-18.594982</td>\n",
       "      <td>29.675285</td>\n",
       "      <td>-55.134228</td>\n",
       "      <td>-23.628544</td>\n",
       "      <td>21.952751</td>\n",
       "      <td>-74.898529</td>\n",
       "      <td>-29.280987</td>\n",
       "      <td>9.089638</td>\n",
       "      <td>-81.241966</td>\n",
       "      <td>-1.600807</td>\n",
       "      <td>9.199806</td>\n",
       "      <td>-21.184177</td>\n",
       "      <td>9.602978</td>\n",
       "      <td>22.597313</td>\n",
       "      <td>-61.013245</td>\n",
       "      <td>5.366776</td>\n",
       "      <td>12.613152</td>\n",
       "      <td>-84.265617</td>\n",
       "      <td>-2.429310</td>\n",
       "      <td>-1.073235</td>\n",
       "      <td>-90.542023</td>\n",
       "      <td>16.659401</td>\n",
       "      <td>0.454193</td>\n",
       "      <td>-17.607121</td>\n",
       "      <td>31.992260</td>\n",
       "      <td>15.967636</td>\n",
       "      <td>-51.509766</td>\n",
       "      <td>30.006121</td>\n",
       "      <td>9.053932</td>\n",
       "      <td>-75.447060</td>\n",
       "      <td>22.099865</td>\n",
       "      <td>-3.387665</td>\n",
       "      <td>-83.625824</td>\n",
       "      <td>31.320591</td>\n",
       "      <td>-11.242180</td>\n",
       "      <td>-12.971455</td>\n",
       "      <td>48.320904</td>\n",
       "      <td>0.762253</td>\n",
       "      <td>-37.152554</td>\n",
       "      <td>49.054413</td>\n",
       "      <td>-1.940414</td>\n",
       "      <td>-54.576313</td>\n",
       "      <td>42.004250</td>\n",
       "      <td>-10.960121</td>\n",
       "      <td>-65.103134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.571891e+09</td>\n",
       "      <td>-0.029152</td>\n",
       "      <td>1.571891e+09</td>\n",
       "      <td>50.0</td>\n",
       "      <td>286.0</td>\n",
       "      <td>368.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>6.786233</td>\n",
       "      <td>-11.333852</td>\n",
       "      <td>47.534789</td>\n",
       "      <td>-20.769446</td>\n",
       "      <td>-11.812599</td>\n",
       "      <td>49.990927</td>\n",
       "      <td>-48.818829</td>\n",
       "      <td>-12.573416</td>\n",
       "      <td>14.808353</td>\n",
       "      <td>-71.526649</td>\n",
       "      <td>-9.472275</td>\n",
       "      <td>-5.685904</td>\n",
       "      <td>-86.040143</td>\n",
       "      <td>-8.497499</td>\n",
       "      <td>-20.978106</td>\n",
       "      <td>-19.892583</td>\n",
       "      <td>15.473871</td>\n",
       "      <td>-19.684725</td>\n",
       "      <td>-16.363919</td>\n",
       "      <td>31.977931</td>\n",
       "      <td>-54.562784</td>\n",
       "      <td>-20.061439</td>\n",
       "      <td>25.205292</td>\n",
       "      <td>-74.945980</td>\n",
       "      <td>-24.964155</td>\n",
       "      <td>12.585976</td>\n",
       "      <td>-82.297033</td>\n",
       "      <td>-0.656441</td>\n",
       "      <td>10.240284</td>\n",
       "      <td>-20.741277</td>\n",
       "      <td>12.169991</td>\n",
       "      <td>25.430847</td>\n",
       "      <td>-59.404511</td>\n",
       "      <td>9.774177</td>\n",
       "      <td>16.861857</td>\n",
       "      <td>-83.445555</td>\n",
       "      <td>3.000662</td>\n",
       "      <td>3.609998</td>\n",
       "      <td>-91.510464</td>\n",
       "      <td>17.612489</td>\n",
       "      <td>1.798022</td>\n",
       "      <td>-16.543760</td>\n",
       "      <td>34.630843</td>\n",
       "      <td>19.197273</td>\n",
       "      <td>-48.656146</td>\n",
       "      <td>34.708936</td>\n",
       "      <td>13.781877</td>\n",
       "      <td>-73.032392</td>\n",
       "      <td>28.021311</td>\n",
       "      <td>1.851053</td>\n",
       "      <td>-82.843496</td>\n",
       "      <td>32.317179</td>\n",
       "      <td>-9.737174</td>\n",
       "      <td>-11.650706</td>\n",
       "      <td>50.569850</td>\n",
       "      <td>3.507205</td>\n",
       "      <td>-34.192403</td>\n",
       "      <td>52.642263</td>\n",
       "      <td>1.569894</td>\n",
       "      <td>-51.594506</td>\n",
       "      <td>46.731619</td>\n",
       "      <td>-7.074512</td>\n",
       "      <td>-63.067226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.571891e+09</td>\n",
       "      <td>0.001225</td>\n",
       "      <td>1.571891e+09</td>\n",
       "      <td>48.0</td>\n",
       "      <td>270.0</td>\n",
       "      <td>347.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>5.317806</td>\n",
       "      <td>-12.499672</td>\n",
       "      <td>47.453987</td>\n",
       "      <td>-22.304150</td>\n",
       "      <td>-13.473961</td>\n",
       "      <td>48.934364</td>\n",
       "      <td>-50.567093</td>\n",
       "      <td>-11.620216</td>\n",
       "      <td>13.915695</td>\n",
       "      <td>-73.125786</td>\n",
       "      <td>-7.157860</td>\n",
       "      <td>-6.519272</td>\n",
       "      <td>-87.022034</td>\n",
       "      <td>-5.696548</td>\n",
       "      <td>-22.351597</td>\n",
       "      <td>-19.449600</td>\n",
       "      <td>15.688126</td>\n",
       "      <td>-19.963085</td>\n",
       "      <td>-15.181034</td>\n",
       "      <td>33.082428</td>\n",
       "      <td>-54.341133</td>\n",
       "      <td>-18.064960</td>\n",
       "      <td>27.075287</td>\n",
       "      <td>-75.106461</td>\n",
       "      <td>-22.477337</td>\n",
       "      <td>14.810486</td>\n",
       "      <td>-83.337776</td>\n",
       "      <td>-0.101135</td>\n",
       "      <td>10.783897</td>\n",
       "      <td>-20.485744</td>\n",
       "      <td>13.475960</td>\n",
       "      <td>26.486450</td>\n",
       "      <td>-58.701767</td>\n",
       "      <td>12.304794</td>\n",
       "      <td>18.931656</td>\n",
       "      <td>-83.193710</td>\n",
       "      <td>6.364304</td>\n",
       "      <td>6.209526</td>\n",
       "      <td>-92.698830</td>\n",
       "      <td>18.149715</td>\n",
       "      <td>2.515739</td>\n",
       "      <td>-15.874870</td>\n",
       "      <td>36.093548</td>\n",
       "      <td>20.546936</td>\n",
       "      <td>-47.148628</td>\n",
       "      <td>37.534588</td>\n",
       "      <td>16.020477</td>\n",
       "      <td>-71.687836</td>\n",
       "      <td>31.792110</td>\n",
       "      <td>4.564583</td>\n",
       "      <td>-82.641403</td>\n",
       "      <td>32.863037</td>\n",
       "      <td>-8.918983</td>\n",
       "      <td>-10.774841</td>\n",
       "      <td>52.029774</td>\n",
       "      <td>4.644127</td>\n",
       "      <td>-32.373928</td>\n",
       "      <td>55.188416</td>\n",
       "      <td>3.101257</td>\n",
       "      <td>-49.667633</td>\n",
       "      <td>50.315052</td>\n",
       "      <td>-5.215370</td>\n",
       "      <td>-61.872467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.571891e+09</td>\n",
       "      <td>0.009088</td>\n",
       "      <td>1.571891e+09</td>\n",
       "      <td>49.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>326.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>5.141285</td>\n",
       "      <td>-12.493469</td>\n",
       "      <td>47.475067</td>\n",
       "      <td>-22.483795</td>\n",
       "      <td>-13.559532</td>\n",
       "      <td>48.828407</td>\n",
       "      <td>-50.828781</td>\n",
       "      <td>-11.390457</td>\n",
       "      <td>13.894257</td>\n",
       "      <td>-73.361900</td>\n",
       "      <td>-6.804001</td>\n",
       "      <td>-6.541451</td>\n",
       "      <td>-87.106232</td>\n",
       "      <td>-5.372879</td>\n",
       "      <td>-22.508564</td>\n",
       "      <td>-19.409599</td>\n",
       "      <td>15.628494</td>\n",
       "      <td>-20.048607</td>\n",
       "      <td>-15.110043</td>\n",
       "      <td>33.002945</td>\n",
       "      <td>-54.432831</td>\n",
       "      <td>-17.841034</td>\n",
       "      <td>27.203735</td>\n",
       "      <td>-75.277794</td>\n",
       "      <td>-22.137730</td>\n",
       "      <td>15.102798</td>\n",
       "      <td>-83.807457</td>\n",
       "      <td>-0.042805</td>\n",
       "      <td>10.788460</td>\n",
       "      <td>-20.483543</td>\n",
       "      <td>13.552137</td>\n",
       "      <td>26.346481</td>\n",
       "      <td>-58.752304</td>\n",
       "      <td>12.602261</td>\n",
       "      <td>18.943726</td>\n",
       "      <td>-83.300186</td>\n",
       "      <td>6.865059</td>\n",
       "      <td>6.356377</td>\n",
       "      <td>-93.104897</td>\n",
       "      <td>18.213963</td>\n",
       "      <td>2.579636</td>\n",
       "      <td>-15.790806</td>\n",
       "      <td>36.240501</td>\n",
       "      <td>20.540024</td>\n",
       "      <td>-47.057739</td>\n",
       "      <td>37.922184</td>\n",
       "      <td>16.111511</td>\n",
       "      <td>-71.599510</td>\n",
       "      <td>32.396820</td>\n",
       "      <td>4.750542</td>\n",
       "      <td>-82.761627</td>\n",
       "      <td>32.941494</td>\n",
       "      <td>-8.807564</td>\n",
       "      <td>-10.625877</td>\n",
       "      <td>52.218910</td>\n",
       "      <td>4.702339</td>\n",
       "      <td>-32.159714</td>\n",
       "      <td>55.573746</td>\n",
       "      <td>3.183128</td>\n",
       "      <td>-49.418533</td>\n",
       "      <td>50.933563</td>\n",
       "      <td>-5.084938</td>\n",
       "      <td>-61.746620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.571891e+09</td>\n",
       "      <td>-0.028308</td>\n",
       "      <td>1.571891e+09</td>\n",
       "      <td>43.0</td>\n",
       "      <td>247.0</td>\n",
       "      <td>325.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>4.931822</td>\n",
       "      <td>-12.186270</td>\n",
       "      <td>47.576642</td>\n",
       "      <td>-22.675898</td>\n",
       "      <td>-13.818677</td>\n",
       "      <td>48.666467</td>\n",
       "      <td>-51.227582</td>\n",
       "      <td>-11.307386</td>\n",
       "      <td>13.924809</td>\n",
       "      <td>-73.618568</td>\n",
       "      <td>-6.853106</td>\n",
       "      <td>-6.694705</td>\n",
       "      <td>-86.806923</td>\n",
       "      <td>-5.842213</td>\n",
       "      <td>-23.149838</td>\n",
       "      <td>-19.529686</td>\n",
       "      <td>15.138880</td>\n",
       "      <td>-20.304312</td>\n",
       "      <td>-15.416904</td>\n",
       "      <td>32.233162</td>\n",
       "      <td>-54.850683</td>\n",
       "      <td>-17.606092</td>\n",
       "      <td>27.212273</td>\n",
       "      <td>-75.955087</td>\n",
       "      <td>-21.369139</td>\n",
       "      <td>15.825144</td>\n",
       "      <td>-85.617107</td>\n",
       "      <td>-0.064653</td>\n",
       "      <td>10.699529</td>\n",
       "      <td>-20.529989</td>\n",
       "      <td>13.204302</td>\n",
       "      <td>25.702415</td>\n",
       "      <td>-59.131582</td>\n",
       "      <td>12.917689</td>\n",
       "      <td>18.736651</td>\n",
       "      <td>-83.820910</td>\n",
       "      <td>7.957404</td>\n",
       "      <td>6.580798</td>\n",
       "      <td>-94.533728</td>\n",
       "      <td>18.309981</td>\n",
       "      <td>2.890611</td>\n",
       "      <td>-15.624303</td>\n",
       "      <td>36.167134</td>\n",
       "      <td>20.690137</td>\n",
       "      <td>-47.079395</td>\n",
       "      <td>38.460759</td>\n",
       "      <td>16.520139</td>\n",
       "      <td>-71.615142</td>\n",
       "      <td>33.674308</td>\n",
       "      <td>5.398041</td>\n",
       "      <td>-83.336438</td>\n",
       "      <td>33.217521</td>\n",
       "      <td>-8.166978</td>\n",
       "      <td>-10.266741</td>\n",
       "      <td>52.535532</td>\n",
       "      <td>5.324988</td>\n",
       "      <td>-31.775413</td>\n",
       "      <td>56.404622</td>\n",
       "      <td>3.893927</td>\n",
       "      <td>-48.932433</td>\n",
       "      <td>52.530867</td>\n",
       "      <td>-4.270894</td>\n",
       "      <td>-61.584274</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Leap timestamp  timestamp diff  emg timestamp   ch1    ch2    ch3   ch4  \\\n",
       "0    1.571891e+09        0.006226   1.571891e+09  50.0  250.0  396.0  89.0   \n",
       "1    1.571891e+09       -0.029152   1.571891e+09  50.0  286.0  368.0  74.0   \n",
       "2    1.571891e+09        0.001225   1.571891e+09  48.0  270.0  347.0  69.0   \n",
       "3    1.571891e+09        0.009088   1.571891e+09  49.0  245.0  326.0  66.0   \n",
       "4    1.571891e+09       -0.028308   1.571891e+09  43.0  247.0  325.0  49.0   \n",
       "\n",
       "    ch5   ch6   ch7   ch8   Wrist x    Wrist y    Wrist z  Thumb Proximal x  \\\n",
       "0  53.0  43.0  31.0  61.0  9.262085  -9.118690  47.617905        -18.114399   \n",
       "1  51.0  29.0  23.0  42.0  6.786233 -11.333852  47.534789        -20.769446   \n",
       "2  50.0  29.0  22.0  37.0  5.317806 -12.499672  47.453987        -22.304150   \n",
       "3  39.0  28.0  23.0  36.0  5.141285 -12.493469  47.475067        -22.483795   \n",
       "4  33.0  25.0  20.0  34.0  4.931822 -12.186270  47.576642        -22.675898   \n",
       "\n",
       "   Thumb Proximal y  Thumb Proximal z  Thumb Intermediate x  \\\n",
       "0         -8.678528         51.673462            -46.709621   \n",
       "1        -11.812599         49.990927            -48.818829   \n",
       "2        -13.473961         48.934364            -50.567093   \n",
       "3        -13.559532         48.828407            -50.828781   \n",
       "4        -13.818677         48.666467            -51.227582   \n",
       "\n",
       "   Thumb Intermediate y  Thumb Intermediate z  Thumb Distal x  Thumb Distal y  \\\n",
       "0            -12.567474             17.094170      -69.877258      -11.163055   \n",
       "1            -12.573416             14.808353      -71.526649       -9.472275   \n",
       "2            -11.620216             13.915695      -73.125786       -7.157860   \n",
       "3            -11.390457             13.894257      -73.361900       -6.804001   \n",
       "4            -11.307386             13.924809      -73.618568       -6.853106   \n",
       "\n",
       "   Thumb Distal z  Thumb Tip x  Thumb Tip y  Thumb Tip z  Index Proximal x  \\\n",
       "0       -3.096992   -85.138763   -10.990265   -17.690151        -20.603283   \n",
       "1       -5.685904   -86.040143    -8.497499   -20.978106        -19.892583   \n",
       "2       -6.519272   -87.022034    -5.696548   -22.351597        -19.449600   \n",
       "3       -6.541451   -87.106232    -5.372879   -22.508564        -19.409599   \n",
       "4       -6.694705   -86.806923    -5.842213   -23.149838        -19.529686   \n",
       "\n",
       "   Index Proximal y  Index Proximal z  Index Intermediate x  \\\n",
       "0         15.033867        -19.298096            -18.594982   \n",
       "1         15.473871        -19.684725            -16.363919   \n",
       "2         15.688126        -19.963085            -15.181034   \n",
       "3         15.628494        -20.048607            -15.110043   \n",
       "4         15.138880        -20.304312            -15.416904   \n",
       "\n",
       "   Index Intermediate y  Index Intermediate z  Index Distal x  Index Distal y  \\\n",
       "0             29.675285            -55.134228      -23.628544       21.952751   \n",
       "1             31.977931            -54.562784      -20.061439       25.205292   \n",
       "2             33.082428            -54.341133      -18.064960       27.075287   \n",
       "3             33.002945            -54.432831      -17.841034       27.203735   \n",
       "4             32.233162            -54.850683      -17.606092       27.212273   \n",
       "\n",
       "   Index Distal z  Index Tip x  Index Tip y  Index Tip z  Middle Proximal x  \\\n",
       "0      -74.898529   -29.280987     9.089638   -81.241966          -1.600807   \n",
       "1      -74.945980   -24.964155    12.585976   -82.297033          -0.656441   \n",
       "2      -75.106461   -22.477337    14.810486   -83.337776          -0.101135   \n",
       "3      -75.277794   -22.137730    15.102798   -83.807457          -0.042805   \n",
       "4      -75.955087   -21.369139    15.825144   -85.617107          -0.064653   \n",
       "\n",
       "   Middle Proximal y  Middle Proximal z  Middle Intermediate x  \\\n",
       "0           9.199806         -21.184177               9.602978   \n",
       "1          10.240284         -20.741277              12.169991   \n",
       "2          10.783897         -20.485744              13.475960   \n",
       "3          10.788460         -20.483543              13.552137   \n",
       "4          10.699529         -20.529989              13.204302   \n",
       "\n",
       "   Middle Intermediate y  Middle Intermediate z  Middle Distal x  \\\n",
       "0              22.597313             -61.013245         5.366776   \n",
       "1              25.430847             -59.404511         9.774177   \n",
       "2              26.486450             -58.701767        12.304794   \n",
       "3              26.346481             -58.752304        12.602261   \n",
       "4              25.702415             -59.131582        12.917689   \n",
       "\n",
       "   Middle Distal y  Middle Distal z  Middle Tip x  Middle Tip y  Middle Tip z  \\\n",
       "0        12.613152       -84.265617     -2.429310     -1.073235    -90.542023   \n",
       "1        16.861857       -83.445555      3.000662      3.609998    -91.510464   \n",
       "2        18.931656       -83.193710      6.364304      6.209526    -92.698830   \n",
       "3        18.943726       -83.300186      6.865059      6.356377    -93.104897   \n",
       "4        18.736651       -83.820910      7.957404      6.580798    -94.533728   \n",
       "\n",
       "   Ring Proximal x  Ring Proximal y  Ring Proximal z  Ring Intermediate x  \\\n",
       "0        16.659401         0.454193       -17.607121            31.992260   \n",
       "1        17.612489         1.798022       -16.543760            34.630843   \n",
       "2        18.149715         2.515739       -15.874870            36.093548   \n",
       "3        18.213963         2.579636       -15.790806            36.240501   \n",
       "4        18.309981         2.890611       -15.624303            36.167134   \n",
       "\n",
       "   Ring Intermediate y  Ring Intermediate z  Ring Distal x  Ring Distal y  \\\n",
       "0            15.967636           -51.509766      30.006121       9.053932   \n",
       "1            19.197273           -48.656146      34.708936      13.781877   \n",
       "2            20.546936           -47.148628      37.534588      16.020477   \n",
       "3            20.540024           -47.057739      37.922184      16.111511   \n",
       "4            20.690137           -47.079395      38.460759      16.520139   \n",
       "\n",
       "   Ring Distal z  Ring Tip x  Ring Tip y  Ring Tip z  Pinky Proximal x  \\\n",
       "0     -75.447060   22.099865   -3.387665  -83.625824         31.320591   \n",
       "1     -73.032392   28.021311    1.851053  -82.843496         32.317179   \n",
       "2     -71.687836   31.792110    4.564583  -82.641403         32.863037   \n",
       "3     -71.599510   32.396820    4.750542  -82.761627         32.941494   \n",
       "4     -71.615142   33.674308    5.398041  -83.336438         33.217521   \n",
       "\n",
       "   Pinky Proximal y  Pinky Proximal z  Pinky Intermediate x  \\\n",
       "0        -11.242180        -12.971455             48.320904   \n",
       "1         -9.737174        -11.650706             50.569850   \n",
       "2         -8.918983        -10.774841             52.029774   \n",
       "3         -8.807564        -10.625877             52.218910   \n",
       "4         -8.166978        -10.266741             52.535532   \n",
       "\n",
       "   Pinky Intermediate y  Pinky Intermediate z  Pinky Distal x  Pinky Distal y  \\\n",
       "0              0.762253            -37.152554       49.054413       -1.940414   \n",
       "1              3.507205            -34.192403       52.642263        1.569894   \n",
       "2              4.644127            -32.373928       55.188416        3.101257   \n",
       "3              4.702339            -32.159714       55.573746        3.183128   \n",
       "4              5.324988            -31.775413       56.404622        3.893927   \n",
       "\n",
       "   Pinky Distal z  Pinky Tip x  Pinky Tip y  Pinky Tip z  \n",
       "0      -54.576313    42.004250   -10.960121   -65.103134  \n",
       "1      -51.594506    46.731619    -7.074512   -63.067226  \n",
       "2      -49.667633    50.315052    -5.215370   -61.872467  \n",
       "3      -49.418533    50.933563    -5.084938   -61.746620  \n",
       "4      -48.932433    52.530867    -4.270894   -61.584274  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(labels=[\"Leap timestamp\", \"timestamp diff\", \"emg timestamp\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ch1</th>\n",
       "      <th>ch2</th>\n",
       "      <th>ch3</th>\n",
       "      <th>ch4</th>\n",
       "      <th>ch5</th>\n",
       "      <th>ch6</th>\n",
       "      <th>ch7</th>\n",
       "      <th>ch8</th>\n",
       "      <th>Wrist x</th>\n",
       "      <th>Wrist y</th>\n",
       "      <th>Wrist z</th>\n",
       "      <th>Thumb Proximal x</th>\n",
       "      <th>Thumb Proximal y</th>\n",
       "      <th>Thumb Proximal z</th>\n",
       "      <th>Thumb Intermediate x</th>\n",
       "      <th>Thumb Intermediate y</th>\n",
       "      <th>Thumb Intermediate z</th>\n",
       "      <th>Thumb Distal x</th>\n",
       "      <th>Thumb Distal y</th>\n",
       "      <th>Thumb Distal z</th>\n",
       "      <th>Thumb Tip x</th>\n",
       "      <th>Thumb Tip y</th>\n",
       "      <th>Thumb Tip z</th>\n",
       "      <th>Index Proximal x</th>\n",
       "      <th>Index Proximal y</th>\n",
       "      <th>Index Proximal z</th>\n",
       "      <th>Index Intermediate x</th>\n",
       "      <th>Index Intermediate y</th>\n",
       "      <th>Index Intermediate z</th>\n",
       "      <th>Index Distal x</th>\n",
       "      <th>Index Distal y</th>\n",
       "      <th>Index Distal z</th>\n",
       "      <th>Index Tip x</th>\n",
       "      <th>Index Tip y</th>\n",
       "      <th>Index Tip z</th>\n",
       "      <th>Middle Proximal x</th>\n",
       "      <th>Middle Proximal y</th>\n",
       "      <th>Middle Proximal z</th>\n",
       "      <th>Middle Intermediate x</th>\n",
       "      <th>Middle Intermediate y</th>\n",
       "      <th>Middle Intermediate z</th>\n",
       "      <th>Middle Distal x</th>\n",
       "      <th>Middle Distal y</th>\n",
       "      <th>Middle Distal z</th>\n",
       "      <th>Middle Tip x</th>\n",
       "      <th>Middle Tip y</th>\n",
       "      <th>Middle Tip z</th>\n",
       "      <th>Ring Proximal x</th>\n",
       "      <th>Ring Proximal y</th>\n",
       "      <th>Ring Proximal z</th>\n",
       "      <th>Ring Intermediate x</th>\n",
       "      <th>Ring Intermediate y</th>\n",
       "      <th>Ring Intermediate z</th>\n",
       "      <th>Ring Distal x</th>\n",
       "      <th>Ring Distal y</th>\n",
       "      <th>Ring Distal z</th>\n",
       "      <th>Ring Tip x</th>\n",
       "      <th>Ring Tip y</th>\n",
       "      <th>Ring Tip z</th>\n",
       "      <th>Pinky Proximal x</th>\n",
       "      <th>Pinky Proximal y</th>\n",
       "      <th>Pinky Proximal z</th>\n",
       "      <th>Pinky Intermediate x</th>\n",
       "      <th>Pinky Intermediate y</th>\n",
       "      <th>Pinky Intermediate z</th>\n",
       "      <th>Pinky Distal x</th>\n",
       "      <th>Pinky Distal y</th>\n",
       "      <th>Pinky Distal z</th>\n",
       "      <th>Pinky Tip x</th>\n",
       "      <th>Pinky Tip y</th>\n",
       "      <th>Pinky Tip z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "      <td>106979.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>107.959076</td>\n",
       "      <td>277.776554</td>\n",
       "      <td>293.447060</td>\n",
       "      <td>112.697202</td>\n",
       "      <td>55.729031</td>\n",
       "      <td>72.685406</td>\n",
       "      <td>82.720188</td>\n",
       "      <td>67.966208</td>\n",
       "      <td>-1.519532</td>\n",
       "      <td>-9.021946</td>\n",
       "      <td>44.597009</td>\n",
       "      <td>-24.352507</td>\n",
       "      <td>-18.957473</td>\n",
       "      <td>39.809937</td>\n",
       "      <td>-35.202575</td>\n",
       "      <td>-26.591242</td>\n",
       "      <td>2.103636</td>\n",
       "      <td>-40.789788</td>\n",
       "      <td>-32.012339</td>\n",
       "      <td>-22.847791</td>\n",
       "      <td>-44.154327</td>\n",
       "      <td>-35.382372</td>\n",
       "      <td>-37.740631</td>\n",
       "      <td>-16.995893</td>\n",
       "      <td>7.373415</td>\n",
       "      <td>-23.683015</td>\n",
       "      <td>-14.404202</td>\n",
       "      <td>3.981787</td>\n",
       "      <td>-54.751712</td>\n",
       "      <td>-13.055133</td>\n",
       "      <td>-3.578775</td>\n",
       "      <td>-67.611467</td>\n",
       "      <td>-12.313513</td>\n",
       "      <td>-10.208400</td>\n",
       "      <td>-73.972310</td>\n",
       "      <td>0.712549</td>\n",
       "      <td>9.388218</td>\n",
       "      <td>-19.306369</td>\n",
       "      <td>9.581468</td>\n",
       "      <td>4.679483</td>\n",
       "      <td>-53.065222</td>\n",
       "      <td>12.692946</td>\n",
       "      <td>-4.684326</td>\n",
       "      <td>-67.946889</td>\n",
       "      <td>13.756485</td>\n",
       "      <td>-12.317275</td>\n",
       "      <td>-74.908446</td>\n",
       "      <td>17.529791</td>\n",
       "      <td>8.095233</td>\n",
       "      <td>-10.276684</td>\n",
       "      <td>25.648988</td>\n",
       "      <td>-0.778659</td>\n",
       "      <td>-40.999281</td>\n",
       "      <td>27.275005</td>\n",
       "      <td>-12.593958</td>\n",
       "      <td>-54.669632</td>\n",
       "      <td>27.006531</td>\n",
       "      <td>-21.898478</td>\n",
       "      <td>-60.856092</td>\n",
       "      <td>32.352225</td>\n",
       "      <td>2.837640</td>\n",
       "      <td>-1.285509</td>\n",
       "      <td>41.970822</td>\n",
       "      <td>-4.485664</td>\n",
       "      <td>-24.370822</td>\n",
       "      <td>43.496761</td>\n",
       "      <td>-12.840841</td>\n",
       "      <td>-34.176921</td>\n",
       "      <td>42.845118</td>\n",
       "      <td>-21.385414</td>\n",
       "      <td>-40.369203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>94.478903</td>\n",
       "      <td>183.819378</td>\n",
       "      <td>181.553457</td>\n",
       "      <td>80.348375</td>\n",
       "      <td>68.751875</td>\n",
       "      <td>110.614351</td>\n",
       "      <td>137.870160</td>\n",
       "      <td>103.867191</td>\n",
       "      <td>18.203014</td>\n",
       "      <td>13.902148</td>\n",
       "      <td>5.902738</td>\n",
       "      <td>17.833762</td>\n",
       "      <td>17.313549</td>\n",
       "      <td>12.854626</td>\n",
       "      <td>10.539880</td>\n",
       "      <td>13.674356</td>\n",
       "      <td>17.937670</td>\n",
       "      <td>20.180079</td>\n",
       "      <td>18.120903</td>\n",
       "      <td>22.667319</td>\n",
       "      <td>30.208039</td>\n",
       "      <td>23.253477</td>\n",
       "      <td>26.870641</td>\n",
       "      <td>9.447936</td>\n",
       "      <td>7.232516</td>\n",
       "      <td>7.711189</td>\n",
       "      <td>22.331166</td>\n",
       "      <td>23.019437</td>\n",
       "      <td>11.387768</td>\n",
       "      <td>28.566881</td>\n",
       "      <td>33.673159</td>\n",
       "      <td>18.030049</td>\n",
       "      <td>32.359781</td>\n",
       "      <td>40.269171</td>\n",
       "      <td>25.337680</td>\n",
       "      <td>8.255159</td>\n",
       "      <td>6.050276</td>\n",
       "      <td>3.587558</td>\n",
       "      <td>22.631948</td>\n",
       "      <td>25.144259</td>\n",
       "      <td>10.117039</td>\n",
       "      <td>30.242326</td>\n",
       "      <td>37.583324</td>\n",
       "      <td>18.901129</td>\n",
       "      <td>34.760672</td>\n",
       "      <td>44.641463</td>\n",
       "      <td>27.063839</td>\n",
       "      <td>5.532313</td>\n",
       "      <td>7.853489</td>\n",
       "      <td>8.013560</td>\n",
       "      <td>18.961775</td>\n",
       "      <td>22.917322</td>\n",
       "      <td>13.537246</td>\n",
       "      <td>27.110144</td>\n",
       "      <td>33.080043</td>\n",
       "      <td>21.107323</td>\n",
       "      <td>32.277698</td>\n",
       "      <td>38.685426</td>\n",
       "      <td>28.408468</td>\n",
       "      <td>4.083498</td>\n",
       "      <td>11.077100</td>\n",
       "      <td>13.504016</td>\n",
       "      <td>14.765910</td>\n",
       "      <td>20.311902</td>\n",
       "      <td>18.142483</td>\n",
       "      <td>21.859076</td>\n",
       "      <td>26.318486</td>\n",
       "      <td>21.132027</td>\n",
       "      <td>28.005897</td>\n",
       "      <td>30.720860</td>\n",
       "      <td>25.151496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>18.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>-49.460617</td>\n",
       "      <td>-51.661484</td>\n",
       "      <td>-8.476379</td>\n",
       "      <td>-59.220070</td>\n",
       "      <td>-58.296936</td>\n",
       "      <td>-27.315063</td>\n",
       "      <td>-70.195053</td>\n",
       "      <td>-71.954811</td>\n",
       "      <td>-49.751572</td>\n",
       "      <td>-92.561234</td>\n",
       "      <td>-90.297638</td>\n",
       "      <td>-74.886850</td>\n",
       "      <td>-112.754237</td>\n",
       "      <td>-103.504150</td>\n",
       "      <td>-94.854360</td>\n",
       "      <td>-35.338264</td>\n",
       "      <td>-31.792175</td>\n",
       "      <td>-35.444645</td>\n",
       "      <td>-69.852450</td>\n",
       "      <td>-67.396919</td>\n",
       "      <td>-74.289215</td>\n",
       "      <td>-85.011714</td>\n",
       "      <td>-87.236955</td>\n",
       "      <td>-96.546054</td>\n",
       "      <td>-97.884471</td>\n",
       "      <td>-102.978952</td>\n",
       "      <td>-112.462585</td>\n",
       "      <td>-24.145329</td>\n",
       "      <td>-21.980112</td>\n",
       "      <td>-25.657837</td>\n",
       "      <td>-54.249256</td>\n",
       "      <td>-65.532608</td>\n",
       "      <td>-72.724304</td>\n",
       "      <td>-75.109543</td>\n",
       "      <td>-91.424866</td>\n",
       "      <td>-100.599316</td>\n",
       "      <td>-91.211243</td>\n",
       "      <td>-109.174942</td>\n",
       "      <td>-118.985252</td>\n",
       "      <td>-12.639398</td>\n",
       "      <td>-25.039429</td>\n",
       "      <td>-26.869192</td>\n",
       "      <td>-40.555109</td>\n",
       "      <td>-60.490051</td>\n",
       "      <td>-66.913315</td>\n",
       "      <td>-59.266098</td>\n",
       "      <td>-81.102333</td>\n",
       "      <td>-93.231049</td>\n",
       "      <td>-75.195114</td>\n",
       "      <td>-92.351674</td>\n",
       "      <td>-111.118563</td>\n",
       "      <td>-30.062690</td>\n",
       "      <td>-36.622925</td>\n",
       "      <td>-36.324131</td>\n",
       "      <td>-37.905338</td>\n",
       "      <td>-56.838379</td>\n",
       "      <td>-66.809570</td>\n",
       "      <td>-43.796612</td>\n",
       "      <td>-71.155746</td>\n",
       "      <td>-80.829399</td>\n",
       "      <td>-53.352108</td>\n",
       "      <td>-81.252701</td>\n",
       "      <td>-96.391819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>61.000000</td>\n",
       "      <td>143.000000</td>\n",
       "      <td>152.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>-16.886156</td>\n",
       "      <td>-17.755783</td>\n",
       "      <td>42.359551</td>\n",
       "      <td>-39.496848</td>\n",
       "      <td>-31.326163</td>\n",
       "      <td>31.505045</td>\n",
       "      <td>-42.614314</td>\n",
       "      <td>-35.798828</td>\n",
       "      <td>-11.512680</td>\n",
       "      <td>-56.313196</td>\n",
       "      <td>-44.501390</td>\n",
       "      <td>-40.588364</td>\n",
       "      <td>-68.302795</td>\n",
       "      <td>-52.288237</td>\n",
       "      <td>-58.127181</td>\n",
       "      <td>-26.110477</td>\n",
       "      <td>2.451675</td>\n",
       "      <td>-30.127332</td>\n",
       "      <td>-35.800728</td>\n",
       "      <td>-14.385666</td>\n",
       "      <td>-63.962860</td>\n",
       "      <td>-39.267851</td>\n",
       "      <td>-32.875201</td>\n",
       "      <td>-81.408615</td>\n",
       "      <td>-41.107635</td>\n",
       "      <td>-45.302944</td>\n",
       "      <td>-93.672626</td>\n",
       "      <td>-6.837919</td>\n",
       "      <td>5.863414</td>\n",
       "      <td>-21.728305</td>\n",
       "      <td>-11.149257</td>\n",
       "      <td>-16.099387</td>\n",
       "      <td>-60.206032</td>\n",
       "      <td>-13.197766</td>\n",
       "      <td>-38.103338</td>\n",
       "      <td>-81.200862</td>\n",
       "      <td>-14.475380</td>\n",
       "      <td>-51.520982</td>\n",
       "      <td>-94.599333</td>\n",
       "      <td>13.344295</td>\n",
       "      <td>2.828438</td>\n",
       "      <td>-17.730928</td>\n",
       "      <td>10.075094</td>\n",
       "      <td>-19.526822</td>\n",
       "      <td>-51.560223</td>\n",
       "      <td>6.909097</td>\n",
       "      <td>-40.784522</td>\n",
       "      <td>-69.958648</td>\n",
       "      <td>3.556110</td>\n",
       "      <td>-54.368904</td>\n",
       "      <td>-81.196796</td>\n",
       "      <td>30.378026</td>\n",
       "      <td>-4.798233</td>\n",
       "      <td>-14.394444</td>\n",
       "      <td>32.979874</td>\n",
       "      <td>-20.168663</td>\n",
       "      <td>-40.403976</td>\n",
       "      <td>28.580751</td>\n",
       "      <td>-33.926491</td>\n",
       "      <td>-51.121618</td>\n",
       "      <td>22.435043</td>\n",
       "      <td>-45.941994</td>\n",
       "      <td>-58.652584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>87.000000</td>\n",
       "      <td>232.000000</td>\n",
       "      <td>260.000000</td>\n",
       "      <td>94.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>-6.817474</td>\n",
       "      <td>-8.803251</td>\n",
       "      <td>45.916725</td>\n",
       "      <td>-28.895584</td>\n",
       "      <td>-20.031019</td>\n",
       "      <td>40.402634</td>\n",
       "      <td>-35.806393</td>\n",
       "      <td>-27.875320</td>\n",
       "      <td>-0.243591</td>\n",
       "      <td>-42.077468</td>\n",
       "      <td>-34.920914</td>\n",
       "      <td>-27.283134</td>\n",
       "      <td>-46.161587</td>\n",
       "      <td>-39.915314</td>\n",
       "      <td>-43.178146</td>\n",
       "      <td>-15.925140</td>\n",
       "      <td>6.985863</td>\n",
       "      <td>-26.454096</td>\n",
       "      <td>-9.026079</td>\n",
       "      <td>4.463394</td>\n",
       "      <td>-56.602979</td>\n",
       "      <td>-6.701809</td>\n",
       "      <td>-5.036469</td>\n",
       "      <td>-71.388425</td>\n",
       "      <td>-6.119522</td>\n",
       "      <td>-14.418533</td>\n",
       "      <td>-80.410423</td>\n",
       "      <td>2.151676</td>\n",
       "      <td>9.575974</td>\n",
       "      <td>-20.124603</td>\n",
       "      <td>15.232921</td>\n",
       "      <td>4.461868</td>\n",
       "      <td>-54.737091</td>\n",
       "      <td>17.435110</td>\n",
       "      <td>-7.066628</td>\n",
       "      <td>-72.400841</td>\n",
       "      <td>15.876219</td>\n",
       "      <td>-17.552639</td>\n",
       "      <td>-82.020462</td>\n",
       "      <td>18.373080</td>\n",
       "      <td>8.872391</td>\n",
       "      <td>-9.174782</td>\n",
       "      <td>29.783724</td>\n",
       "      <td>-2.790802</td>\n",
       "      <td>-41.144341</td>\n",
       "      <td>28.554492</td>\n",
       "      <td>-17.606827</td>\n",
       "      <td>-56.429403</td>\n",
       "      <td>24.772669</td>\n",
       "      <td>-29.923187</td>\n",
       "      <td>-64.962646</td>\n",
       "      <td>33.172597</td>\n",
       "      <td>3.786240</td>\n",
       "      <td>2.370359</td>\n",
       "      <td>43.076133</td>\n",
       "      <td>-7.020432</td>\n",
       "      <td>-22.701141</td>\n",
       "      <td>43.237972</td>\n",
       "      <td>-18.004105</td>\n",
       "      <td>-32.667877</td>\n",
       "      <td>41.146488</td>\n",
       "      <td>-28.819870</td>\n",
       "      <td>-38.904312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>124.000000</td>\n",
       "      <td>379.000000</td>\n",
       "      <td>398.000000</td>\n",
       "      <td>127.000000</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>16.217522</td>\n",
       "      <td>-0.516635</td>\n",
       "      <td>48.589417</td>\n",
       "      <td>-8.741714</td>\n",
       "      <td>-7.933365</td>\n",
       "      <td>50.493407</td>\n",
       "      <td>-28.042894</td>\n",
       "      <td>-19.092545</td>\n",
       "      <td>16.548199</td>\n",
       "      <td>-26.790266</td>\n",
       "      <td>-22.851822</td>\n",
       "      <td>-7.325100</td>\n",
       "      <td>-23.168304</td>\n",
       "      <td>-23.262767</td>\n",
       "      <td>-21.588488</td>\n",
       "      <td>-9.653668</td>\n",
       "      <td>12.051170</td>\n",
       "      <td>-17.442369</td>\n",
       "      <td>3.830158</td>\n",
       "      <td>21.979660</td>\n",
       "      <td>-48.346376</td>\n",
       "      <td>9.978733</td>\n",
       "      <td>24.674629</td>\n",
       "      <td>-58.375332</td>\n",
       "      <td>13.583131</td>\n",
       "      <td>23.936172</td>\n",
       "      <td>-60.066422</td>\n",
       "      <td>7.391723</td>\n",
       "      <td>13.382727</td>\n",
       "      <td>-17.935727</td>\n",
       "      <td>27.901146</td>\n",
       "      <td>25.355267</td>\n",
       "      <td>-48.052736</td>\n",
       "      <td>37.783436</td>\n",
       "      <td>27.261452</td>\n",
       "      <td>-59.167010</td>\n",
       "      <td>43.332731</td>\n",
       "      <td>25.106003</td>\n",
       "      <td>-60.922623</td>\n",
       "      <td>22.278203</td>\n",
       "      <td>14.082314</td>\n",
       "      <td>-4.153049</td>\n",
       "      <td>41.117867</td>\n",
       "      <td>16.907257</td>\n",
       "      <td>-32.404678</td>\n",
       "      <td>50.301309</td>\n",
       "      <td>12.930918</td>\n",
       "      <td>-43.088371</td>\n",
       "      <td>54.606854</td>\n",
       "      <td>6.906043</td>\n",
       "      <td>-45.007335</td>\n",
       "      <td>35.187468</td>\n",
       "      <td>10.987781</td>\n",
       "      <td>9.982524</td>\n",
       "      <td>54.273039</td>\n",
       "      <td>11.055683</td>\n",
       "      <td>-9.839353</td>\n",
       "      <td>62.777967</td>\n",
       "      <td>6.611061</td>\n",
       "      <td>-18.714948</td>\n",
       "      <td>67.228989</td>\n",
       "      <td>-0.268571</td>\n",
       "      <td>-23.747971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1597.000000</td>\n",
       "      <td>1524.000000</td>\n",
       "      <td>1622.000000</td>\n",
       "      <td>1841.000000</td>\n",
       "      <td>1822.000000</td>\n",
       "      <td>1776.000000</td>\n",
       "      <td>1665.000000</td>\n",
       "      <td>1869.000000</td>\n",
       "      <td>52.359741</td>\n",
       "      <td>50.699184</td>\n",
       "      <td>54.694229</td>\n",
       "      <td>48.278114</td>\n",
       "      <td>55.201233</td>\n",
       "      <td>61.440742</td>\n",
       "      <td>14.705590</td>\n",
       "      <td>48.795957</td>\n",
       "      <td>66.071671</td>\n",
       "      <td>33.443270</td>\n",
       "      <td>72.289636</td>\n",
       "      <td>81.401413</td>\n",
       "      <td>50.398455</td>\n",
       "      <td>93.400940</td>\n",
       "      <td>96.671875</td>\n",
       "      <td>29.582220</td>\n",
       "      <td>33.404694</td>\n",
       "      <td>15.407104</td>\n",
       "      <td>55.390396</td>\n",
       "      <td>62.662933</td>\n",
       "      <td>33.897133</td>\n",
       "      <td>73.737137</td>\n",
       "      <td>82.416367</td>\n",
       "      <td>46.695574</td>\n",
       "      <td>87.999207</td>\n",
       "      <td>96.914932</td>\n",
       "      <td>53.381760</td>\n",
       "      <td>23.445007</td>\n",
       "      <td>25.185989</td>\n",
       "      <td>6.137421</td>\n",
       "      <td>66.375984</td>\n",
       "      <td>69.576004</td>\n",
       "      <td>25.039421</td>\n",
       "      <td>88.228264</td>\n",
       "      <td>94.551773</td>\n",
       "      <td>44.773242</td>\n",
       "      <td>102.207489</td>\n",
       "      <td>111.433588</td>\n",
       "      <td>55.553363</td>\n",
       "      <td>26.836068</td>\n",
       "      <td>25.465968</td>\n",
       "      <td>18.247375</td>\n",
       "      <td>65.926038</td>\n",
       "      <td>64.414276</td>\n",
       "      <td>41.517410</td>\n",
       "      <td>90.692032</td>\n",
       "      <td>86.780737</td>\n",
       "      <td>58.788498</td>\n",
       "      <td>107.192108</td>\n",
       "      <td>102.598709</td>\n",
       "      <td>70.328949</td>\n",
       "      <td>39.572960</td>\n",
       "      <td>34.939362</td>\n",
       "      <td>35.891083</td>\n",
       "      <td>69.175903</td>\n",
       "      <td>53.101288</td>\n",
       "      <td>52.591827</td>\n",
       "      <td>85.342997</td>\n",
       "      <td>67.743378</td>\n",
       "      <td>65.750656</td>\n",
       "      <td>101.063904</td>\n",
       "      <td>81.727936</td>\n",
       "      <td>76.889313</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 ch1            ch2            ch3            ch4  \\\n",
       "count  106979.000000  106979.000000  106979.000000  106979.000000   \n",
       "mean      107.959076     277.776554     293.447060     112.697202   \n",
       "std        94.478903     183.819378     181.553457      80.348375   \n",
       "min        18.000000      20.000000      22.000000      20.000000   \n",
       "25%        61.000000     143.000000     152.000000      72.000000   \n",
       "50%        87.000000     232.000000     260.000000      94.000000   \n",
       "75%       124.000000     379.000000     398.000000     127.000000   \n",
       "max      1597.000000    1524.000000    1622.000000    1841.000000   \n",
       "\n",
       "                 ch5            ch6            ch7            ch8  \\\n",
       "count  106979.000000  106979.000000  106979.000000  106979.000000   \n",
       "mean       55.729031      72.685406      82.720188      67.966208   \n",
       "std        68.751875     110.614351     137.870160     103.867191   \n",
       "min        17.000000      16.000000      15.000000      15.000000   \n",
       "25%        33.000000      28.000000      26.000000      27.000000   \n",
       "50%        40.000000      33.000000      31.000000      34.000000   \n",
       "75%        58.000000      59.000000      66.000000      56.000000   \n",
       "max      1822.000000    1776.000000    1665.000000    1869.000000   \n",
       "\n",
       "             Wrist x        Wrist y        Wrist z  Thumb Proximal x  \\\n",
       "count  106979.000000  106979.000000  106979.000000     106979.000000   \n",
       "mean       -1.519532      -9.021946      44.597009        -24.352507   \n",
       "std        18.203014      13.902148       5.902738         17.833762   \n",
       "min       -49.460617     -51.661484      -8.476379        -59.220070   \n",
       "25%       -16.886156     -17.755783      42.359551        -39.496848   \n",
       "50%        -6.817474      -8.803251      45.916725        -28.895584   \n",
       "75%        16.217522      -0.516635      48.589417         -8.741714   \n",
       "max        52.359741      50.699184      54.694229         48.278114   \n",
       "\n",
       "       Thumb Proximal y  Thumb Proximal z  Thumb Intermediate x  \\\n",
       "count     106979.000000     106979.000000         106979.000000   \n",
       "mean         -18.957473         39.809937            -35.202575   \n",
       "std           17.313549         12.854626             10.539880   \n",
       "min          -58.296936        -27.315063            -70.195053   \n",
       "25%          -31.326163         31.505045            -42.614314   \n",
       "50%          -20.031019         40.402634            -35.806393   \n",
       "75%           -7.933365         50.493407            -28.042894   \n",
       "max           55.201233         61.440742             14.705590   \n",
       "\n",
       "       Thumb Intermediate y  Thumb Intermediate z  Thumb Distal x  \\\n",
       "count         106979.000000         106979.000000   106979.000000   \n",
       "mean             -26.591242              2.103636      -40.789788   \n",
       "std               13.674356             17.937670       20.180079   \n",
       "min              -71.954811            -49.751572      -92.561234   \n",
       "25%              -35.798828            -11.512680      -56.313196   \n",
       "50%              -27.875320             -0.243591      -42.077468   \n",
       "75%              -19.092545             16.548199      -26.790266   \n",
       "max               48.795957             66.071671       33.443270   \n",
       "\n",
       "       Thumb Distal y  Thumb Distal z    Thumb Tip x    Thumb Tip y  \\\n",
       "count   106979.000000   106979.000000  106979.000000  106979.000000   \n",
       "mean       -32.012339      -22.847791     -44.154327     -35.382372   \n",
       "std         18.120903       22.667319      30.208039      23.253477   \n",
       "min        -90.297638      -74.886850    -112.754237    -103.504150   \n",
       "25%        -44.501390      -40.588364     -68.302795     -52.288237   \n",
       "50%        -34.920914      -27.283134     -46.161587     -39.915314   \n",
       "75%        -22.851822       -7.325100     -23.168304     -23.262767   \n",
       "max         72.289636       81.401413      50.398455      93.400940   \n",
       "\n",
       "         Thumb Tip z  Index Proximal x  Index Proximal y  Index Proximal z  \\\n",
       "count  106979.000000     106979.000000     106979.000000     106979.000000   \n",
       "mean      -37.740631        -16.995893          7.373415        -23.683015   \n",
       "std        26.870641          9.447936          7.232516          7.711189   \n",
       "min       -94.854360        -35.338264        -31.792175        -35.444645   \n",
       "25%       -58.127181        -26.110477          2.451675        -30.127332   \n",
       "50%       -43.178146        -15.925140          6.985863        -26.454096   \n",
       "75%       -21.588488         -9.653668         12.051170        -17.442369   \n",
       "max        96.671875         29.582220         33.404694         15.407104   \n",
       "\n",
       "       Index Intermediate x  Index Intermediate y  Index Intermediate z  \\\n",
       "count         106979.000000         106979.000000         106979.000000   \n",
       "mean             -14.404202              3.981787            -54.751712   \n",
       "std               22.331166             23.019437             11.387768   \n",
       "min              -69.852450            -67.396919            -74.289215   \n",
       "25%              -35.800728            -14.385666            -63.962860   \n",
       "50%               -9.026079              4.463394            -56.602979   \n",
       "75%                3.830158             21.979660            -48.346376   \n",
       "max               55.390396             62.662933             33.897133   \n",
       "\n",
       "       Index Distal x  Index Distal y  Index Distal z    Index Tip x  \\\n",
       "count   106979.000000   106979.000000   106979.000000  106979.000000   \n",
       "mean       -13.055133       -3.578775      -67.611467     -12.313513   \n",
       "std         28.566881       33.673159       18.030049      32.359781   \n",
       "min        -85.011714      -87.236955      -96.546054     -97.884471   \n",
       "25%        -39.267851      -32.875201      -81.408615     -41.107635   \n",
       "50%         -6.701809       -5.036469      -71.388425      -6.119522   \n",
       "75%          9.978733       24.674629      -58.375332      13.583131   \n",
       "max         73.737137       82.416367       46.695574      87.999207   \n",
       "\n",
       "         Index Tip y    Index Tip z  Middle Proximal x  Middle Proximal y  \\\n",
       "count  106979.000000  106979.000000      106979.000000      106979.000000   \n",
       "mean      -10.208400     -73.972310           0.712549           9.388218   \n",
       "std        40.269171      25.337680           8.255159           6.050276   \n",
       "min      -102.978952    -112.462585         -24.145329         -21.980112   \n",
       "25%       -45.302944     -93.672626          -6.837919           5.863414   \n",
       "50%       -14.418533     -80.410423           2.151676           9.575974   \n",
       "75%        23.936172     -60.066422           7.391723          13.382727   \n",
       "max        96.914932      53.381760          23.445007          25.185989   \n",
       "\n",
       "       Middle Proximal z  Middle Intermediate x  Middle Intermediate y  \\\n",
       "count      106979.000000          106979.000000          106979.000000   \n",
       "mean          -19.306369               9.581468               4.679483   \n",
       "std             3.587558              22.631948              25.144259   \n",
       "min           -25.657837             -54.249256             -65.532608   \n",
       "25%           -21.728305             -11.149257             -16.099387   \n",
       "50%           -20.124603              15.232921               4.461868   \n",
       "75%           -17.935727              27.901146              25.355267   \n",
       "max             6.137421              66.375984              69.576004   \n",
       "\n",
       "       Middle Intermediate z  Middle Distal x  Middle Distal y  \\\n",
       "count          106979.000000    106979.000000    106979.000000   \n",
       "mean              -53.065222        12.692946        -4.684326   \n",
       "std                10.117039        30.242326        37.583324   \n",
       "min               -72.724304       -75.109543       -91.424866   \n",
       "25%               -60.206032       -13.197766       -38.103338   \n",
       "50%               -54.737091        17.435110        -7.066628   \n",
       "75%               -48.052736        37.783436        27.261452   \n",
       "max                25.039421        88.228264        94.551773   \n",
       "\n",
       "       Middle Distal z   Middle Tip x   Middle Tip y   Middle Tip z  \\\n",
       "count    106979.000000  106979.000000  106979.000000  106979.000000   \n",
       "mean        -67.946889      13.756485     -12.317275     -74.908446   \n",
       "std          18.901129      34.760672      44.641463      27.063839   \n",
       "min        -100.599316     -91.211243    -109.174942    -118.985252   \n",
       "25%         -81.200862     -14.475380     -51.520982     -94.599333   \n",
       "50%         -72.400841      15.876219     -17.552639     -82.020462   \n",
       "75%         -59.167010      43.332731      25.106003     -60.922623   \n",
       "max          44.773242     102.207489     111.433588      55.553363   \n",
       "\n",
       "       Ring Proximal x  Ring Proximal y  Ring Proximal z  Ring Intermediate x  \\\n",
       "count    106979.000000    106979.000000    106979.000000        106979.000000   \n",
       "mean         17.529791         8.095233       -10.276684            25.648988   \n",
       "std           5.532313         7.853489         8.013560            18.961775   \n",
       "min         -12.639398       -25.039429       -26.869192           -40.555109   \n",
       "25%          13.344295         2.828438       -17.730928            10.075094   \n",
       "50%          18.373080         8.872391        -9.174782            29.783724   \n",
       "75%          22.278203        14.082314        -4.153049            41.117867   \n",
       "max          26.836068        25.465968        18.247375            65.926038   \n",
       "\n",
       "       Ring Intermediate y  Ring Intermediate z  Ring Distal x  Ring Distal y  \\\n",
       "count        106979.000000        106979.000000  106979.000000  106979.000000   \n",
       "mean             -0.778659           -40.999281      27.275005     -12.593958   \n",
       "std              22.917322            13.537246      27.110144      33.080043   \n",
       "min             -60.490051           -66.913315     -59.266098     -81.102333   \n",
       "25%             -19.526822           -51.560223       6.909097     -40.784522   \n",
       "50%              -2.790802           -41.144341      28.554492     -17.606827   \n",
       "75%              16.907257           -32.404678      50.301309      12.930918   \n",
       "max              64.414276            41.517410      90.692032      86.780737   \n",
       "\n",
       "       Ring Distal z     Ring Tip x     Ring Tip y     Ring Tip z  \\\n",
       "count  106979.000000  106979.000000  106979.000000  106979.000000   \n",
       "mean      -54.669632      27.006531     -21.898478     -60.856092   \n",
       "std        21.107323      32.277698      38.685426      28.408468   \n",
       "min       -93.231049     -75.195114     -92.351674    -111.118563   \n",
       "25%       -69.958648       3.556110     -54.368904     -81.196796   \n",
       "50%       -56.429403      24.772669     -29.923187     -64.962646   \n",
       "75%       -43.088371      54.606854       6.906043     -45.007335   \n",
       "max        58.788498     107.192108     102.598709      70.328949   \n",
       "\n",
       "       Pinky Proximal x  Pinky Proximal y  Pinky Proximal z  \\\n",
       "count     106979.000000     106979.000000     106979.000000   \n",
       "mean          32.352225          2.837640         -1.285509   \n",
       "std            4.083498         11.077100         13.504016   \n",
       "min          -30.062690        -36.622925        -36.324131   \n",
       "25%           30.378026         -4.798233        -14.394444   \n",
       "50%           33.172597          3.786240          2.370359   \n",
       "75%           35.187468         10.987781          9.982524   \n",
       "max           39.572960         34.939362         35.891083   \n",
       "\n",
       "       Pinky Intermediate x  Pinky Intermediate y  Pinky Intermediate z  \\\n",
       "count         106979.000000         106979.000000         106979.000000   \n",
       "mean              41.970822             -4.485664            -24.370822   \n",
       "std               14.765910             20.311902             18.142483   \n",
       "min              -37.905338            -56.838379            -66.809570   \n",
       "25%               32.979874            -20.168663            -40.403976   \n",
       "50%               43.076133             -7.020432            -22.701141   \n",
       "75%               54.273039             11.055683             -9.839353   \n",
       "max               69.175903             53.101288             52.591827   \n",
       "\n",
       "       Pinky Distal x  Pinky Distal y  Pinky Distal z    Pinky Tip x  \\\n",
       "count   106979.000000   106979.000000   106979.000000  106979.000000   \n",
       "mean        43.496761      -12.840841      -34.176921      42.845118   \n",
       "std         21.859076       26.318486       21.132027      28.005897   \n",
       "min        -43.796612      -71.155746      -80.829399     -53.352108   \n",
       "25%         28.580751      -33.926491      -51.121618      22.435043   \n",
       "50%         43.237972      -18.004105      -32.667877      41.146488   \n",
       "75%         62.777967        6.611061      -18.714948      67.228989   \n",
       "max         85.342997       67.743378       65.750656     101.063904   \n",
       "\n",
       "         Pinky Tip y    Pinky Tip z  \n",
       "count  106979.000000  106979.000000  \n",
       "mean      -21.385414     -40.369203  \n",
       "std        30.720860      25.151496  \n",
       "min       -81.252701     -96.391819  \n",
       "25%       -45.941994     -58.652584  \n",
       "50%       -28.819870     -38.904312  \n",
       "75%        -0.268571     -23.747971  \n",
       "max        81.727936      76.889313  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df = df[:5000]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_features(x_train):\n",
    "    shape = (7, 13, 8)\n",
    "    reshape = (-1, shape[0], shape[1], shape[2])\n",
    "    x_train = x_train.replace(-np.inf, 0)\n",
    "    x_train = x_train.replace(np.inf, 0)\n",
    "    #x_train = np.log(x_train.values)\n",
    "    x_train = x_train.values\n",
    "    x_train_norm = x_train.reshape(reshape)\n",
    "    return x_train_norm\n",
    "\n",
    "#features = preprocess_features(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(106979, 18)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_ar = df.loc[:, 'ch1':'ch8'].values\n",
    "label_ar = df.loc[:, 'Wrist x':].values\n",
    "label_ar_tips = label_ar[:,[0,1,2,12,13,14,24,25,26,36,37,38,48,49,50,60,61,62]]\n",
    "label_ar_tips.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(106979, 8)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_ar.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(106979, 63)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_ar.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(106956, 24, 8)\n",
      "(106956, 63)\n"
     ]
    }
   ],
   "source": [
    "seq_length = 24\n",
    "\n",
    "def overlap_samples(seq_length, feats, labels):\n",
    "    new_l = labels[seq_length - 1:]\n",
    "    feat_list = [feats[i:i + seq_length] for i in range(feats.shape[0] - seq_length + 1)]\n",
    "    new_f = np.array(feat_list)\n",
    "    return new_f, new_l\n",
    "\n",
    "features, labels = overlap_samples(seq_length, feature_ar, label_ar)\n",
    "features, labels_tips = overlap_samples(seq_length, feature_ar, label_ar_tips)\n",
    "print(features.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Basics\n",
    "[attention mechanism from scratch](https://towardsdatascience.com/learning-attention-mechanism-from-scratch-f08706aaf6b6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention scoring\n",
    "\n",
    "#### Inputs to the scoring function\n",
    "\n",
    "Start by looking at the inputs we'll give to the scoring function.\n",
    "We will assume we're in the first step in the decoding phase.\n",
    "First input to scoring function is hidden state of decoder.\n",
    "Assuming a toy RNN with three hidden nodes -- not usable in real life but easier to illustrate\n",
    "\n",
    "### decoder hidden state vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_hidden_state = [5, 1, 20] # Decoder hidden state (query vector?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize this vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x109dcd898>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 108x324 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "plt.figure(figsize=(1.5, 4.5))\n",
    "sns.heatmap(np.transpose(np.matrix(dec_hidden_state)), annot=True, cmap=sns.light_palette(\"purple\", as_cmap=True), linewidths=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder hidden state = annotation\n",
    "\n",
    "Our first scoring function will score a single annotation (encoder hidden state), which looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation = [3, 12, 45] # i.e. Encoder hidden state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1166bf908>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 108x324 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualizing single annotation\n",
    "plt.figure(figsize=(1.5, 4.5))\n",
    "sns.heatmap(np.transpose(np.matrix(annotation)), annot=True,\n",
    "            cmap=sns.light_palette(\"orange\",as_cmap=True), linewidths=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring a single annotation\n",
    "dot product of decoder hidden state and encoder hidden state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "927"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def single_dot_attention_score(dec_hidden_state, enc_hidden_state):\n",
    "    return np.dot(dec_hidden_state, enc_hidden_state)\n",
    "\n",
    "single_dot_attention_score(dec_hidden_state, annotation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annotations Matrix\n",
    "All the scoring of annotations at once. To do that, here's annotation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations = np.transpose([[3, 12, 45], [59, 2, 5], [1, 43, 5], [4, 3, 45.3]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be visualized like this (each column is a hidden state of an encoder time step):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = sns.heatmap(annotations, annot=True, cmap=sns.light_palette(\"orange\", as_cmap=True), linewidths=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement: Scoring all annotations at once\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([927., 397., 148., 929.])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def dot_attention_score(dec_hidden, annotations):\n",
    "    return np.matmul(np.transpose(dec_hidden_state), annotations)\n",
    "\n",
    "attention_weights_raw = dot_attention_score(dec_hidden_state, annotations)\n",
    "attention_weights_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at these scores the 929 will get the most attention from the decoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax\n",
    "After getting scores we apply softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4,)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def softmax(x):\n",
    "    x = np.array(x, dtype=np.float128)\n",
    "    e_x = np.exp(x)\n",
    "    return e_x / e_x.sum(axis=0)\n",
    "\n",
    "attention_weights = softmax(attention_weights_raw)\n",
    "attention_weights.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Its good to appreciate again, even after knowing which annotation will get the most focus, \n",
    "just how much more drastic the softmax makes the difference.\n",
    "The first and last annotation had 927 and 929 after the softmax they get .119 and .880 respectively\n",
    "Even a variation of less than 1% ((1 - 929/927)*100) gets incremented to a variation of 800% ((1 - 929/927)*100)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying the scores back on the annotations\n",
    "Now that we have our scores, let's multiply each annotation by its score to proceed closer to the attention context vector. This is the multiplication part of this formula (we'll tackle the summation part in the latter cells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.57608766e-001, 4.68881939e-230, 5.76614420e-340,\n",
       "        3.52318831e+000],\n",
       "       [1.43043506e+000, 1.58943030e-231, 2.47944200e-338,\n",
       "        2.64239123e+000],\n",
       "       [5.36413149e+000, 3.97357575e-231, 2.88307210e-339,\n",
       "        3.99001076e+001]], dtype=float128)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def apply_attention_scores(attention_weights, annotations):\n",
    "    return attention_weights * annotations\n",
    "\n",
    "applied_attention = apply_attention_scores(attention_weights, annotations)\n",
    "applied_attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how the context vector looks now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualizing annotations after applying attention\n",
    "ax = sns.heatmap(applied_attention, cmap=sns.light_palette(\"orange\", as_cmap=True), linewidths=1, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contrasting this with the original annotations matrix and the second and third column have been reduced \n",
    "greatly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Attention Context Vector\n",
    "All that remains to produce our attention context vector now is to sum up the four columns to produce a single attention context vector.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.88079708,  4.0728263 , 45.26423912], dtype=float128)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_attention_vector(applied_attention):\n",
    "    return np.sum(applied_attention, axis=1)\n",
    "\n",
    "attention_vector = calculate_attention_vector(applied_attention)\n",
    "attention_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1419aa780>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 108x324 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualizing attention context vector\n",
    "plt.figure(figsize=(1.5, 4.5))\n",
    "sns.heatmap(np.transpose(np.matrix(attention_vector)), cmap=sns.light_palette(\"blue\", as_cmap=True), linewidths=1, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention\n",
    "**This model will incorporate a component of attention**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model from phillip peremy\n",
    "https://github.com/philipperemy/keras-attention-mechanism/blob/master/attention_lstm.py\n",
    "Attention vector\n",
    "\n",
    "Also, sometimes the time series can be N-dimensional. It could be interesting to have one atention vector per dimension. Let's\n",
    "\n",
    "Attention can just then be a softmax applied to an output of something?\n",
    "\n",
    "The permute function switches the positions of the axis and the dims argument tells how you want the final positions to be.\n",
    "\n",
    "For example, if x is 4-dimensional and of the shape (None, 2, 4, 5, 8) - (None is the batch size here) and if you specify dims = (3, 2, 1, 4), then the following four steps will take place:\n",
    "\n",
    "1. Third dimension will move to first\n",
    "2. Second dimension will move to second\n",
    "3. First dimension will move to third\n",
    "4. Fourth dimension will move to fourth\n",
    "\n",
    "Remember, the indexing starts at 1 and not 0. The dimension zero is the batch size. So finally the output\n",
    "\n",
    "**RepeatVector**\n",
    "Repeats the input vector n times\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Updated version:\n",
    "https://github.com/philipperemy/keras-attention-mechanism/issues/14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def attention_3d_block_2(hidden_states):\n",
    "    # hidden_states.shape = (batch_size, time_steps, hidden_size)\n",
    "    hidden_size = int(hidden_states.shape[2])\n",
    "    # Inside dense layer hidden_states dot W => score_first_part\n",
    "    # (batch_size, time_steps, hidden_size) dot (hidden_size, hidden_size) => (batch_size, time_steps, hidden_size)\n",
    "    # W is the trainable weight matrix of attention\n",
    "    # Luong's multiplicative style score\n",
    "    score_first_part = Dense(hidden_size, use_bias=False, name='attention_score_vec')(hidden_states)\n",
    "    #  score_first_part dot last_hidden_state => attention_weights\n",
    "    # (batch_size, time_steps, hidden_size) dot (batch_size, hidden_size) => (batch_size, time_steps)\n",
    "    h_t = Lambda(lambda x: x[:, -1, :], output_shape=(hidden_size,), name='last_hidden_state')(hidden_states)\n",
    "    score = dot([score_first_part, h_t], [2, 1], name='attention_score')\n",
    "    attention_weights = Activation('softmax', name='attention_weight')(score)\n",
    "    # (batch_size, time_steps, hidden_size) dot (batch_size, time_steps) => (batch_size, hidden_size)\n",
    "    context_vector = dot([hidden_states, attention_weights], [1, 1], name='context_vector')\n",
    "    pre_activation = concatenate([context_vector, h_t], name='attention_output')\n",
    "    attention_vector = Dense(128, use_bias=False, activation='tanh', name='attention_vector')(pre_activation)\n",
    "    return attention_vector\n",
    "    #return pre_activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_27 (InputLayer)           (None, 24, 8)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_41 (LSTM)                  (None, 24, 40)       7840        input_27[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_23 (Dropout)            (None, 24, 40)       0           lstm_41[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_21 (Batc (None, 24, 40)       160         dropout_23[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "permute_28 (Permute)            (None, 40, 24)       0           batch_normalization_v1_21[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_14 (Reshape)            (None, 40, 24)       0           permute_28[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_29 (Dense)                (None, 40, 24)       600         reshape_14[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 24)           0           dense_29[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_14 (RepeatVector) (None, 40, 24)       0           lambda_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "permute_29 (Permute)            (None, 24, 40)       0           repeat_vector_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_14 (Add)                    (None, 24, 40)       0           permute_29[0][0]                 \n",
      "                                                                 batch_normalization_v1_21[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_42 (LSTM)                  (None, 24, 40)       12960       add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_24 (Dropout)            (None, 24, 40)       0           lstm_42[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_22 (Batc (None, 24, 40)       160         dropout_24[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "permute_30 (Permute)            (None, 40, 24)       0           batch_normalization_v1_22[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_15 (Reshape)            (None, 40, 24)       0           permute_30[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_30 (Dense)                (None, 40, 24)       600         reshape_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, 24)           0           dense_30[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_15 (RepeatVector) (None, 40, 24)       0           lambda_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "permute_31 (Permute)            (None, 24, 40)       0           repeat_vector_15[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_15 (Add)                    (None, 24, 40)       0           permute_31[0][0]                 \n",
      "                                                                 batch_normalization_v1_22[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_9 (Flatten)             (None, 960)          0           add_15[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_31 (Dense)                (None, 63)           60543       flatten_9[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 82,863\n",
      "Trainable params: 82,703\n",
      "Non-trainable params: 160\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "INPUT_DIM = 8\n",
    "TIME_STEPS = 24\n",
    "# if True, the attention vector is shared across the input_dimensions where the attention is applied.\n",
    "SINGLE_ATTENTION_VECTOR = True\n",
    "APPLY_ATTENTION_BEFORE_LSTM = False\n",
    "\n",
    "\n",
    "def attention_3d_block_cnn(inputs,timesteps):\n",
    "    input_dim = int(inputs.shape[2])\n",
    "    time_steps=timesteps\n",
    "    a_probs = Conv1D(input_dim,3,strides=1,padding='same',activation='softmax')(inputs)\n",
    "    output_attention_mul= Multiply()([inputs, a_probs]) #name='attention_mul'\n",
    "    return output_attention_mu\n",
    "\n",
    "def attention_3d_block(inputs):\n",
    "    # inputs.shape = (batch_size, time_steps, input_dim)\n",
    "    input_dim = int(inputs.shape[2])\n",
    "    a = Permute((2, 1))(inputs) # changes the order of the parameters (in this case 2 becomes 1 and 1 becomes 2)\n",
    "    a = Reshape((input_dim, TIME_STEPS))(a) # this line is not useful. It's just to know which dimension is what.\n",
    "    a = Dense(TIME_STEPS, activation='softmax')(a)\n",
    "    if SINGLE_ATTENTION_VECTOR:\n",
    "        a = Lambda(lambda x: tf.keras.backend.mean(x, axis=1))(a)\n",
    "        a = RepeatVector(input_dim)(a)\n",
    "    a_probs = Permute((2, 1))(a) #, name='attention_vec')(a)\n",
    "    output_attention_mul = Add()([a_probs, inputs])\n",
    "    return output_attention_mul\n",
    "\n",
    "def model_attention_applied_after_lstm():\n",
    "    inputs = Input(shape=(TIME_STEPS, INPUT_DIM,))\n",
    "    \n",
    "    lstm_units = 40\n",
    "    \n",
    "    model = LSTM(lstm_units, return_sequences=True)(inputs)\n",
    "    model = Dropout(0.3)(model)\n",
    "    model = BatchNormalization()(model)\n",
    "    model = attention_3d_block(model)\n",
    "    #model = LSTM(lstm_units, return_sequences=True)(model)\n",
    "    #model = attention_3d_block_cnn()(model)\n",
    "    \n",
    "    model = LSTM(lstm_units, return_sequences=True)(model)\n",
    "    model = Dropout(0.3)(model)\n",
    "    model = BatchNormalization()(model)\n",
    "    model = attention_3d_block(model)\n",
    "    \n",
    "    model = Flatten()(model)\n",
    "    output = Dense(63, activation=None)(model)\n",
    "    model = Model(inputs, output)\n",
    "    return model\n",
    "\n",
    "model = model_attention_applied_after_lstm()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(lr=0.005, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.00, amsgrad=True)\n",
    "model.compile(optimizer=optimizer, loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106956/106956 [==============================] - 41s 383us/sample - loss: 1553.1603\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1553.1602660580418"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(features, labels, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention shared\n",
    "val loss = 430"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 85564 samples, validate on 21392 samples\n",
      "Epoch 1/40\n",
      "85564/85564 [==============================] - 118s 1ms/sample - loss: 361.9699 - val_loss: 514.5101\n",
      "Epoch 2/40\n",
      "85564/85564 [==============================] - 125s 1ms/sample - loss: 306.6589 - val_loss: 429.1403\n",
      "Epoch 3/40\n",
      "85564/85564 [==============================] - 121s 1ms/sample - loss: 286.3154 - val_loss: 443.1745\n",
      "Epoch 4/40\n",
      "85564/85564 [==============================] - 121s 1ms/sample - loss: 275.4276 - val_loss: 524.1820\n",
      "Epoch 5/40\n",
      "85564/85564 [==============================] - 119s 1ms/sample - loss: 268.7602 - val_loss: 433.6133\n",
      "Epoch 6/40\n",
      "85564/85564 [==============================] - 118s 1ms/sample - loss: 264.2084 - val_loss: 449.7018\n",
      "Epoch 7/40\n",
      "85564/85564 [==============================] - 118s 1ms/sample - loss: 258.4215 - val_loss: 422.5237\n",
      "Epoch 8/40\n",
      "85564/85564 [==============================] - 118s 1ms/sample - loss: 262.3262 - val_loss: 478.1697\n",
      "Epoch 9/40\n",
      "85564/85564 [==============================] - 118s 1ms/sample - loss: 261.7812 - val_loss: 432.0519\n",
      "Epoch 10/40\n",
      "85564/85564 [==============================] - 114s 1ms/sample - loss: 249.9113 - val_loss: 448.7812\n",
      "Epoch 11/40\n",
      "85564/85564 [==============================] - 104s 1ms/sample - loss: 246.3505 - val_loss: 424.4854\n",
      "Epoch 12/40\n",
      "53088/85564 [=================>............] - ETA: 40s - loss: 243.3002"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-96-e2e3ddcb9129>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTrainValTensorBoard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    878\u001b[0m           \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m           validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, mode, validation_in_fit, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3074\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3075\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3076\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3077\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3078\u001b[0m     return nest.pack_sequence_as(self._outputs_structure,\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit(features, labels, epochs=40, verbose=1, validation_split=0.2, callbacks=[TrainValTensorBoard()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('Attention_jose_all_loss400_vloss500_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.legend(['Train', 'Test'])\n",
    "plt.title('Attention')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (4977,63) (4977,18) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-17377843fbde>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0merror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msq_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mavg_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msq_error\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxticks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrotation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m90\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (4977,63) (4977,18) "
     ]
    }
   ],
   "source": [
    "error = labels - preds\n",
    "sq_error = error * error\n",
    "avg_error = np.mean(sq_error, axis=0)\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.xticks(rotation=90)\n",
    "plt.ylabel('Prediction Error (mm)')\n",
    "bar = plt.bar(df.columns[8:], avg_error)\n",
    "for i in range(0,63,3):\n",
    "    bar[i].set_color('coral')\n",
    "    bar[i+1].set_color('olivedrab')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
