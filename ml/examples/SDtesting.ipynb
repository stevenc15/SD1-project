{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports from Sanzid\n",
    "import h5py\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import numpy\n",
    "import statistics\n",
    "from numpy import loadtxt\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas\n",
    "import math\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from statistics import stdev\n",
    "import math\n",
    "import h5py\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from scipy.signal import butter,filtfilt\n",
    "import sys\n",
    "import numpy as np # linear algebra\n",
    "from scipy.stats import randint\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv), data manipulation as in SQL\n",
    "import matplotlib.pyplot as plt # this is used for the plot the graph\n",
    "import seaborn as sns # used for plot interactive graph.\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# from tsf.model import TransformerForecaster\n",
    "\n",
    "\n",
    "# from tensorflow.keras.utils import np_utils\n",
    "import itertools\n",
    "###  Library for attention layers\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "#from tqdm import tqdm # Processing time measurement\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import statistics\n",
    "import gc\n",
    "import torch.nn.init as init\n",
    "\n",
    "############################################################################################################################################################################\n",
    "############################################################################################################################################################################\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.utils.weight_norm as weight_norm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "#from torchsummary import summary\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "#from torchsummary import summary\n",
    "\n",
    "\n",
    "\n",
    "import torch.optim as optim\n",
    "import gc\n",
    "\n",
    "#from tqdm import tqdm_notebook\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "\n",
    "#imports for this notebook using sensor distillation utils\n",
    "import sys #you need this to add path to utils folder\n",
    "sys.path.append('../Sensor_distillation_utils') #now you can import from utils folder\n",
    "from sensor_distillation import train_SD\n",
    "from student_model import student\n",
    "from teacher_model import teacher\n",
    "from train import train_kinematics\n",
    "from predictions import prediction_test\n",
    "\n",
    "#loading in data from lightweight kinematics net\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"\"\\n#WINDOWING DATA\\n## Creating a dataset with overlapping window of 100 samples with overlap of 50 samples ##\\n\\n# # convert an array of values into a dataset matrix\\ndef create_dataset_present(dataset_1, window=100):\\n  dataX= []\\n  k=0\\n  shift=50\\n  for i in range(int(len(dataset_1)/shift)-1):\\n    j=shift*k\\n    a = dataset_1[j:j+window,:]\\n    dataX.append(a)\\n    k=k+1\\n  return np.array(dataX)\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Convert an array of values into a dataset matrix\n",
    "def create_dataset_present(dataset_1, window=100):\n",
    "    # Ensure the dataset is a NumPy array\n",
    "    if isinstance(dataset_1, pd.DataFrame):\n",
    "        dataset_1 = dataset_1.values\n",
    "    dataX = []\n",
    "    k = 0\n",
    "    shift = 50\n",
    "    for i in range(int(len(dataset_1) / shift) - 1):\n",
    "        j = shift * k\n",
    "        a = dataset_1[j:j + window, :]\n",
    "        dataX.append(a)\n",
    "        k = k + 1\n",
    "    return np.array(dataX)\n",
    "\"\"\"\"\"\n",
    "#WINDOWING DATA\n",
    "## Creating a dataset with overlapping window of 100 samples with overlap of 50 samples ##\n",
    "\n",
    "# # convert an array of values into a dataset matrix\n",
    "def create_dataset_present(dataset_1, window=100):\n",
    "  dataX= []\n",
    "  k=0\n",
    "  shift=50\n",
    "  for i in range(int(len(dataset_1)/shift)-1):\n",
    "    j=shift*k\n",
    "    a = dataset_1[j:j+window,:]\n",
    "    dataX.append(a)\n",
    "    k=k+1\n",
    "  return np.array(dataX)\n",
    "\"\"\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64581, 97)\n",
      "Index(['arm_flex_r', 'arm_add_r', 'elbow_flex_r'], dtype='object')\n",
      "(64581, 48)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\"\\n# Get two 3-channel arrays for ACC and GYRO for sensor 2\\ndata_acc = combined_csv[[\\'ACCX2\\', \\'ACCY2\\', \\'ACCZ2\\']].values\\ndata_gyr = combined_csv[[\\'GYROX2\\', \\'GYROY2\\', \\'GYROZ2\\']].values\\n\\n# Normalize the data\\nscaler_acc = StandardScaler()\\nscaler_gyr = StandardScaler()\\nscaler_joints = StandardScaler()\\n\\ndata_acc = scaler_acc.fit_transform(data_acc)\\ndata_gyr = scaler_gyr.fit_transform(data_gyr)\\ndata_joints = combined_csv[[\\'elbow_flex_r\\']].values\\ndata_joints = scaler_joints.fit_transform(data_joints)\\n\\nprint (data_acc.shape, data_gyr.shape, data_joints.shape)\\n\\n#data_acc = train X data_gyr = train X data_joints = train y\\n################\\nw=100\\n\\ntrain_X_3_acc=create_dataset_present(data_acc,w)\\ntrain_X_3_gyr=create_dataset_present(data_gyr,w)\\ntrain_y_3=create_dataset_present(data_joints,w)\\n\\ntrain_y_3=train_y_3.reshape(train_y_3.shape[0],w,6)\\n\\ntrain_X_1D, X_validation_1D, train_y_5, Y_validation = train_test_split(train_X_3,train_y_3, test_size=0.20, random_state=True)\\n\\nprint(train_X_1D.shape,train_y_5.shape,X_validation_1D.shape,Y_validation.shape)\\n\\nfeatures=6\\ntrain_X_2D=train_X_1D.reshape(train_X_1D.shape[0],train_X_1D.shape[1],features,8)\\nX_validation_2D= X_validation_1D.reshape(X_validation_1D.shape[0], X_validation_1D.shape[1],features,8)\\n\\n# Create dataset instances with overlapping windows\\ndataset = Sensor_Distillation_Dataset(data_acc, data_gyr, data_joints, w=1000, overlap=500)\\n\\n\\n# Split the dataset into training and validation sets\\ntrain_size = int(0.9 * len(dataset))\\nval_size = len(dataset) - train_size\\ntrain_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\\n\\nprint (len(train_dataset), len(val_dataset))\\n\\n# Create DataLoaders\\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\\nval_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\\n\\nprint(len(val_loader))\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Load and clean the combined CSV data\n",
    "import sys #you need this to add path to utils folder\n",
    "sys.path.append('../../datacollection') #now you can import from utils folder\n",
    "from batch_process import combine_and_save_data\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "remake_combined = False\n",
    "if remake_combined == True:   \n",
    "    combine_and_save_data(\"../../datacollection/vicon/subject_1/processed\", \"../../datacollection/vicon/subject_1/sensors\", \"../../datacollection/vicon/subject_1/combined\")\n",
    "\n",
    "combined_csv = pd.read_csv(\"../../datacollection/vicon/subject_1/combined/P001_T001_armSwing_fast_combined.csv\")\n",
    "\n",
    "print(combined_csv.shape)\n",
    "\n",
    "train_joint_data = combined_csv.iloc[:, 25:29]\n",
    "columns_to_drop = [\"arm_rot_r\"]\n",
    "train_joint_data = train_joint_data.drop(columns = columns_to_drop)\n",
    "print(train_joint_data.columns)\n",
    "train_imu_data = combined_csv.iloc[:, 49:]\n",
    "print(train_imu_data.shape)\n",
    "#50 parriba\n",
    "\"\"\"\"\n",
    "# Get two 3-channel arrays for ACC and GYRO for sensor 2\n",
    "data_acc = combined_csv[['ACCX2', 'ACCY2', 'ACCZ2']].values\n",
    "data_gyr = combined_csv[['GYROX2', 'GYROY2', 'GYROZ2']].values\n",
    "\n",
    "# Normalize the data\n",
    "scaler_acc = StandardScaler()\n",
    "scaler_gyr = StandardScaler()\n",
    "scaler_joints = StandardScaler()\n",
    "\n",
    "data_acc = scaler_acc.fit_transform(data_acc)\n",
    "data_gyr = scaler_gyr.fit_transform(data_gyr)\n",
    "data_joints = combined_csv[['elbow_flex_r']].values\n",
    "data_joints = scaler_joints.fit_transform(data_joints)\n",
    "\n",
    "print (data_acc.shape, data_gyr.shape, data_joints.shape)\n",
    "\n",
    "#data_acc = train X data_gyr = train X data_joints = train y\n",
    "################\n",
    "w=100\n",
    "\n",
    "train_X_3_acc=create_dataset_present(data_acc,w)\n",
    "train_X_3_gyr=create_dataset_present(data_gyr,w)\n",
    "train_y_3=create_dataset_present(data_joints,w)\n",
    "\n",
    "train_y_3=train_y_3.reshape(train_y_3.shape[0],w,6)\n",
    "\n",
    "train_X_1D, X_validation_1D, train_y_5, Y_validation = train_test_split(train_X_3,train_y_3, test_size=0.20, random_state=True)\n",
    "\n",
    "print(train_X_1D.shape,train_y_5.shape,X_validation_1D.shape,Y_validation.shape)\n",
    "\n",
    "features=6\n",
    "train_X_2D=train_X_1D.reshape(train_X_1D.shape[0],train_X_1D.shape[1],features,8)\n",
    "X_validation_2D= X_validation_1D.reshape(X_validation_1D.shape[0], X_validation_1D.shape[1],features,8)\n",
    "\n",
    "# Create dataset instances with overlapping windows\n",
    "dataset = Sensor_Distillation_Dataset(data_acc, data_gyr, data_joints, w=1000, overlap=500)\n",
    "\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "print (len(train_dataset), len(val_dataset))\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "print(len(val_loader))\n",
    "\"\"\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64581, 3)\n",
      "(64581, 48)\n"
     ]
    }
   ],
   "source": [
    "#load in test data\n",
    "remake_combined_test = False\n",
    "if remake_combined_test == True:   \n",
    "    combine_and_save_data(\"../../datacollection/vicon/subject_2/processed\", \"../../datacollection/vicon/subject_2/sensors\", \"../../datacollection/vicon/subject_2/combined\")\n",
    "\n",
    "combined_csv_test = pd.read_csv(\"../../datacollection/vicon/subject_2/combined/P002_T001_armSwing_fast_combined.csv\")\n",
    "\n",
    "\n",
    "test_joint_data = combined_csv.iloc[:, 25:29]\n",
    "columns_to_drop = [\"arm_rot_r\"]\n",
    "test_joint_data = test_joint_data.drop(columns = columns_to_drop)\n",
    "print(test_joint_data.shape)\n",
    "test_imu_data = combined_csv.iloc[:, 49:]\n",
    "print(test_imu_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1290, 100, 3)\n"
     ]
    }
   ],
   "source": [
    "#WINDOW DATA\n",
    "### Reconstruction/Present Dataset ###\n",
    "w=100\n",
    "\n",
    "train_X_3=create_dataset_present(train_imu_data,w)\n",
    "train_y_3=create_dataset_present(train_joint_data,w)\n",
    "print (train_y_3.shape)\n",
    "test_X_1D=create_dataset_present(test_imu_data,w)\n",
    "test_y=create_dataset_present(test_joint_data,w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RESHAPING TARGETS TO 3D ARRAY \n",
    "# -rows, -window_length, -columns\n",
    "train_y_3=train_y_3.reshape(train_y_3.shape[0],w,3)\n",
    "test_y=test_y.reshape(test_y.shape[0],w,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1032, 100, 48) (1032, 100, 3) (258, 100, 48) (258, 100, 3)\n"
     ]
    }
   ],
   "source": [
    "#SPLIT TRAINING & VALIDATION\n",
    "train_X_1D, X_validation_1D, train_y_5, Y_validation = train_test_split(train_X_3,train_y_3, test_size=0.20, random_state=True)\n",
    "\n",
    "print(train_X_1D.shape,train_y_5.shape,X_validation_1D.shape,Y_validation.shape)\n",
    "#shape is num_rows, window_length, num_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1032, 100, 6, 8) (1290, 100, 6, 8) (258, 100, 6, 8)\n"
     ]
    }
   ],
   "source": [
    "#features or columns in the dataset\n",
    "channels=6\n",
    "#1D is -rows, -window_length, columns\n",
    "#2D is just reshaping the 1D to -rows, -window_length, -columns, -num_sensors\n",
    "train_X_2D=train_X_1D.reshape(train_X_1D.shape[0],train_X_1D.shape[1],channels,8)\n",
    "test_X_2D=test_X_1D.reshape(test_X_1D.shape[0],test_X_1D.shape[1],channels,8)\n",
    "X_validation_2D= X_validation_1D.reshape(X_validation_1D.shape[0],\n",
    "                                                   X_validation_1D.shape[1],channels,8)\n",
    "\n",
    "\n",
    "print(train_X_2D.shape,test_X_2D.shape,X_validation_2D.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TURNING BACK THE DATA TO DESIRED FORM\n",
    "## Evaluting data with the original form without any overlaps\n",
    "\n",
    "# # convert an array of values into a dataset matrix\n",
    "def unpack_dataset_present(dataset_1):\n",
    "  dataX= []\n",
    "  k=1\n",
    "  l=0\n",
    "  shift=100\n",
    "  for i in range(int(len(dataset_1)/shift)-1):\n",
    "    j=shift*k\n",
    "    a = dataset_1[l:j,:]\n",
    "    l=0\n",
    "    l=j+50\n",
    "    dataX=np.append(dataX,a)\n",
    "    k=k+1\n",
    "    j=0\n",
    "  return np.array(dataX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1290, 100, 3])\n"
     ]
    }
   ],
   "source": [
    "#PREP THE DATA\n",
    "### IMUs- Chest, Waist, Right Foot, Right shank, Right thigh, Left Foot, Left shank, Left thigh, 2D-body coordinate\n",
    "### 0:48- IMU, 48:92-2D body coordinate, 92:97-- Target\n",
    "\n",
    "\n",
    "### Data Processing\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "#MAKE 1D INTO TENSOR\n",
    "train_features_1D = torch.Tensor(train_X_1D)\n",
    "val_features_1D = torch.Tensor(X_validation_1D)\n",
    "test_features_1D = torch.Tensor(test_X_1D)\n",
    "\n",
    "#MAKE 2D INTO TENSOR\n",
    "train_features_2D = torch.Tensor(train_X_2D)\n",
    "val_features_2D = torch.Tensor(X_validation_2D)\n",
    "test_features_2D = torch.Tensor(test_X_2D)\n",
    "\n",
    "#TARGETS TO TENSORS\n",
    "train_targets = torch.Tensor(train_y_5)\n",
    "val_targets = torch.Tensor(Y_validation)\n",
    "test_targets = torch.Tensor(test_y)\n",
    "\n",
    "print(test_targets.shape)\n",
    "\n",
    "## all Modality Features\n",
    "train = TensorDataset(train_features_1D, train_features_2D, train_targets)\n",
    "val = TensorDataset(val_features_1D, val_features_2D,val_targets)\n",
    "test = TensorDataset(test_features_1D,test_features_2D,test_targets)\n",
    "\n",
    "train_loader = DataLoader(train, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "val_loader = DataLoader(val, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "test_loader = DataLoader(test, batch_size=batch_size, shuffle=False, drop_last=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#define device\n",
    "# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SOME MODEL PARAMETERS\n",
    "alpha=0.50\n",
    "num_epoch=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_features_1D shape: torch.Size([64, 100, 48])\n",
      "data_features_2D shape: torch.Size([64, 100, 6, 8])\n",
      "data_targets shape: torch.Size([64, 100, 3])\n",
      "##################################\n",
      "first passage through encoder gru\n",
      "input to encoder_gru, torch.Size([64, 100, 48])\n",
      "shape of out_1 after lstm1, torch.Size([64, 100, 256])\n",
      "shape of out_1 dropout1, torch.Size([64, 100, 256])\n",
      "shape of out_2 after lstm1, torch.Size([64, 100, 128])\n",
      "shape of out_2 dropout2, torch.Size([64, 100, 128])\n",
      "shape of out_2 after flatten, torch.Size([64, 12800])\n",
      "necessary shape for last linear layer = 128000, 600\n",
      "shape encoder_gru output, torch.Size([64, 300])\n",
      "shape encoder_gru output after view, torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 1D\n",
      "shape of input, torch.Size([64, 100, 48])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "Shape after transpose: torch.Size([64, 48, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 100])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 100])\n",
      "Shape after pool: torch.Size([64, 64, 50])\n",
      "Shape after conv3 relu: torch.Size([64, 64, 50])\n",
      "Shape after conv4 relu: torch.Size([64, 64, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 50])\n",
      "Shape after pool: torch.Size([64, 64, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 128])\n",
      "Shape after dropout1: torch.Size([64, 25, 128])\n",
      "necessary shape for fc2 is 25*64, 25*64\n",
      "Shape after fc2: torch.Size([64, 25, 1600])\n",
      "Shape after dropout2: torch.Size([64, 25, 1600])\n",
      "Shape after flatten: torch.Size([64, 40000])\n",
      "Shape of model_2_output: torch.Size([64, 12800])\n",
      "Shape of model_2_output_1 (concactenating last dimension of model_2_output and x): torch.Size([64, 52800])\n",
      "necessary shape for linear 528 * w, 6*w\n",
      "Shape of output_C2: torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 2D\n",
      "shape of input x_1, torch.Size([64, 100, 48])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "shape of input x, torch.Size([64, 100, 6, 8])\n",
      "inputs_2D_N: torch.Size([64, 8, 6, 100])\n",
      " inputs_2D_N: torch.Size([64, 8, 6, 100])\n",
      "inputs_2D_N: torch.Size([64, 100, 6, 8])\n",
      "Shape after transpose: torch.Size([64, 8, 6, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 6, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 6, 100])\n",
      "Shape after pool: torch.Size([64, 64, 3, 50])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 3, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 3, 50])\n",
      "Shape after pool: torch.Size([64, 64, 1, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 1, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 1, 1600])\n",
      "Shape after dropout: torch.Size([64, 25, 1, 1600])\n",
      "necessary shape for fc2 is 64, 32\n",
      "Shape after dropout2: torch.Size([64, 25, 1, 64])\n",
      "Shape after flatten: torch.Size([64, 1600])\n",
      "Shape of model_3_output: torch.Size([64, 12800])\n",
      "Shape of model_3_output after cat: torch.Size([64, 14400])\n",
      "Shape of output_C1: torch.Size([64, 100, 3])\n",
      "shape of out_gru, torch.Size([64, 100, 3])\n",
      "shape of out_C2, torch.Size([64, 100, 3])\n",
      "shape of out_C1, torch.Size([64, 100, 3])\n",
      "output: torch.Size([64, 100, 3])\n",
      "data_features_1D shape: torch.Size([64, 100, 48])\n",
      "data_features_2D shape: torch.Size([64, 100, 6, 8])\n",
      "data_targets shape: torch.Size([64, 100, 3])\n",
      "##################################\n",
      "first passage through encoder gru\n",
      "input to encoder_gru, torch.Size([64, 100, 48])\n",
      "shape of out_1 after lstm1, torch.Size([64, 100, 256])\n",
      "shape of out_1 dropout1, torch.Size([64, 100, 256])\n",
      "shape of out_2 after lstm1, torch.Size([64, 100, 128])\n",
      "shape of out_2 dropout2, torch.Size([64, 100, 128])\n",
      "shape of out_2 after flatten, torch.Size([64, 12800])\n",
      "necessary shape for last linear layer = 128000, 600\n",
      "shape encoder_gru output, torch.Size([64, 300])\n",
      "shape encoder_gru output after view, torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 1D\n",
      "shape of input, torch.Size([64, 100, 48])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "Shape after transpose: torch.Size([64, 48, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 100])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 100])\n",
      "Shape after pool: torch.Size([64, 64, 50])\n",
      "Shape after conv3 relu: torch.Size([64, 64, 50])\n",
      "Shape after conv4 relu: torch.Size([64, 64, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 50])\n",
      "Shape after pool: torch.Size([64, 64, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 128])\n",
      "Shape after dropout1: torch.Size([64, 25, 128])\n",
      "necessary shape for fc2 is 25*64, 25*64\n",
      "Shape after fc2: torch.Size([64, 25, 1600])\n",
      "Shape after dropout2: torch.Size([64, 25, 1600])\n",
      "Shape after flatten: torch.Size([64, 40000])\n",
      "Shape of model_2_output: torch.Size([64, 12800])\n",
      "Shape of model_2_output_1 (concactenating last dimension of model_2_output and x): torch.Size([64, 52800])\n",
      "necessary shape for linear 528 * w, 6*w\n",
      "Shape of output_C2: torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 2D\n",
      "shape of input x_1, torch.Size([64, 100, 48])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "shape of input x, torch.Size([64, 100, 6, 8])\n",
      "inputs_2D_N: torch.Size([64, 8, 6, 100])\n",
      " inputs_2D_N: torch.Size([64, 8, 6, 100])\n",
      "inputs_2D_N: torch.Size([64, 100, 6, 8])\n",
      "Shape after transpose: torch.Size([64, 8, 6, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 6, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 6, 100])\n",
      "Shape after pool: torch.Size([64, 64, 3, 50])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 3, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 3, 50])\n",
      "Shape after pool: torch.Size([64, 64, 1, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 1, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 1, 1600])\n",
      "Shape after dropout: torch.Size([64, 25, 1, 1600])\n",
      "necessary shape for fc2 is 64, 32\n",
      "Shape after dropout2: torch.Size([64, 25, 1, 64])\n",
      "Shape after flatten: torch.Size([64, 1600])\n",
      "Shape of model_3_output: torch.Size([64, 12800])\n",
      "Shape of model_3_output after cat: torch.Size([64, 14400])\n",
      "Shape of output_C1: torch.Size([64, 100, 3])\n",
      "shape of out_gru, torch.Size([64, 100, 3])\n",
      "shape of out_C2, torch.Size([64, 100, 3])\n",
      "shape of out_C1, torch.Size([64, 100, 3])\n",
      "output: torch.Size([64, 100, 3])\n",
      "data_features_1D shape: torch.Size([64, 100, 48])\n",
      "data_features_2D shape: torch.Size([64, 100, 6, 8])\n",
      "data_targets shape: torch.Size([64, 100, 3])\n",
      "##################################\n",
      "first passage through encoder gru\n",
      "input to encoder_gru, torch.Size([64, 100, 48])\n",
      "shape of out_1 after lstm1, torch.Size([64, 100, 256])\n",
      "shape of out_1 dropout1, torch.Size([64, 100, 256])\n",
      "shape of out_2 after lstm1, torch.Size([64, 100, 128])\n",
      "shape of out_2 dropout2, torch.Size([64, 100, 128])\n",
      "shape of out_2 after flatten, torch.Size([64, 12800])\n",
      "necessary shape for last linear layer = 128000, 600\n",
      "shape encoder_gru output, torch.Size([64, 300])\n",
      "shape encoder_gru output after view, torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 1D\n",
      "shape of input, torch.Size([64, 100, 48])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "Shape after transpose: torch.Size([64, 48, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 100])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 100])\n",
      "Shape after pool: torch.Size([64, 64, 50])\n",
      "Shape after conv3 relu: torch.Size([64, 64, 50])\n",
      "Shape after conv4 relu: torch.Size([64, 64, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 50])\n",
      "Shape after pool: torch.Size([64, 64, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 128])\n",
      "Shape after dropout1: torch.Size([64, 25, 128])\n",
      "necessary shape for fc2 is 25*64, 25*64\n",
      "Shape after fc2: torch.Size([64, 25, 1600])\n",
      "Shape after dropout2: torch.Size([64, 25, 1600])\n",
      "Shape after flatten: torch.Size([64, 40000])\n",
      "Shape of model_2_output: torch.Size([64, 12800])\n",
      "Shape of model_2_output_1 (concactenating last dimension of model_2_output and x): torch.Size([64, 52800])\n",
      "necessary shape for linear 528 * w, 6*w\n",
      "Shape of output_C2: torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 2D\n",
      "shape of input x_1, torch.Size([64, 100, 48])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "shape of input x, torch.Size([64, 100, 6, 8])\n",
      "inputs_2D_N: torch.Size([64, 8, 6, 100])\n",
      " inputs_2D_N: torch.Size([64, 8, 6, 100])\n",
      "inputs_2D_N: torch.Size([64, 100, 6, 8])\n",
      "Shape after transpose: torch.Size([64, 8, 6, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 6, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 6, 100])\n",
      "Shape after pool: torch.Size([64, 64, 3, 50])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 3, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 3, 50])\n",
      "Shape after pool: torch.Size([64, 64, 1, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 1, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 1, 1600])\n",
      "Shape after dropout: torch.Size([64, 25, 1, 1600])\n",
      "necessary shape for fc2 is 64, 32\n",
      "Shape after dropout2: torch.Size([64, 25, 1, 64])\n",
      "Shape after flatten: torch.Size([64, 1600])\n",
      "Shape of model_3_output: torch.Size([64, 12800])\n",
      "Shape of model_3_output after cat: torch.Size([64, 14400])\n",
      "Shape of output_C1: torch.Size([64, 100, 3])\n",
      "shape of out_gru, torch.Size([64, 100, 3])\n",
      "shape of out_C2, torch.Size([64, 100, 3])\n",
      "shape of out_C1, torch.Size([64, 100, 3])\n",
      "output: torch.Size([64, 100, 3])\n",
      "data_features_1D shape: torch.Size([64, 100, 48])\n",
      "data_features_2D shape: torch.Size([64, 100, 6, 8])\n",
      "data_targets shape: torch.Size([64, 100, 3])\n",
      "##################################\n",
      "first passage through encoder gru\n",
      "input to encoder_gru, torch.Size([64, 100, 48])\n",
      "shape of out_1 after lstm1, torch.Size([64, 100, 256])\n",
      "shape of out_1 dropout1, torch.Size([64, 100, 256])\n",
      "shape of out_2 after lstm1, torch.Size([64, 100, 128])\n",
      "shape of out_2 dropout2, torch.Size([64, 100, 128])\n",
      "shape of out_2 after flatten, torch.Size([64, 12800])\n",
      "necessary shape for last linear layer = 128000, 600\n",
      "shape encoder_gru output, torch.Size([64, 300])\n",
      "shape encoder_gru output after view, torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 1D\n",
      "shape of input, torch.Size([64, 100, 48])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "Shape after transpose: torch.Size([64, 48, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 100])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 100])\n",
      "Shape after pool: torch.Size([64, 64, 50])\n",
      "Shape after conv3 relu: torch.Size([64, 64, 50])\n",
      "Shape after conv4 relu: torch.Size([64, 64, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 50])\n",
      "Shape after pool: torch.Size([64, 64, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 128])\n",
      "Shape after dropout1: torch.Size([64, 25, 128])\n",
      "necessary shape for fc2 is 25*64, 25*64\n",
      "Shape after fc2: torch.Size([64, 25, 1600])\n",
      "Shape after dropout2: torch.Size([64, 25, 1600])\n",
      "Shape after flatten: torch.Size([64, 40000])\n",
      "Shape of model_2_output: torch.Size([64, 12800])\n",
      "Shape of model_2_output_1 (concactenating last dimension of model_2_output and x): torch.Size([64, 52800])\n",
      "necessary shape for linear 528 * w, 6*w\n",
      "Shape of output_C2: torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 2D\n",
      "shape of input x_1, torch.Size([64, 100, 48])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "shape of input x, torch.Size([64, 100, 6, 8])\n",
      "inputs_2D_N: torch.Size([64, 8, 6, 100])\n",
      " inputs_2D_N: torch.Size([64, 8, 6, 100])\n",
      "inputs_2D_N: torch.Size([64, 100, 6, 8])\n",
      "Shape after transpose: torch.Size([64, 8, 6, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 6, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 6, 100])\n",
      "Shape after pool: torch.Size([64, 64, 3, 50])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 3, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 3, 50])\n",
      "Shape after pool: torch.Size([64, 64, 1, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 1, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 1, 1600])\n",
      "Shape after dropout: torch.Size([64, 25, 1, 1600])\n",
      "necessary shape for fc2 is 64, 32\n",
      "Shape after dropout2: torch.Size([64, 25, 1, 64])\n",
      "Shape after flatten: torch.Size([64, 1600])\n",
      "Shape of model_3_output: torch.Size([64, 12800])\n",
      "Shape of model_3_output after cat: torch.Size([64, 14400])\n",
      "Shape of output_C1: torch.Size([64, 100, 3])\n",
      "shape of out_gru, torch.Size([64, 100, 3])\n",
      "shape of out_C2, torch.Size([64, 100, 3])\n",
      "shape of out_C1, torch.Size([64, 100, 3])\n",
      "output: torch.Size([64, 100, 3])\n",
      "data_features_1D shape: torch.Size([64, 100, 48])\n",
      "data_features_2D shape: torch.Size([64, 100, 6, 8])\n",
      "data_targets shape: torch.Size([64, 100, 3])\n",
      "##################################\n",
      "first passage through encoder gru\n",
      "input to encoder_gru, torch.Size([64, 100, 48])\n",
      "shape of out_1 after lstm1, torch.Size([64, 100, 256])\n",
      "shape of out_1 dropout1, torch.Size([64, 100, 256])\n",
      "shape of out_2 after lstm1, torch.Size([64, 100, 128])\n",
      "shape of out_2 dropout2, torch.Size([64, 100, 128])\n",
      "shape of out_2 after flatten, torch.Size([64, 12800])\n",
      "necessary shape for last linear layer = 128000, 600\n",
      "shape encoder_gru output, torch.Size([64, 300])\n",
      "shape encoder_gru output after view, torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 1D\n",
      "shape of input, torch.Size([64, 100, 48])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "Shape after transpose: torch.Size([64, 48, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 100])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 100])\n",
      "Shape after pool: torch.Size([64, 64, 50])\n",
      "Shape after conv3 relu: torch.Size([64, 64, 50])\n",
      "Shape after conv4 relu: torch.Size([64, 64, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 50])\n",
      "Shape after pool: torch.Size([64, 64, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 128])\n",
      "Shape after dropout1: torch.Size([64, 25, 128])\n",
      "necessary shape for fc2 is 25*64, 25*64\n",
      "Shape after fc2: torch.Size([64, 25, 1600])\n",
      "Shape after dropout2: torch.Size([64, 25, 1600])\n",
      "Shape after flatten: torch.Size([64, 40000])\n",
      "Shape of model_2_output: torch.Size([64, 12800])\n",
      "Shape of model_2_output_1 (concactenating last dimension of model_2_output and x): torch.Size([64, 52800])\n",
      "necessary shape for linear 528 * w, 6*w\n",
      "Shape of output_C2: torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 2D\n",
      "shape of input x_1, torch.Size([64, 100, 48])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "shape of input x, torch.Size([64, 100, 6, 8])\n",
      "inputs_2D_N: torch.Size([64, 8, 6, 100])\n",
      " inputs_2D_N: torch.Size([64, 8, 6, 100])\n",
      "inputs_2D_N: torch.Size([64, 100, 6, 8])\n",
      "Shape after transpose: torch.Size([64, 8, 6, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 6, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 6, 100])\n",
      "Shape after pool: torch.Size([64, 64, 3, 50])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 3, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 3, 50])\n",
      "Shape after pool: torch.Size([64, 64, 1, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 1, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 1, 1600])\n",
      "Shape after dropout: torch.Size([64, 25, 1, 1600])\n",
      "necessary shape for fc2 is 64, 32\n",
      "Shape after dropout2: torch.Size([64, 25, 1, 64])\n",
      "Shape after flatten: torch.Size([64, 1600])\n",
      "Shape of model_3_output: torch.Size([64, 12800])\n",
      "Shape of model_3_output after cat: torch.Size([64, 14400])\n",
      "Shape of output_C1: torch.Size([64, 100, 3])\n",
      "shape of out_gru, torch.Size([64, 100, 3])\n",
      "shape of out_C2, torch.Size([64, 100, 3])\n",
      "shape of out_C1, torch.Size([64, 100, 3])\n",
      "output: torch.Size([64, 100, 3])\n",
      "data_features_1D shape: torch.Size([64, 100, 48])\n",
      "data_features_2D shape: torch.Size([64, 100, 6, 8])\n",
      "data_targets shape: torch.Size([64, 100, 3])\n",
      "##################################\n",
      "first passage through encoder gru\n",
      "input to encoder_gru, torch.Size([64, 100, 48])\n",
      "shape of out_1 after lstm1, torch.Size([64, 100, 256])\n",
      "shape of out_1 dropout1, torch.Size([64, 100, 256])\n",
      "shape of out_2 after lstm1, torch.Size([64, 100, 128])\n",
      "shape of out_2 dropout2, torch.Size([64, 100, 128])\n",
      "shape of out_2 after flatten, torch.Size([64, 12800])\n",
      "necessary shape for last linear layer = 128000, 600\n",
      "shape encoder_gru output, torch.Size([64, 300])\n",
      "shape encoder_gru output after view, torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 1D\n",
      "shape of input, torch.Size([64, 100, 48])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "Shape after transpose: torch.Size([64, 48, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 100])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 100])\n",
      "Shape after pool: torch.Size([64, 64, 50])\n",
      "Shape after conv3 relu: torch.Size([64, 64, 50])\n",
      "Shape after conv4 relu: torch.Size([64, 64, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 50])\n",
      "Shape after pool: torch.Size([64, 64, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 128])\n",
      "Shape after dropout1: torch.Size([64, 25, 128])\n",
      "necessary shape for fc2 is 25*64, 25*64\n",
      "Shape after fc2: torch.Size([64, 25, 1600])\n",
      "Shape after dropout2: torch.Size([64, 25, 1600])\n",
      "Shape after flatten: torch.Size([64, 40000])\n",
      "Shape of model_2_output: torch.Size([64, 12800])\n",
      "Shape of model_2_output_1 (concactenating last dimension of model_2_output and x): torch.Size([64, 52800])\n",
      "necessary shape for linear 528 * w, 6*w\n",
      "Shape of output_C2: torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 2D\n",
      "shape of input x_1, torch.Size([64, 100, 48])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "shape of input x, torch.Size([64, 100, 6, 8])\n",
      "inputs_2D_N: torch.Size([64, 8, 6, 100])\n",
      " inputs_2D_N: torch.Size([64, 8, 6, 100])\n",
      "inputs_2D_N: torch.Size([64, 100, 6, 8])\n",
      "Shape after transpose: torch.Size([64, 8, 6, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 6, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 6, 100])\n",
      "Shape after pool: torch.Size([64, 64, 3, 50])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 3, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 3, 50])\n",
      "Shape after pool: torch.Size([64, 64, 1, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 1, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 1, 1600])\n",
      "Shape after dropout: torch.Size([64, 25, 1, 1600])\n",
      "necessary shape for fc2 is 64, 32\n",
      "Shape after dropout2: torch.Size([64, 25, 1, 64])\n",
      "Shape after flatten: torch.Size([64, 1600])\n",
      "Shape of model_3_output: torch.Size([64, 12800])\n",
      "Shape of model_3_output after cat: torch.Size([64, 14400])\n",
      "Shape of output_C1: torch.Size([64, 100, 3])\n",
      "shape of out_gru, torch.Size([64, 100, 3])\n",
      "shape of out_C2, torch.Size([64, 100, 3])\n",
      "shape of out_C1, torch.Size([64, 100, 3])\n",
      "output: torch.Size([64, 100, 3])\n",
      "data_features_1D shape: torch.Size([64, 100, 48])\n",
      "data_features_2D shape: torch.Size([64, 100, 6, 8])\n",
      "data_targets shape: torch.Size([64, 100, 3])\n",
      "##################################\n",
      "first passage through encoder gru\n",
      "input to encoder_gru, torch.Size([64, 100, 48])\n",
      "shape of out_1 after lstm1, torch.Size([64, 100, 256])\n",
      "shape of out_1 dropout1, torch.Size([64, 100, 256])\n",
      "shape of out_2 after lstm1, torch.Size([64, 100, 128])\n",
      "shape of out_2 dropout2, torch.Size([64, 100, 128])\n",
      "shape of out_2 after flatten, torch.Size([64, 12800])\n",
      "necessary shape for last linear layer = 128000, 600\n",
      "shape encoder_gru output, torch.Size([64, 300])\n",
      "shape encoder_gru output after view, torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 1D\n",
      "shape of input, torch.Size([64, 100, 48])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "Shape after transpose: torch.Size([64, 48, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 100])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 100])\n",
      "Shape after pool: torch.Size([64, 64, 50])\n",
      "Shape after conv3 relu: torch.Size([64, 64, 50])\n",
      "Shape after conv4 relu: torch.Size([64, 64, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 50])\n",
      "Shape after pool: torch.Size([64, 64, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 128])\n",
      "Shape after dropout1: torch.Size([64, 25, 128])\n",
      "necessary shape for fc2 is 25*64, 25*64\n",
      "Shape after fc2: torch.Size([64, 25, 1600])\n",
      "Shape after dropout2: torch.Size([64, 25, 1600])\n",
      "Shape after flatten: torch.Size([64, 40000])\n",
      "Shape of model_2_output: torch.Size([64, 12800])\n",
      "Shape of model_2_output_1 (concactenating last dimension of model_2_output and x): torch.Size([64, 52800])\n",
      "necessary shape for linear 528 * w, 6*w\n",
      "Shape of output_C2: torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 2D\n",
      "shape of input x_1, torch.Size([64, 100, 48])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "shape of input x, torch.Size([64, 100, 6, 8])\n",
      "inputs_2D_N: torch.Size([64, 8, 6, 100])\n",
      " inputs_2D_N: torch.Size([64, 8, 6, 100])\n",
      "inputs_2D_N: torch.Size([64, 100, 6, 8])\n",
      "Shape after transpose: torch.Size([64, 8, 6, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 6, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 6, 100])\n",
      "Shape after pool: torch.Size([64, 64, 3, 50])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 3, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 3, 50])\n",
      "Shape after pool: torch.Size([64, 64, 1, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 1, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 1, 1600])\n",
      "Shape after dropout: torch.Size([64, 25, 1, 1600])\n",
      "necessary shape for fc2 is 64, 32\n",
      "Shape after dropout2: torch.Size([64, 25, 1, 64])\n",
      "Shape after flatten: torch.Size([64, 1600])\n",
      "Shape of model_3_output: torch.Size([64, 12800])\n",
      "Shape of model_3_output after cat: torch.Size([64, 14400])\n",
      "Shape of output_C1: torch.Size([64, 100, 3])\n",
      "shape of out_gru, torch.Size([64, 100, 3])\n",
      "shape of out_C2, torch.Size([64, 100, 3])\n",
      "shape of out_C1, torch.Size([64, 100, 3])\n",
      "output: torch.Size([64, 100, 3])\n",
      "data_features_1D shape: torch.Size([64, 100, 48])\n",
      "data_features_2D shape: torch.Size([64, 100, 6, 8])\n",
      "data_targets shape: torch.Size([64, 100, 3])\n",
      "##################################\n",
      "first passage through encoder gru\n",
      "input to encoder_gru, torch.Size([64, 100, 48])\n",
      "shape of out_1 after lstm1, torch.Size([64, 100, 256])\n",
      "shape of out_1 dropout1, torch.Size([64, 100, 256])\n",
      "shape of out_2 after lstm1, torch.Size([64, 100, 128])\n",
      "shape of out_2 dropout2, torch.Size([64, 100, 128])\n",
      "shape of out_2 after flatten, torch.Size([64, 12800])\n",
      "necessary shape for last linear layer = 128000, 600\n",
      "shape encoder_gru output, torch.Size([64, 300])\n",
      "shape encoder_gru output after view, torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 1D\n",
      "shape of input, torch.Size([64, 100, 48])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "Shape after transpose: torch.Size([64, 48, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 100])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 100])\n",
      "Shape after pool: torch.Size([64, 64, 50])\n",
      "Shape after conv3 relu: torch.Size([64, 64, 50])\n",
      "Shape after conv4 relu: torch.Size([64, 64, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 50])\n",
      "Shape after pool: torch.Size([64, 64, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 128])\n",
      "Shape after dropout1: torch.Size([64, 25, 128])\n",
      "necessary shape for fc2 is 25*64, 25*64\n",
      "Shape after fc2: torch.Size([64, 25, 1600])\n",
      "Shape after dropout2: torch.Size([64, 25, 1600])\n",
      "Shape after flatten: torch.Size([64, 40000])\n",
      "Shape of model_2_output: torch.Size([64, 12800])\n",
      "Shape of model_2_output_1 (concactenating last dimension of model_2_output and x): torch.Size([64, 52800])\n",
      "necessary shape for linear 528 * w, 6*w\n",
      "Shape of output_C2: torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 2D\n",
      "shape of input x_1, torch.Size([64, 100, 48])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "shape of input x, torch.Size([64, 100, 6, 8])\n",
      "inputs_2D_N: torch.Size([64, 8, 6, 100])\n",
      " inputs_2D_N: torch.Size([64, 8, 6, 100])\n",
      "inputs_2D_N: torch.Size([64, 100, 6, 8])\n",
      "Shape after transpose: torch.Size([64, 8, 6, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 6, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 6, 100])\n",
      "Shape after pool: torch.Size([64, 64, 3, 50])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 3, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 3, 50])\n",
      "Shape after pool: torch.Size([64, 64, 1, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 1, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 1, 1600])\n",
      "Shape after dropout: torch.Size([64, 25, 1, 1600])\n",
      "necessary shape for fc2 is 64, 32\n",
      "Shape after dropout2: torch.Size([64, 25, 1, 64])\n",
      "Shape after flatten: torch.Size([64, 1600])\n",
      "Shape of model_3_output: torch.Size([64, 12800])\n",
      "Shape of model_3_output after cat: torch.Size([64, 14400])\n",
      "Shape of output_C1: torch.Size([64, 100, 3])\n",
      "shape of out_gru, torch.Size([64, 100, 3])\n",
      "shape of out_C2, torch.Size([64, 100, 3])\n",
      "shape of out_C1, torch.Size([64, 100, 3])\n",
      "output: torch.Size([64, 100, 3])\n",
      "data_features_1D shape: torch.Size([64, 100, 48])\n",
      "data_features_2D shape: torch.Size([64, 100, 6, 8])\n",
      "data_targets shape: torch.Size([64, 100, 3])\n",
      "##################################\n",
      "first passage through encoder gru\n",
      "input to encoder_gru, torch.Size([64, 100, 48])\n",
      "shape of out_1 after lstm1, torch.Size([64, 100, 256])\n",
      "shape of out_1 dropout1, torch.Size([64, 100, 256])\n",
      "shape of out_2 after lstm1, torch.Size([64, 100, 128])\n",
      "shape of out_2 dropout2, torch.Size([64, 100, 128])\n",
      "shape of out_2 after flatten, torch.Size([64, 12800])\n",
      "necessary shape for last linear layer = 128000, 600\n",
      "shape encoder_gru output, torch.Size([64, 300])\n",
      "shape encoder_gru output after view, torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 1D\n",
      "shape of input, torch.Size([64, 100, 48])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "Shape after transpose: torch.Size([64, 48, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 100])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 100])\n",
      "Shape after pool: torch.Size([64, 64, 50])\n",
      "Shape after conv3 relu: torch.Size([64, 64, 50])\n",
      "Shape after conv4 relu: torch.Size([64, 64, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 50])\n",
      "Shape after pool: torch.Size([64, 64, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 128])\n",
      "Shape after dropout1: torch.Size([64, 25, 128])\n",
      "necessary shape for fc2 is 25*64, 25*64\n",
      "Shape after fc2: torch.Size([64, 25, 1600])\n",
      "Shape after dropout2: torch.Size([64, 25, 1600])\n",
      "Shape after flatten: torch.Size([64, 40000])\n",
      "Shape of model_2_output: torch.Size([64, 12800])\n",
      "Shape of model_2_output_1 (concactenating last dimension of model_2_output and x): torch.Size([64, 52800])\n",
      "necessary shape for linear 528 * w, 6*w\n",
      "Shape of output_C2: torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 2D\n",
      "shape of input x_1, torch.Size([64, 100, 48])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "shape of input x, torch.Size([64, 100, 6, 8])\n",
      "inputs_2D_N: torch.Size([64, 8, 6, 100])\n",
      " inputs_2D_N: torch.Size([64, 8, 6, 100])\n",
      "inputs_2D_N: torch.Size([64, 100, 6, 8])\n",
      "Shape after transpose: torch.Size([64, 8, 6, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 6, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 6, 100])\n",
      "Shape after pool: torch.Size([64, 64, 3, 50])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 3, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 3, 50])\n",
      "Shape after pool: torch.Size([64, 64, 1, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 1, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 1, 1600])\n",
      "Shape after dropout: torch.Size([64, 25, 1, 1600])\n",
      "necessary shape for fc2 is 64, 32\n",
      "Shape after dropout2: torch.Size([64, 25, 1, 64])\n",
      "Shape after flatten: torch.Size([64, 1600])\n",
      "Shape of model_3_output: torch.Size([64, 12800])\n",
      "Shape of model_3_output after cat: torch.Size([64, 14400])\n",
      "Shape of output_C1: torch.Size([64, 100, 3])\n",
      "shape of out_gru, torch.Size([64, 100, 3])\n",
      "shape of out_C2, torch.Size([64, 100, 3])\n",
      "shape of out_C1, torch.Size([64, 100, 3])\n",
      "output: torch.Size([64, 100, 3])\n",
      "data_features_1D shape: torch.Size([64, 100, 48])\n",
      "data_features_2D shape: torch.Size([64, 100, 6, 8])\n",
      "data_targets shape: torch.Size([64, 100, 3])\n",
      "##################################\n",
      "first passage through encoder gru\n",
      "input to encoder_gru, torch.Size([64, 100, 48])\n",
      "shape of out_1 after lstm1, torch.Size([64, 100, 256])\n",
      "shape of out_1 dropout1, torch.Size([64, 100, 256])\n",
      "shape of out_2 after lstm1, torch.Size([64, 100, 128])\n",
      "shape of out_2 dropout2, torch.Size([64, 100, 128])\n",
      "shape of out_2 after flatten, torch.Size([64, 12800])\n",
      "necessary shape for last linear layer = 128000, 600\n",
      "shape encoder_gru output, torch.Size([64, 300])\n",
      "shape encoder_gru output after view, torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 1D\n",
      "shape of input, torch.Size([64, 100, 48])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "Shape after transpose: torch.Size([64, 48, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 100])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 100])\n",
      "Shape after pool: torch.Size([64, 64, 50])\n",
      "Shape after conv3 relu: torch.Size([64, 64, 50])\n",
      "Shape after conv4 relu: torch.Size([64, 64, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 50])\n",
      "Shape after pool: torch.Size([64, 64, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 128])\n",
      "Shape after dropout1: torch.Size([64, 25, 128])\n",
      "necessary shape for fc2 is 25*64, 25*64\n",
      "Shape after fc2: torch.Size([64, 25, 1600])\n",
      "Shape after dropout2: torch.Size([64, 25, 1600])\n",
      "Shape after flatten: torch.Size([64, 40000])\n",
      "Shape of model_2_output: torch.Size([64, 12800])\n",
      "Shape of model_2_output_1 (concactenating last dimension of model_2_output and x): torch.Size([64, 52800])\n",
      "necessary shape for linear 528 * w, 6*w\n",
      "Shape of output_C2: torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 2D\n",
      "shape of input x_1, torch.Size([64, 100, 48])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "shape of input x, torch.Size([64, 100, 6, 8])\n",
      "inputs_2D_N: torch.Size([64, 8, 6, 100])\n",
      " inputs_2D_N: torch.Size([64, 8, 6, 100])\n",
      "inputs_2D_N: torch.Size([64, 100, 6, 8])\n",
      "Shape after transpose: torch.Size([64, 8, 6, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 6, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 6, 100])\n",
      "Shape after pool: torch.Size([64, 64, 3, 50])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 3, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 3, 50])\n",
      "Shape after pool: torch.Size([64, 64, 1, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 1, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 1, 1600])\n",
      "Shape after dropout: torch.Size([64, 25, 1, 1600])\n",
      "necessary shape for fc2 is 64, 32\n",
      "Shape after dropout2: torch.Size([64, 25, 1, 64])\n",
      "Shape after flatten: torch.Size([64, 1600])\n",
      "Shape of model_3_output: torch.Size([64, 12800])\n",
      "Shape of model_3_output after cat: torch.Size([64, 14400])\n",
      "Shape of output_C1: torch.Size([64, 100, 3])\n",
      "shape of out_gru, torch.Size([64, 100, 3])\n",
      "shape of out_C2, torch.Size([64, 100, 3])\n",
      "shape of out_C1, torch.Size([64, 100, 3])\n",
      "output: torch.Size([64, 100, 3])\n",
      "data_features_1D shape: torch.Size([64, 100, 48])\n",
      "data_features_2D shape: torch.Size([64, 100, 6, 8])\n",
      "data_targets shape: torch.Size([64, 100, 3])\n",
      "##################################\n",
      "first passage through encoder gru\n",
      "input to encoder_gru, torch.Size([64, 100, 48])\n",
      "shape of out_1 after lstm1, torch.Size([64, 100, 256])\n",
      "shape of out_1 dropout1, torch.Size([64, 100, 256])\n",
      "shape of out_2 after lstm1, torch.Size([64, 100, 128])\n",
      "shape of out_2 dropout2, torch.Size([64, 100, 128])\n",
      "shape of out_2 after flatten, torch.Size([64, 12800])\n",
      "necessary shape for last linear layer = 128000, 600\n",
      "shape encoder_gru output, torch.Size([64, 300])\n",
      "shape encoder_gru output after view, torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 1D\n",
      "shape of input, torch.Size([64, 100, 48])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "Shape after transpose: torch.Size([64, 48, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 100])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 100])\n",
      "Shape after pool: torch.Size([64, 64, 50])\n",
      "Shape after conv3 relu: torch.Size([64, 64, 50])\n",
      "Shape after conv4 relu: torch.Size([64, 64, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 50])\n",
      "Shape after pool: torch.Size([64, 64, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 128])\n",
      "Shape after dropout1: torch.Size([64, 25, 128])\n",
      "necessary shape for fc2 is 25*64, 25*64\n",
      "Shape after fc2: torch.Size([64, 25, 1600])\n",
      "Shape after dropout2: torch.Size([64, 25, 1600])\n",
      "Shape after flatten: torch.Size([64, 40000])\n",
      "Shape of model_2_output: torch.Size([64, 12800])\n",
      "Shape of model_2_output_1 (concactenating last dimension of model_2_output and x): torch.Size([64, 52800])\n",
      "necessary shape for linear 528 * w, 6*w\n",
      "Shape of output_C2: torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 2D\n",
      "shape of input x_1, torch.Size([64, 100, 48])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "shape of input x, torch.Size([64, 100, 6, 8])\n",
      "inputs_2D_N: torch.Size([64, 8, 6, 100])\n",
      " inputs_2D_N: torch.Size([64, 8, 6, 100])\n",
      "inputs_2D_N: torch.Size([64, 100, 6, 8])\n",
      "Shape after transpose: torch.Size([64, 8, 6, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 6, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 6, 100])\n",
      "Shape after pool: torch.Size([64, 64, 3, 50])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 3, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 3, 50])\n",
      "Shape after pool: torch.Size([64, 64, 1, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 1, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 1, 1600])\n",
      "Shape after dropout: torch.Size([64, 25, 1, 1600])\n",
      "necessary shape for fc2 is 64, 32\n",
      "Shape after dropout2: torch.Size([64, 25, 1, 64])\n",
      "Shape after flatten: torch.Size([64, 1600])\n",
      "Shape of model_3_output: torch.Size([64, 12800])\n",
      "Shape of model_3_output after cat: torch.Size([64, 14400])\n",
      "Shape of output_C1: torch.Size([64, 100, 3])\n",
      "shape of out_gru, torch.Size([64, 100, 3])\n",
      "shape of out_C2, torch.Size([64, 100, 3])\n",
      "shape of out_C1, torch.Size([64, 100, 3])\n",
      "output: torch.Size([64, 100, 3])\n",
      "data_features_1D shape: torch.Size([64, 100, 48])\n",
      "data_features_2D shape: torch.Size([64, 100, 6, 8])\n",
      "data_targets shape: torch.Size([64, 100, 3])\n",
      "##################################\n",
      "first passage through encoder gru\n",
      "input to encoder_gru, torch.Size([64, 100, 48])\n",
      "shape of out_1 after lstm1, torch.Size([64, 100, 256])\n",
      "shape of out_1 dropout1, torch.Size([64, 100, 256])\n",
      "shape of out_2 after lstm1, torch.Size([64, 100, 128])\n",
      "shape of out_2 dropout2, torch.Size([64, 100, 128])\n",
      "shape of out_2 after flatten, torch.Size([64, 12800])\n",
      "necessary shape for last linear layer = 128000, 600\n",
      "shape encoder_gru output, torch.Size([64, 300])\n",
      "shape encoder_gru output after view, torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 1D\n",
      "shape of input, torch.Size([64, 100, 48])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "Shape after transpose: torch.Size([64, 48, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 100])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 100])\n",
      "Shape after pool: torch.Size([64, 64, 50])\n",
      "Shape after conv3 relu: torch.Size([64, 64, 50])\n",
      "Shape after conv4 relu: torch.Size([64, 64, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 50])\n",
      "Shape after pool: torch.Size([64, 64, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 128])\n",
      "Shape after dropout1: torch.Size([64, 25, 128])\n",
      "necessary shape for fc2 is 25*64, 25*64\n",
      "Shape after fc2: torch.Size([64, 25, 1600])\n",
      "Shape after dropout2: torch.Size([64, 25, 1600])\n",
      "Shape after flatten: torch.Size([64, 40000])\n",
      "Shape of model_2_output: torch.Size([64, 12800])\n",
      "Shape of model_2_output_1 (concactenating last dimension of model_2_output and x): torch.Size([64, 52800])\n",
      "necessary shape for linear 528 * w, 6*w\n",
      "Shape of output_C2: torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 2D\n",
      "shape of input x_1, torch.Size([64, 100, 48])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "shape of input x, torch.Size([64, 100, 6, 8])\n",
      "inputs_2D_N: torch.Size([64, 8, 6, 100])\n",
      " inputs_2D_N: torch.Size([64, 8, 6, 100])\n",
      "inputs_2D_N: torch.Size([64, 100, 6, 8])\n",
      "Shape after transpose: torch.Size([64, 8, 6, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 6, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 6, 100])\n",
      "Shape after pool: torch.Size([64, 64, 3, 50])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 3, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 3, 50])\n",
      "Shape after pool: torch.Size([64, 64, 1, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 1, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 1, 1600])\n",
      "Shape after dropout: torch.Size([64, 25, 1, 1600])\n",
      "necessary shape for fc2 is 64, 32\n",
      "Shape after dropout2: torch.Size([64, 25, 1, 64])\n",
      "Shape after flatten: torch.Size([64, 1600])\n",
      "Shape of model_3_output: torch.Size([64, 12800])\n",
      "Shape of model_3_output after cat: torch.Size([64, 14400])\n",
      "Shape of output_C1: torch.Size([64, 100, 3])\n",
      "shape of out_gru, torch.Size([64, 100, 3])\n",
      "shape of out_C2, torch.Size([64, 100, 3])\n",
      "shape of out_C1, torch.Size([64, 100, 3])\n",
      "output: torch.Size([64, 100, 3])\n",
      "data_features_1D shape: torch.Size([64, 100, 48])\n",
      "data_features_2D shape: torch.Size([64, 100, 6, 8])\n",
      "data_targets shape: torch.Size([64, 100, 3])\n",
      "##################################\n",
      "first passage through encoder gru\n",
      "input to encoder_gru, torch.Size([64, 100, 48])\n",
      "shape of out_1 after lstm1, torch.Size([64, 100, 256])\n",
      "shape of out_1 dropout1, torch.Size([64, 100, 256])\n",
      "shape of out_2 after lstm1, torch.Size([64, 100, 128])\n",
      "shape of out_2 dropout2, torch.Size([64, 100, 128])\n",
      "shape of out_2 after flatten, torch.Size([64, 12800])\n",
      "necessary shape for last linear layer = 128000, 600\n",
      "shape encoder_gru output, torch.Size([64, 300])\n",
      "shape encoder_gru output after view, torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 1D\n",
      "shape of input, torch.Size([64, 100, 48])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "Shape after transpose: torch.Size([64, 48, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 100])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 100])\n",
      "Shape after pool: torch.Size([64, 64, 50])\n",
      "Shape after conv3 relu: torch.Size([64, 64, 50])\n",
      "Shape after conv4 relu: torch.Size([64, 64, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 50])\n",
      "Shape after pool: torch.Size([64, 64, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 128])\n",
      "Shape after dropout1: torch.Size([64, 25, 128])\n",
      "necessary shape for fc2 is 25*64, 25*64\n",
      "Shape after fc2: torch.Size([64, 25, 1600])\n",
      "Shape after dropout2: torch.Size([64, 25, 1600])\n",
      "Shape after flatten: torch.Size([64, 40000])\n",
      "Shape of model_2_output: torch.Size([64, 12800])\n",
      "Shape of model_2_output_1 (concactenating last dimension of model_2_output and x): torch.Size([64, 52800])\n",
      "necessary shape for linear 528 * w, 6*w\n",
      "Shape of output_C2: torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 2D\n",
      "shape of input x_1, torch.Size([64, 100, 48])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "shape of input x, torch.Size([64, 100, 6, 8])\n",
      "inputs_2D_N: torch.Size([64, 8, 6, 100])\n",
      " inputs_2D_N: torch.Size([64, 8, 6, 100])\n",
      "inputs_2D_N: torch.Size([64, 100, 6, 8])\n",
      "Shape after transpose: torch.Size([64, 8, 6, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 6, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 6, 100])\n",
      "Shape after pool: torch.Size([64, 64, 3, 50])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 3, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 3, 50])\n",
      "Shape after pool: torch.Size([64, 64, 1, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 1, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 1, 1600])\n",
      "Shape after dropout: torch.Size([64, 25, 1, 1600])\n",
      "necessary shape for fc2 is 64, 32\n",
      "Shape after dropout2: torch.Size([64, 25, 1, 64])\n",
      "Shape after flatten: torch.Size([64, 1600])\n",
      "Shape of model_3_output: torch.Size([64, 12800])\n",
      "Shape of model_3_output after cat: torch.Size([64, 14400])\n",
      "Shape of output_C1: torch.Size([64, 100, 3])\n",
      "shape of out_gru, torch.Size([64, 100, 3])\n",
      "shape of out_C2, torch.Size([64, 100, 3])\n",
      "shape of out_C1, torch.Size([64, 100, 3])\n",
      "output: torch.Size([64, 100, 3])\n",
      "data_features_1D shape: torch.Size([64, 100, 48])\n",
      "data_features_2D shape: torch.Size([64, 100, 6, 8])\n",
      "data_targets shape: torch.Size([64, 100, 3])\n",
      "##################################\n",
      "first passage through encoder gru\n",
      "input to encoder_gru, torch.Size([64, 100, 48])\n",
      "shape of out_1 after lstm1, torch.Size([64, 100, 256])\n",
      "shape of out_1 dropout1, torch.Size([64, 100, 256])\n",
      "shape of out_2 after lstm1, torch.Size([64, 100, 128])\n",
      "shape of out_2 dropout2, torch.Size([64, 100, 128])\n",
      "shape of out_2 after flatten, torch.Size([64, 12800])\n",
      "necessary shape for last linear layer = 128000, 600\n",
      "shape encoder_gru output, torch.Size([64, 300])\n",
      "shape encoder_gru output after view, torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 1D\n",
      "shape of input, torch.Size([64, 100, 48])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "Shape after transpose: torch.Size([64, 48, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 100])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 100])\n",
      "Shape after pool: torch.Size([64, 64, 50])\n",
      "Shape after conv3 relu: torch.Size([64, 64, 50])\n",
      "Shape after conv4 relu: torch.Size([64, 64, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 50])\n",
      "Shape after pool: torch.Size([64, 64, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 128])\n",
      "Shape after dropout1: torch.Size([64, 25, 128])\n",
      "necessary shape for fc2 is 25*64, 25*64\n",
      "Shape after fc2: torch.Size([64, 25, 1600])\n",
      "Shape after dropout2: torch.Size([64, 25, 1600])\n",
      "Shape after flatten: torch.Size([64, 40000])\n",
      "Shape of model_2_output: torch.Size([64, 12800])\n",
      "Shape of model_2_output_1 (concactenating last dimension of model_2_output and x): torch.Size([64, 52800])\n",
      "necessary shape for linear 528 * w, 6*w\n",
      "Shape of output_C2: torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 2D\n",
      "shape of input x_1, torch.Size([64, 100, 48])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "shape of input x, torch.Size([64, 100, 6, 8])\n",
      "inputs_2D_N: torch.Size([64, 8, 6, 100])\n",
      " inputs_2D_N: torch.Size([64, 8, 6, 100])\n",
      "inputs_2D_N: torch.Size([64, 100, 6, 8])\n",
      "Shape after transpose: torch.Size([64, 8, 6, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 6, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 6, 100])\n",
      "Shape after pool: torch.Size([64, 64, 3, 50])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 3, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 3, 50])\n",
      "Shape after pool: torch.Size([64, 64, 1, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 1, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 1, 1600])\n",
      "Shape after dropout: torch.Size([64, 25, 1, 1600])\n",
      "necessary shape for fc2 is 64, 32\n",
      "Shape after dropout2: torch.Size([64, 25, 1, 64])\n",
      "Shape after flatten: torch.Size([64, 1600])\n",
      "Shape of model_3_output: torch.Size([64, 12800])\n",
      "Shape of model_3_output after cat: torch.Size([64, 14400])\n",
      "Shape of output_C1: torch.Size([64, 100, 3])\n",
      "shape of out_gru, torch.Size([64, 100, 3])\n",
      "shape of out_C2, torch.Size([64, 100, 3])\n",
      "shape of out_C1, torch.Size([64, 100, 3])\n",
      "output: torch.Size([64, 100, 3])\n",
      "data_features_1D shape: torch.Size([64, 100, 48])\n",
      "data_features_2D shape: torch.Size([64, 100, 6, 8])\n",
      "data_targets shape: torch.Size([64, 100, 3])\n",
      "##################################\n",
      "first passage through encoder gru\n",
      "input to encoder_gru, torch.Size([64, 100, 48])\n",
      "shape of out_1 after lstm1, torch.Size([64, 100, 256])\n",
      "shape of out_1 dropout1, torch.Size([64, 100, 256])\n",
      "shape of out_2 after lstm1, torch.Size([64, 100, 128])\n",
      "shape of out_2 dropout2, torch.Size([64, 100, 128])\n",
      "shape of out_2 after flatten, torch.Size([64, 12800])\n",
      "necessary shape for last linear layer = 128000, 600\n",
      "shape encoder_gru output, torch.Size([64, 300])\n",
      "shape encoder_gru output after view, torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 1D\n",
      "shape of input, torch.Size([64, 100, 48])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "Shape after transpose: torch.Size([64, 48, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 100])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 100])\n",
      "Shape after pool: torch.Size([64, 64, 50])\n",
      "Shape after conv3 relu: torch.Size([64, 64, 50])\n",
      "Shape after conv4 relu: torch.Size([64, 64, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 50])\n",
      "Shape after pool: torch.Size([64, 64, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 128])\n",
      "Shape after dropout1: torch.Size([64, 25, 128])\n",
      "necessary shape for fc2 is 25*64, 25*64\n",
      "Shape after fc2: torch.Size([64, 25, 1600])\n",
      "Shape after dropout2: torch.Size([64, 25, 1600])\n",
      "Shape after flatten: torch.Size([64, 40000])\n",
      "Shape of model_2_output: torch.Size([64, 12800])\n",
      "Shape of model_2_output_1 (concactenating last dimension of model_2_output and x): torch.Size([64, 52800])\n",
      "necessary shape for linear 528 * w, 6*w\n",
      "Shape of output_C2: torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 2D\n",
      "shape of input x_1, torch.Size([64, 100, 48])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "shape of input x, torch.Size([64, 100, 6, 8])\n",
      "inputs_2D_N: torch.Size([64, 8, 6, 100])\n",
      " inputs_2D_N: torch.Size([64, 8, 6, 100])\n",
      "inputs_2D_N: torch.Size([64, 100, 6, 8])\n",
      "Shape after transpose: torch.Size([64, 8, 6, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 6, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 6, 100])\n",
      "Shape after pool: torch.Size([64, 64, 3, 50])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 3, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 3, 50])\n",
      "Shape after pool: torch.Size([64, 64, 1, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 1, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 1, 1600])\n",
      "Shape after dropout: torch.Size([64, 25, 1, 1600])\n",
      "necessary shape for fc2 is 64, 32\n",
      "Shape after dropout2: torch.Size([64, 25, 1, 64])\n",
      "Shape after flatten: torch.Size([64, 1600])\n",
      "Shape of model_3_output: torch.Size([64, 12800])\n",
      "Shape of model_3_output after cat: torch.Size([64, 14400])\n",
      "Shape of output_C1: torch.Size([64, 100, 3])\n",
      "shape of out_gru, torch.Size([64, 100, 3])\n",
      "shape of out_C2, torch.Size([64, 100, 3])\n",
      "shape of out_C1, torch.Size([64, 100, 3])\n",
      "output: torch.Size([64, 100, 3])\n",
      "data_features_1D shape: torch.Size([64, 100, 48])\n",
      "data_features_2D shape: torch.Size([64, 100, 6, 8])\n",
      "data_targets shape: torch.Size([64, 100, 3])\n",
      "##################################\n",
      "first passage through encoder gru\n",
      "input to encoder_gru, torch.Size([64, 100, 48])\n",
      "shape of out_1 after lstm1, torch.Size([64, 100, 256])\n",
      "shape of out_1 dropout1, torch.Size([64, 100, 256])\n",
      "shape of out_2 after lstm1, torch.Size([64, 100, 128])\n",
      "shape of out_2 dropout2, torch.Size([64, 100, 128])\n",
      "shape of out_2 after flatten, torch.Size([64, 12800])\n",
      "necessary shape for last linear layer = 128000, 600\n",
      "shape encoder_gru output, torch.Size([64, 300])\n",
      "shape encoder_gru output after view, torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 1D\n",
      "shape of input, torch.Size([64, 100, 48])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "Shape after transpose: torch.Size([64, 48, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 100])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 100])\n",
      "Shape after pool: torch.Size([64, 64, 50])\n",
      "Shape after conv3 relu: torch.Size([64, 64, 50])\n",
      "Shape after conv4 relu: torch.Size([64, 64, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 50])\n",
      "Shape after pool: torch.Size([64, 64, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 128])\n",
      "Shape after dropout1: torch.Size([64, 25, 128])\n",
      "necessary shape for fc2 is 25*64, 25*64\n",
      "Shape after fc2: torch.Size([64, 25, 1600])\n",
      "Shape after dropout2: torch.Size([64, 25, 1600])\n",
      "Shape after flatten: torch.Size([64, 40000])\n",
      "Shape of model_2_output: torch.Size([64, 12800])\n",
      "Shape of model_2_output_1 (concactenating last dimension of model_2_output and x): torch.Size([64, 52800])\n",
      "necessary shape for linear 528 * w, 6*w\n",
      "Shape of output_C2: torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 2D\n",
      "shape of input x_1, torch.Size([64, 100, 48])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "shape of input x, torch.Size([64, 100, 6, 8])\n",
      "inputs_2D_N: torch.Size([64, 8, 6, 100])\n",
      " inputs_2D_N: torch.Size([64, 8, 6, 100])\n",
      "inputs_2D_N: torch.Size([64, 100, 6, 8])\n",
      "Shape after transpose: torch.Size([64, 8, 6, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 6, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 6, 100])\n",
      "Shape after pool: torch.Size([64, 64, 3, 50])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 3, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 3, 50])\n",
      "Shape after pool: torch.Size([64, 64, 1, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 1, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 1, 1600])\n",
      "Shape after dropout: torch.Size([64, 25, 1, 1600])\n",
      "necessary shape for fc2 is 64, 32\n",
      "Shape after dropout2: torch.Size([64, 25, 1, 64])\n",
      "Shape after flatten: torch.Size([64, 1600])\n",
      "Shape of model_3_output: torch.Size([64, 12800])\n",
      "Shape of model_3_output after cat: torch.Size([64, 14400])\n",
      "Shape of output_C1: torch.Size([64, 100, 3])\n",
      "shape of out_gru, torch.Size([64, 100, 3])\n",
      "shape of out_C2, torch.Size([64, 100, 3])\n",
      "shape of out_C1, torch.Size([64, 100, 3])\n",
      "output: torch.Size([64, 100, 3])\n",
      "data_features_1D shape: torch.Size([8, 100, 48])\n",
      "data_features_2D shape: torch.Size([8, 100, 6, 8])\n",
      "data_targets shape: torch.Size([8, 100, 3])\n",
      "##################################\n",
      "first passage through encoder gru\n",
      "input to encoder_gru, torch.Size([8, 100, 48])\n",
      "shape of out_1 after lstm1, torch.Size([8, 100, 256])\n",
      "shape of out_1 dropout1, torch.Size([8, 100, 256])\n",
      "shape of out_2 after lstm1, torch.Size([8, 100, 128])\n",
      "shape of out_2 dropout2, torch.Size([8, 100, 128])\n",
      "shape of out_2 after flatten, torch.Size([8, 12800])\n",
      "necessary shape for last linear layer = 128000, 600\n",
      "shape encoder_gru output, torch.Size([8, 300])\n",
      "shape encoder_gru output after view, torch.Size([8, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 1D\n",
      "shape of input, torch.Size([8, 100, 48])\n",
      "input_1D_N_1: torch.Size([8, 48, 100])\n",
      "input_1D_N_1: torch.Size([8, 48, 100])\n",
      "input_1D_N_1: torch.Size([8, 48, 100])\n",
      "Shape after transpose: torch.Size([8, 48, 100])\n",
      "Shape after conv1 relu: torch.Size([8, 64, 100])\n",
      "Shape after conv2 relu: torch.Size([8, 64, 100])\n",
      "Shape after BN_2: torch.Size([8, 64, 100])\n",
      "Shape after pool: torch.Size([8, 64, 50])\n",
      "Shape after conv3 relu: torch.Size([8, 64, 50])\n",
      "Shape after conv4 relu: torch.Size([8, 64, 50])\n",
      "Shape after BN_4: torch.Size([8, 64, 50])\n",
      "Shape after pool: torch.Size([8, 64, 25])\n",
      "Shape after transpose: torch.Size([8, 25, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([8, 25, 128])\n",
      "Shape after dropout1: torch.Size([8, 25, 128])\n",
      "necessary shape for fc2 is 25*64, 25*64\n",
      "Shape after fc2: torch.Size([8, 25, 1600])\n",
      "Shape after dropout2: torch.Size([8, 25, 1600])\n",
      "Shape after flatten: torch.Size([8, 40000])\n",
      "Shape of model_2_output: torch.Size([8, 12800])\n",
      "Shape of model_2_output_1 (concactenating last dimension of model_2_output and x): torch.Size([8, 52800])\n",
      "necessary shape for linear 528 * w, 6*w\n",
      "Shape of output_C2: torch.Size([8, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 2D\n",
      "shape of input x_1, torch.Size([8, 100, 48])\n",
      "input_1D_N_1: torch.Size([8, 48, 100])\n",
      "input_1D_N_1: torch.Size([8, 48, 100])\n",
      "input_1D_N_1: torch.Size([8, 48, 100])\n",
      "shape of input x, torch.Size([8, 100, 6, 8])\n",
      "inputs_2D_N: torch.Size([8, 8, 6, 100])\n",
      " inputs_2D_N: torch.Size([8, 8, 6, 100])\n",
      "inputs_2D_N: torch.Size([8, 100, 6, 8])\n",
      "Shape after transpose: torch.Size([8, 8, 6, 100])\n",
      "Shape after conv1 relu: torch.Size([8, 64, 6, 100])\n",
      "Shape after BN_2: torch.Size([8, 64, 6, 100])\n",
      "Shape after pool: torch.Size([8, 64, 3, 50])\n",
      "Shape after conv2 relu: torch.Size([8, 64, 3, 50])\n",
      "Shape after BN_4: torch.Size([8, 64, 3, 50])\n",
      "Shape after pool: torch.Size([8, 64, 1, 25])\n",
      "Shape after transpose: torch.Size([8, 25, 1, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([8, 25, 1, 1600])\n",
      "Shape after dropout: torch.Size([8, 25, 1, 1600])\n",
      "necessary shape for fc2 is 64, 32\n",
      "Shape after dropout2: torch.Size([8, 25, 1, 64])\n",
      "Shape after flatten: torch.Size([8, 1600])\n",
      "Shape of model_3_output: torch.Size([8, 12800])\n",
      "Shape of model_3_output after cat: torch.Size([8, 14400])\n",
      "Shape of output_C1: torch.Size([8, 100, 3])\n",
      "shape of out_gru, torch.Size([8, 100, 3])\n",
      "shape of out_C2, torch.Size([8, 100, 3])\n",
      "shape of out_C1, torch.Size([8, 100, 3])\n",
      "output: torch.Size([8, 100, 3])\n",
      "##################################\n",
      "first passage through encoder gru\n",
      "input to encoder_gru, torch.Size([64, 100, 48])\n",
      "shape of out_1 after lstm1, torch.Size([64, 100, 256])\n",
      "shape of out_1 dropout1, torch.Size([64, 100, 256])\n",
      "shape of out_2 after lstm1, torch.Size([64, 100, 128])\n",
      "shape of out_2 dropout2, torch.Size([64, 100, 128])\n",
      "shape of out_2 after flatten, torch.Size([64, 12800])\n",
      "necessary shape for last linear layer = 128000, 600\n",
      "shape encoder_gru output, torch.Size([64, 300])\n",
      "shape encoder_gru output after view, torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 1D\n",
      "shape of input, torch.Size([64, 100, 48])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "Shape after transpose: torch.Size([64, 48, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 100])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 100])\n",
      "Shape after pool: torch.Size([64, 64, 50])\n",
      "Shape after conv3 relu: torch.Size([64, 64, 50])\n",
      "Shape after conv4 relu: torch.Size([64, 64, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 50])\n",
      "Shape after pool: torch.Size([64, 64, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 128])\n",
      "Shape after dropout1: torch.Size([64, 25, 128])\n",
      "necessary shape for fc2 is 25*64, 25*64\n",
      "Shape after fc2: torch.Size([64, 25, 1600])\n",
      "Shape after dropout2: torch.Size([64, 25, 1600])\n",
      "Shape after flatten: torch.Size([64, 40000])\n",
      "Shape of model_2_output: torch.Size([64, 12800])\n",
      "Shape of model_2_output_1 (concactenating last dimension of model_2_output and x): torch.Size([64, 52800])\n",
      "necessary shape for linear 528 * w, 6*w\n",
      "Shape of output_C2: torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 2D\n",
      "shape of input x_1, torch.Size([64, 100, 48])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "shape of input x, torch.Size([64, 100, 6, 8])\n",
      "inputs_2D_N: torch.Size([64, 8, 6, 100])\n",
      " inputs_2D_N: torch.Size([64, 8, 6, 100])\n",
      "inputs_2D_N: torch.Size([64, 100, 6, 8])\n",
      "Shape after transpose: torch.Size([64, 8, 6, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 6, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 6, 100])\n",
      "Shape after pool: torch.Size([64, 64, 3, 50])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 3, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 3, 50])\n",
      "Shape after pool: torch.Size([64, 64, 1, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 1, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 1, 1600])\n",
      "Shape after dropout: torch.Size([64, 25, 1, 1600])\n",
      "necessary shape for fc2 is 64, 32\n",
      "Shape after dropout2: torch.Size([64, 25, 1, 64])\n",
      "Shape after flatten: torch.Size([64, 1600])\n",
      "Shape of model_3_output: torch.Size([64, 12800])\n",
      "Shape of model_3_output after cat: torch.Size([64, 14400])\n",
      "Shape of output_C1: torch.Size([64, 100, 3])\n",
      "shape of out_gru, torch.Size([64, 100, 3])\n",
      "shape of out_C2, torch.Size([64, 100, 3])\n",
      "shape of out_C1, torch.Size([64, 100, 3])\n",
      "##################################\n",
      "first passage through encoder gru\n",
      "input to encoder_gru, torch.Size([64, 100, 48])\n",
      "shape of out_1 after lstm1, torch.Size([64, 100, 256])\n",
      "shape of out_1 dropout1, torch.Size([64, 100, 256])\n",
      "shape of out_2 after lstm1, torch.Size([64, 100, 128])\n",
      "shape of out_2 dropout2, torch.Size([64, 100, 128])\n",
      "shape of out_2 after flatten, torch.Size([64, 12800])\n",
      "necessary shape for last linear layer = 128000, 600\n",
      "shape encoder_gru output, torch.Size([64, 300])\n",
      "shape encoder_gru output after view, torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 1D\n",
      "shape of input, torch.Size([64, 100, 48])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "Shape after transpose: torch.Size([64, 48, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 100])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 100])\n",
      "Shape after pool: torch.Size([64, 64, 50])\n",
      "Shape after conv3 relu: torch.Size([64, 64, 50])\n",
      "Shape after conv4 relu: torch.Size([64, 64, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 50])\n",
      "Shape after pool: torch.Size([64, 64, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 128])\n",
      "Shape after dropout1: torch.Size([64, 25, 128])\n",
      "necessary shape for fc2 is 25*64, 25*64\n",
      "Shape after fc2: torch.Size([64, 25, 1600])\n",
      "Shape after dropout2: torch.Size([64, 25, 1600])\n",
      "Shape after flatten: torch.Size([64, 40000])\n",
      "Shape of model_2_output: torch.Size([64, 12800])\n",
      "Shape of model_2_output_1 (concactenating last dimension of model_2_output and x): torch.Size([64, 52800])\n",
      "necessary shape for linear 528 * w, 6*w\n",
      "Shape of output_C2: torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 2D\n",
      "shape of input x_1, torch.Size([64, 100, 48])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "shape of input x, torch.Size([64, 100, 6, 8])\n",
      "inputs_2D_N: torch.Size([64, 8, 6, 100])\n",
      " inputs_2D_N: torch.Size([64, 8, 6, 100])\n",
      "inputs_2D_N: torch.Size([64, 100, 6, 8])\n",
      "Shape after transpose: torch.Size([64, 8, 6, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 6, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 6, 100])\n",
      "Shape after pool: torch.Size([64, 64, 3, 50])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 3, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 3, 50])\n",
      "Shape after pool: torch.Size([64, 64, 1, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 1, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 1, 1600])\n",
      "Shape after dropout: torch.Size([64, 25, 1, 1600])\n",
      "necessary shape for fc2 is 64, 32\n",
      "Shape after dropout2: torch.Size([64, 25, 1, 64])\n",
      "Shape after flatten: torch.Size([64, 1600])\n",
      "Shape of model_3_output: torch.Size([64, 12800])\n",
      "Shape of model_3_output after cat: torch.Size([64, 14400])\n",
      "Shape of output_C1: torch.Size([64, 100, 3])\n",
      "shape of out_gru, torch.Size([64, 100, 3])\n",
      "shape of out_C2, torch.Size([64, 100, 3])\n",
      "shape of out_C1, torch.Size([64, 100, 3])\n",
      "##################################\n",
      "first passage through encoder gru\n",
      "input to encoder_gru, torch.Size([64, 100, 48])\n",
      "shape of out_1 after lstm1, torch.Size([64, 100, 256])\n",
      "shape of out_1 dropout1, torch.Size([64, 100, 256])\n",
      "shape of out_2 after lstm1, torch.Size([64, 100, 128])\n",
      "shape of out_2 dropout2, torch.Size([64, 100, 128])\n",
      "shape of out_2 after flatten, torch.Size([64, 12800])\n",
      "necessary shape for last linear layer = 128000, 600\n",
      "shape encoder_gru output, torch.Size([64, 300])\n",
      "shape encoder_gru output after view, torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 1D\n",
      "shape of input, torch.Size([64, 100, 48])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "Shape after transpose: torch.Size([64, 48, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 100])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 100])\n",
      "Shape after pool: torch.Size([64, 64, 50])\n",
      "Shape after conv3 relu: torch.Size([64, 64, 50])\n",
      "Shape after conv4 relu: torch.Size([64, 64, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 50])\n",
      "Shape after pool: torch.Size([64, 64, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 128])\n",
      "Shape after dropout1: torch.Size([64, 25, 128])\n",
      "necessary shape for fc2 is 25*64, 25*64\n",
      "Shape after fc2: torch.Size([64, 25, 1600])\n",
      "Shape after dropout2: torch.Size([64, 25, 1600])\n",
      "Shape after flatten: torch.Size([64, 40000])\n",
      "Shape of model_2_output: torch.Size([64, 12800])\n",
      "Shape of model_2_output_1 (concactenating last dimension of model_2_output and x): torch.Size([64, 52800])\n",
      "necessary shape for linear 528 * w, 6*w\n",
      "Shape of output_C2: torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 2D\n",
      "shape of input x_1, torch.Size([64, 100, 48])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "shape of input x, torch.Size([64, 100, 6, 8])\n",
      "inputs_2D_N: torch.Size([64, 8, 6, 100])\n",
      " inputs_2D_N: torch.Size([64, 8, 6, 100])\n",
      "inputs_2D_N: torch.Size([64, 100, 6, 8])\n",
      "Shape after transpose: torch.Size([64, 8, 6, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 6, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 6, 100])\n",
      "Shape after pool: torch.Size([64, 64, 3, 50])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 3, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 3, 50])\n",
      "Shape after pool: torch.Size([64, 64, 1, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 1, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 1, 1600])\n",
      "Shape after dropout: torch.Size([64, 25, 1, 1600])\n",
      "necessary shape for fc2 is 64, 32\n",
      "Shape after dropout2: torch.Size([64, 25, 1, 64])\n",
      "Shape after flatten: torch.Size([64, 1600])\n",
      "Shape of model_3_output: torch.Size([64, 12800])\n",
      "Shape of model_3_output after cat: torch.Size([64, 14400])\n",
      "Shape of output_C1: torch.Size([64, 100, 3])\n",
      "shape of out_gru, torch.Size([64, 100, 3])\n",
      "shape of out_C2, torch.Size([64, 100, 3])\n",
      "shape of out_C1, torch.Size([64, 100, 3])\n",
      "##################################\n",
      "first passage through encoder gru\n",
      "input to encoder_gru, torch.Size([64, 100, 48])\n",
      "shape of out_1 after lstm1, torch.Size([64, 100, 256])\n",
      "shape of out_1 dropout1, torch.Size([64, 100, 256])\n",
      "shape of out_2 after lstm1, torch.Size([64, 100, 128])\n",
      "shape of out_2 dropout2, torch.Size([64, 100, 128])\n",
      "shape of out_2 after flatten, torch.Size([64, 12800])\n",
      "necessary shape for last linear layer = 128000, 600\n",
      "shape encoder_gru output, torch.Size([64, 300])\n",
      "shape encoder_gru output after view, torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 1D\n",
      "shape of input, torch.Size([64, 100, 48])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "Shape after transpose: torch.Size([64, 48, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 100])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 100])\n",
      "Shape after pool: torch.Size([64, 64, 50])\n",
      "Shape after conv3 relu: torch.Size([64, 64, 50])\n",
      "Shape after conv4 relu: torch.Size([64, 64, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 50])\n",
      "Shape after pool: torch.Size([64, 64, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 128])\n",
      "Shape after dropout1: torch.Size([64, 25, 128])\n",
      "necessary shape for fc2 is 25*64, 25*64\n",
      "Shape after fc2: torch.Size([64, 25, 1600])\n",
      "Shape after dropout2: torch.Size([64, 25, 1600])\n",
      "Shape after flatten: torch.Size([64, 40000])\n",
      "Shape of model_2_output: torch.Size([64, 12800])\n",
      "Shape of model_2_output_1 (concactenating last dimension of model_2_output and x): torch.Size([64, 52800])\n",
      "necessary shape for linear 528 * w, 6*w\n",
      "Shape of output_C2: torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 2D\n",
      "shape of input x_1, torch.Size([64, 100, 48])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "shape of input x, torch.Size([64, 100, 6, 8])\n",
      "inputs_2D_N: torch.Size([64, 8, 6, 100])\n",
      " inputs_2D_N: torch.Size([64, 8, 6, 100])\n",
      "inputs_2D_N: torch.Size([64, 100, 6, 8])\n",
      "Shape after transpose: torch.Size([64, 8, 6, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 6, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 6, 100])\n",
      "Shape after pool: torch.Size([64, 64, 3, 50])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 3, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 3, 50])\n",
      "Shape after pool: torch.Size([64, 64, 1, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 1, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 1, 1600])\n",
      "Shape after dropout: torch.Size([64, 25, 1, 1600])\n",
      "necessary shape for fc2 is 64, 32\n",
      "Shape after dropout2: torch.Size([64, 25, 1, 64])\n",
      "Shape after flatten: torch.Size([64, 1600])\n",
      "Shape of model_3_output: torch.Size([64, 12800])\n",
      "Shape of model_3_output after cat: torch.Size([64, 14400])\n",
      "Shape of output_C1: torch.Size([64, 100, 3])\n",
      "shape of out_gru, torch.Size([64, 100, 3])\n",
      "shape of out_C2, torch.Size([64, 100, 3])\n",
      "shape of out_C1, torch.Size([64, 100, 3])\n",
      "##################################\n",
      "first passage through encoder gru\n",
      "input to encoder_gru, torch.Size([2, 100, 48])\n",
      "shape of out_1 after lstm1, torch.Size([2, 100, 256])\n",
      "shape of out_1 dropout1, torch.Size([2, 100, 256])\n",
      "shape of out_2 after lstm1, torch.Size([2, 100, 128])\n",
      "shape of out_2 dropout2, torch.Size([2, 100, 128])\n",
      "shape of out_2 after flatten, torch.Size([2, 12800])\n",
      "necessary shape for last linear layer = 128000, 600\n",
      "shape encoder_gru output, torch.Size([2, 300])\n",
      "shape encoder_gru output after view, torch.Size([2, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 1D\n",
      "shape of input, torch.Size([2, 100, 48])\n",
      "input_1D_N_1: torch.Size([2, 48, 100])\n",
      "input_1D_N_1: torch.Size([2, 48, 100])\n",
      "input_1D_N_1: torch.Size([2, 48, 100])\n",
      "Shape after transpose: torch.Size([2, 48, 100])\n",
      "Shape after conv1 relu: torch.Size([2, 64, 100])\n",
      "Shape after conv2 relu: torch.Size([2, 64, 100])\n",
      "Shape after BN_2: torch.Size([2, 64, 100])\n",
      "Shape after pool: torch.Size([2, 64, 50])\n",
      "Shape after conv3 relu: torch.Size([2, 64, 50])\n",
      "Shape after conv4 relu: torch.Size([2, 64, 50])\n",
      "Shape after BN_4: torch.Size([2, 64, 50])\n",
      "Shape after pool: torch.Size([2, 64, 25])\n",
      "Shape after transpose: torch.Size([2, 25, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([2, 25, 128])\n",
      "Shape after dropout1: torch.Size([2, 25, 128])\n",
      "necessary shape for fc2 is 25*64, 25*64\n",
      "Shape after fc2: torch.Size([2, 25, 1600])\n",
      "Shape after dropout2: torch.Size([2, 25, 1600])\n",
      "Shape after flatten: torch.Size([2, 40000])\n",
      "Shape of model_2_output: torch.Size([2, 12800])\n",
      "Shape of model_2_output_1 (concactenating last dimension of model_2_output and x): torch.Size([2, 52800])\n",
      "necessary shape for linear 528 * w, 6*w\n",
      "Shape of output_C2: torch.Size([2, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 2D\n",
      "shape of input x_1, torch.Size([2, 100, 48])\n",
      "input_1D_N_1: torch.Size([2, 48, 100])\n",
      "input_1D_N_1: torch.Size([2, 48, 100])\n",
      "input_1D_N_1: torch.Size([2, 48, 100])\n",
      "shape of input x, torch.Size([2, 100, 6, 8])\n",
      "inputs_2D_N: torch.Size([2, 8, 6, 100])\n",
      " inputs_2D_N: torch.Size([2, 8, 6, 100])\n",
      "inputs_2D_N: torch.Size([2, 100, 6, 8])\n",
      "Shape after transpose: torch.Size([2, 8, 6, 100])\n",
      "Shape after conv1 relu: torch.Size([2, 64, 6, 100])\n",
      "Shape after BN_2: torch.Size([2, 64, 6, 100])\n",
      "Shape after pool: torch.Size([2, 64, 3, 50])\n",
      "Shape after conv2 relu: torch.Size([2, 64, 3, 50])\n",
      "Shape after BN_4: torch.Size([2, 64, 3, 50])\n",
      "Shape after pool: torch.Size([2, 64, 1, 25])\n",
      "Shape after transpose: torch.Size([2, 25, 1, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([2, 25, 1, 1600])\n",
      "Shape after dropout: torch.Size([2, 25, 1, 1600])\n",
      "necessary shape for fc2 is 64, 32\n",
      "Shape after dropout2: torch.Size([2, 25, 1, 64])\n",
      "Shape after flatten: torch.Size([2, 1600])\n",
      "Shape of model_3_output: torch.Size([2, 12800])\n",
      "Shape of model_3_output after cat: torch.Size([2, 14400])\n",
      "Shape of output_C1: torch.Size([2, 100, 3])\n",
      "shape of out_gru, torch.Size([2, 100, 3])\n",
      "shape of out_C2, torch.Size([2, 100, 3])\n",
      "shape of out_C1, torch.Size([2, 100, 3])\n",
      "Epoch: 1, time: 33.9421, Training Loss: 22.0355,  Validation loss: 12.8581\n",
      "Training time: 34.634392738342285 seconds\n"
     ]
    }
   ],
   "source": [
    "k_1=0\n",
    "k_2=48\n",
    "k_3=0\n",
    "k_4=8\n",
    "\n",
    "lr = 0.001\n",
    "model = teacher(k_2-k_1,k_4-k_3,100)\n",
    "\n",
    "gait_Net_teacher = train_kinematics(device, train_loader, val_loader, lr,num_epoch,model,\"../../datacollection/vicon/subject_1/test/test.pth\",k_1, k_2, k_3, k_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################\n",
      "first passage through encoder gru\n",
      "input to encoder_gru, torch.Size([64, 100, 48])\n",
      "shape of out_1 after lstm1, torch.Size([64, 100, 256])\n",
      "shape of out_1 dropout1, torch.Size([64, 100, 256])\n",
      "shape of out_2 after lstm1, torch.Size([64, 100, 128])\n",
      "shape of out_2 dropout2, torch.Size([64, 100, 128])\n",
      "shape of out_2 after flatten, torch.Size([64, 12800])\n",
      "necessary shape for last linear layer = 128000, 600\n",
      "shape encoder_gru output, torch.Size([64, 300])\n",
      "shape encoder_gru output after view, torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 1D\n",
      "shape of input, torch.Size([64, 100, 48])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "Shape after transpose: torch.Size([64, 48, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 100])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 100])\n",
      "Shape after pool: torch.Size([64, 64, 50])\n",
      "Shape after conv3 relu: torch.Size([64, 64, 50])\n",
      "Shape after conv4 relu: torch.Size([64, 64, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 50])\n",
      "Shape after pool: torch.Size([64, 64, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 128])\n",
      "Shape after dropout1: torch.Size([64, 25, 128])\n",
      "necessary shape for fc2 is 25*64, 25*64\n",
      "Shape after fc2: torch.Size([64, 25, 1600])\n",
      "Shape after dropout2: torch.Size([64, 25, 1600])\n",
      "Shape after flatten: torch.Size([64, 40000])\n",
      "Shape of model_2_output: torch.Size([64, 12800])\n",
      "Shape of model_2_output_1 (concactenating last dimension of model_2_output and x): torch.Size([64, 52800])\n",
      "necessary shape for linear 528 * w, 6*w\n",
      "Shape of output_C2: torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 2D\n",
      "shape of input x_1, torch.Size([64, 100, 48])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "shape of input x, torch.Size([64, 100, 6, 8])\n",
      "inputs_2D_N: torch.Size([64, 8, 6, 100])\n",
      " inputs_2D_N: torch.Size([64, 8, 6, 100])\n",
      "inputs_2D_N: torch.Size([64, 100, 6, 8])\n",
      "Shape after transpose: torch.Size([64, 8, 6, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 6, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 6, 100])\n",
      "Shape after pool: torch.Size([64, 64, 3, 50])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 3, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 3, 50])\n",
      "Shape after pool: torch.Size([64, 64, 1, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 1, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 1, 1600])\n",
      "Shape after dropout: torch.Size([64, 25, 1, 1600])\n",
      "necessary shape for fc2 is 64, 32\n",
      "Shape after dropout2: torch.Size([64, 25, 1, 64])\n",
      "Shape after flatten: torch.Size([64, 1600])\n",
      "Shape of model_3_output: torch.Size([64, 12800])\n",
      "Shape of model_3_output after cat: torch.Size([64, 14400])\n",
      "Shape of output_C1: torch.Size([64, 100, 3])\n",
      "shape of out_gru, torch.Size([64, 100, 3])\n",
      "shape of out_C2, torch.Size([64, 100, 3])\n",
      "shape of out_C1, torch.Size([64, 100, 3])\n",
      "##################################\n",
      "first passage through encoder gru\n",
      "input to encoder_gru, torch.Size([64, 100, 48])\n",
      "shape of out_1 after lstm1, torch.Size([64, 100, 256])\n",
      "shape of out_1 dropout1, torch.Size([64, 100, 256])\n",
      "shape of out_2 after lstm1, torch.Size([64, 100, 128])\n",
      "shape of out_2 dropout2, torch.Size([64, 100, 128])\n",
      "shape of out_2 after flatten, torch.Size([64, 12800])\n",
      "necessary shape for last linear layer = 128000, 600\n",
      "shape encoder_gru output, torch.Size([64, 300])\n",
      "shape encoder_gru output after view, torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 1D\n",
      "shape of input, torch.Size([64, 100, 48])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "Shape after transpose: torch.Size([64, 48, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 100])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 100])\n",
      "Shape after pool: torch.Size([64, 64, 50])\n",
      "Shape after conv3 relu: torch.Size([64, 64, 50])\n",
      "Shape after conv4 relu: torch.Size([64, 64, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 50])\n",
      "Shape after pool: torch.Size([64, 64, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 128])\n",
      "Shape after dropout1: torch.Size([64, 25, 128])\n",
      "necessary shape for fc2 is 25*64, 25*64\n",
      "Shape after fc2: torch.Size([64, 25, 1600])\n",
      "Shape after dropout2: torch.Size([64, 25, 1600])\n",
      "Shape after flatten: torch.Size([64, 40000])\n",
      "Shape of model_2_output: torch.Size([64, 12800])\n",
      "Shape of model_2_output_1 (concactenating last dimension of model_2_output and x): torch.Size([64, 52800])\n",
      "necessary shape for linear 528 * w, 6*w\n",
      "Shape of output_C2: torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 2D\n",
      "shape of input x_1, torch.Size([64, 100, 48])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "shape of input x, torch.Size([64, 100, 6, 8])\n",
      "inputs_2D_N: torch.Size([64, 8, 6, 100])\n",
      " inputs_2D_N: torch.Size([64, 8, 6, 100])\n",
      "inputs_2D_N: torch.Size([64, 100, 6, 8])\n",
      "Shape after transpose: torch.Size([64, 8, 6, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 6, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 6, 100])\n",
      "Shape after pool: torch.Size([64, 64, 3, 50])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 3, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 3, 50])\n",
      "Shape after pool: torch.Size([64, 64, 1, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 1, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 1, 1600])\n",
      "Shape after dropout: torch.Size([64, 25, 1, 1600])\n",
      "necessary shape for fc2 is 64, 32\n",
      "Shape after dropout2: torch.Size([64, 25, 1, 64])\n",
      "Shape after flatten: torch.Size([64, 1600])\n",
      "Shape of model_3_output: torch.Size([64, 12800])\n",
      "Shape of model_3_output after cat: torch.Size([64, 14400])\n",
      "Shape of output_C1: torch.Size([64, 100, 3])\n",
      "shape of out_gru, torch.Size([64, 100, 3])\n",
      "shape of out_C2, torch.Size([64, 100, 3])\n",
      "shape of out_C1, torch.Size([64, 100, 3])\n",
      "##################################\n",
      "first passage through encoder gru\n",
      "input to encoder_gru, torch.Size([64, 100, 48])\n",
      "shape of out_1 after lstm1, torch.Size([64, 100, 256])\n",
      "shape of out_1 dropout1, torch.Size([64, 100, 256])\n",
      "shape of out_2 after lstm1, torch.Size([64, 100, 128])\n",
      "shape of out_2 dropout2, torch.Size([64, 100, 128])\n",
      "shape of out_2 after flatten, torch.Size([64, 12800])\n",
      "necessary shape for last linear layer = 128000, 600\n",
      "shape encoder_gru output, torch.Size([64, 300])\n",
      "shape encoder_gru output after view, torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 1D\n",
      "shape of input, torch.Size([64, 100, 48])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "Shape after transpose: torch.Size([64, 48, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 100])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 100])\n",
      "Shape after pool: torch.Size([64, 64, 50])\n",
      "Shape after conv3 relu: torch.Size([64, 64, 50])\n",
      "Shape after conv4 relu: torch.Size([64, 64, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 50])\n",
      "Shape after pool: torch.Size([64, 64, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 128])\n",
      "Shape after dropout1: torch.Size([64, 25, 128])\n",
      "necessary shape for fc2 is 25*64, 25*64\n",
      "Shape after fc2: torch.Size([64, 25, 1600])\n",
      "Shape after dropout2: torch.Size([64, 25, 1600])\n",
      "Shape after flatten: torch.Size([64, 40000])\n",
      "Shape of model_2_output: torch.Size([64, 12800])\n",
      "Shape of model_2_output_1 (concactenating last dimension of model_2_output and x): torch.Size([64, 52800])\n",
      "necessary shape for linear 528 * w, 6*w\n",
      "Shape of output_C2: torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 2D\n",
      "shape of input x_1, torch.Size([64, 100, 48])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "shape of input x, torch.Size([64, 100, 6, 8])\n",
      "inputs_2D_N: torch.Size([64, 8, 6, 100])\n",
      " inputs_2D_N: torch.Size([64, 8, 6, 100])\n",
      "inputs_2D_N: torch.Size([64, 100, 6, 8])\n",
      "Shape after transpose: torch.Size([64, 8, 6, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 6, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 6, 100])\n",
      "Shape after pool: torch.Size([64, 64, 3, 50])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 3, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 3, 50])\n",
      "Shape after pool: torch.Size([64, 64, 1, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 1, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 1, 1600])\n",
      "Shape after dropout: torch.Size([64, 25, 1, 1600])\n",
      "necessary shape for fc2 is 64, 32\n",
      "Shape after dropout2: torch.Size([64, 25, 1, 64])\n",
      "Shape after flatten: torch.Size([64, 1600])\n",
      "Shape of model_3_output: torch.Size([64, 12800])\n",
      "Shape of model_3_output after cat: torch.Size([64, 14400])\n",
      "Shape of output_C1: torch.Size([64, 100, 3])\n",
      "shape of out_gru, torch.Size([64, 100, 3])\n",
      "shape of out_C2, torch.Size([64, 100, 3])\n",
      "shape of out_C1, torch.Size([64, 100, 3])\n",
      "##################################\n",
      "first passage through encoder gru\n",
      "input to encoder_gru, torch.Size([64, 100, 48])\n",
      "shape of out_1 after lstm1, torch.Size([64, 100, 256])\n",
      "shape of out_1 dropout1, torch.Size([64, 100, 256])\n",
      "shape of out_2 after lstm1, torch.Size([64, 100, 128])\n",
      "shape of out_2 dropout2, torch.Size([64, 100, 128])\n",
      "shape of out_2 after flatten, torch.Size([64, 12800])\n",
      "necessary shape for last linear layer = 128000, 600\n",
      "shape encoder_gru output, torch.Size([64, 300])\n",
      "shape encoder_gru output after view, torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 1D\n",
      "shape of input, torch.Size([64, 100, 48])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "Shape after transpose: torch.Size([64, 48, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 100])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 100])\n",
      "Shape after pool: torch.Size([64, 64, 50])\n",
      "Shape after conv3 relu: torch.Size([64, 64, 50])\n",
      "Shape after conv4 relu: torch.Size([64, 64, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 50])\n",
      "Shape after pool: torch.Size([64, 64, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 128])\n",
      "Shape after dropout1: torch.Size([64, 25, 128])\n",
      "necessary shape for fc2 is 25*64, 25*64\n",
      "Shape after fc2: torch.Size([64, 25, 1600])\n",
      "Shape after dropout2: torch.Size([64, 25, 1600])\n",
      "Shape after flatten: torch.Size([64, 40000])\n",
      "Shape of model_2_output: torch.Size([64, 12800])\n",
      "Shape of model_2_output_1 (concactenating last dimension of model_2_output and x): torch.Size([64, 52800])\n",
      "necessary shape for linear 528 * w, 6*w\n",
      "Shape of output_C2: torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 2D\n",
      "shape of input x_1, torch.Size([64, 100, 48])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "shape of input x, torch.Size([64, 100, 6, 8])\n",
      "inputs_2D_N: torch.Size([64, 8, 6, 100])\n",
      " inputs_2D_N: torch.Size([64, 8, 6, 100])\n",
      "inputs_2D_N: torch.Size([64, 100, 6, 8])\n",
      "Shape after transpose: torch.Size([64, 8, 6, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 6, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 6, 100])\n",
      "Shape after pool: torch.Size([64, 64, 3, 50])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 3, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 3, 50])\n",
      "Shape after pool: torch.Size([64, 64, 1, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 1, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 1, 1600])\n",
      "Shape after dropout: torch.Size([64, 25, 1, 1600])\n",
      "necessary shape for fc2 is 64, 32\n",
      "Shape after dropout2: torch.Size([64, 25, 1, 64])\n",
      "Shape after flatten: torch.Size([64, 1600])\n",
      "Shape of model_3_output: torch.Size([64, 12800])\n",
      "Shape of model_3_output after cat: torch.Size([64, 14400])\n",
      "Shape of output_C1: torch.Size([64, 100, 3])\n",
      "shape of out_gru, torch.Size([64, 100, 3])\n",
      "shape of out_C2, torch.Size([64, 100, 3])\n",
      "shape of out_C1, torch.Size([64, 100, 3])\n",
      "##################################\n",
      "first passage through encoder gru\n",
      "input to encoder_gru, torch.Size([64, 100, 48])\n",
      "shape of out_1 after lstm1, torch.Size([64, 100, 256])\n",
      "shape of out_1 dropout1, torch.Size([64, 100, 256])\n",
      "shape of out_2 after lstm1, torch.Size([64, 100, 128])\n",
      "shape of out_2 dropout2, torch.Size([64, 100, 128])\n",
      "shape of out_2 after flatten, torch.Size([64, 12800])\n",
      "necessary shape for last linear layer = 128000, 600\n",
      "shape encoder_gru output, torch.Size([64, 300])\n",
      "shape encoder_gru output after view, torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 1D\n",
      "shape of input, torch.Size([64, 100, 48])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "Shape after transpose: torch.Size([64, 48, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 100])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 100])\n",
      "Shape after pool: torch.Size([64, 64, 50])\n",
      "Shape after conv3 relu: torch.Size([64, 64, 50])\n",
      "Shape after conv4 relu: torch.Size([64, 64, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 50])\n",
      "Shape after pool: torch.Size([64, 64, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 128])\n",
      "Shape after dropout1: torch.Size([64, 25, 128])\n",
      "necessary shape for fc2 is 25*64, 25*64\n",
      "Shape after fc2: torch.Size([64, 25, 1600])\n",
      "Shape after dropout2: torch.Size([64, 25, 1600])\n",
      "Shape after flatten: torch.Size([64, 40000])\n",
      "Shape of model_2_output: torch.Size([64, 12800])\n",
      "Shape of model_2_output_1 (concactenating last dimension of model_2_output and x): torch.Size([64, 52800])\n",
      "necessary shape for linear 528 * w, 6*w\n",
      "Shape of output_C2: torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 2D\n",
      "shape of input x_1, torch.Size([64, 100, 48])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "shape of input x, torch.Size([64, 100, 6, 8])\n",
      "inputs_2D_N: torch.Size([64, 8, 6, 100])\n",
      " inputs_2D_N: torch.Size([64, 8, 6, 100])\n",
      "inputs_2D_N: torch.Size([64, 100, 6, 8])\n",
      "Shape after transpose: torch.Size([64, 8, 6, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 6, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 6, 100])\n",
      "Shape after pool: torch.Size([64, 64, 3, 50])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 3, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 3, 50])\n",
      "Shape after pool: torch.Size([64, 64, 1, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 1, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 1, 1600])\n",
      "Shape after dropout: torch.Size([64, 25, 1, 1600])\n",
      "necessary shape for fc2 is 64, 32\n",
      "Shape after dropout2: torch.Size([64, 25, 1, 64])\n",
      "Shape after flatten: torch.Size([64, 1600])\n",
      "Shape of model_3_output: torch.Size([64, 12800])\n",
      "Shape of model_3_output after cat: torch.Size([64, 14400])\n",
      "Shape of output_C1: torch.Size([64, 100, 3])\n",
      "shape of out_gru, torch.Size([64, 100, 3])\n",
      "shape of out_C2, torch.Size([64, 100, 3])\n",
      "shape of out_C1, torch.Size([64, 100, 3])\n",
      "##################################\n",
      "first passage through encoder gru\n",
      "input to encoder_gru, torch.Size([64, 100, 48])\n",
      "shape of out_1 after lstm1, torch.Size([64, 100, 256])\n",
      "shape of out_1 dropout1, torch.Size([64, 100, 256])\n",
      "shape of out_2 after lstm1, torch.Size([64, 100, 128])\n",
      "shape of out_2 dropout2, torch.Size([64, 100, 128])\n",
      "shape of out_2 after flatten, torch.Size([64, 12800])\n",
      "necessary shape for last linear layer = 128000, 600\n",
      "shape encoder_gru output, torch.Size([64, 300])\n",
      "shape encoder_gru output after view, torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 1D\n",
      "shape of input, torch.Size([64, 100, 48])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "Shape after transpose: torch.Size([64, 48, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 100])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 100])\n",
      "Shape after pool: torch.Size([64, 64, 50])\n",
      "Shape after conv3 relu: torch.Size([64, 64, 50])\n",
      "Shape after conv4 relu: torch.Size([64, 64, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 50])\n",
      "Shape after pool: torch.Size([64, 64, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 128])\n",
      "Shape after dropout1: torch.Size([64, 25, 128])\n",
      "necessary shape for fc2 is 25*64, 25*64\n",
      "Shape after fc2: torch.Size([64, 25, 1600])\n",
      "Shape after dropout2: torch.Size([64, 25, 1600])\n",
      "Shape after flatten: torch.Size([64, 40000])\n",
      "Shape of model_2_output: torch.Size([64, 12800])\n",
      "Shape of model_2_output_1 (concactenating last dimension of model_2_output and x): torch.Size([64, 52800])\n",
      "necessary shape for linear 528 * w, 6*w\n",
      "Shape of output_C2: torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 2D\n",
      "shape of input x_1, torch.Size([64, 100, 48])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "shape of input x, torch.Size([64, 100, 6, 8])\n",
      "inputs_2D_N: torch.Size([64, 8, 6, 100])\n",
      " inputs_2D_N: torch.Size([64, 8, 6, 100])\n",
      "inputs_2D_N: torch.Size([64, 100, 6, 8])\n",
      "Shape after transpose: torch.Size([64, 8, 6, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 6, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 6, 100])\n",
      "Shape after pool: torch.Size([64, 64, 3, 50])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 3, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 3, 50])\n",
      "Shape after pool: torch.Size([64, 64, 1, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 1, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 1, 1600])\n",
      "Shape after dropout: torch.Size([64, 25, 1, 1600])\n",
      "necessary shape for fc2 is 64, 32\n",
      "Shape after dropout2: torch.Size([64, 25, 1, 64])\n",
      "Shape after flatten: torch.Size([64, 1600])\n",
      "Shape of model_3_output: torch.Size([64, 12800])\n",
      "Shape of model_3_output after cat: torch.Size([64, 14400])\n",
      "Shape of output_C1: torch.Size([64, 100, 3])\n",
      "shape of out_gru, torch.Size([64, 100, 3])\n",
      "shape of out_C2, torch.Size([64, 100, 3])\n",
      "shape of out_C1, torch.Size([64, 100, 3])\n",
      "##################################\n",
      "first passage through encoder gru\n",
      "input to encoder_gru, torch.Size([64, 100, 48])\n",
      "shape of out_1 after lstm1, torch.Size([64, 100, 256])\n",
      "shape of out_1 dropout1, torch.Size([64, 100, 256])\n",
      "shape of out_2 after lstm1, torch.Size([64, 100, 128])\n",
      "shape of out_2 dropout2, torch.Size([64, 100, 128])\n",
      "shape of out_2 after flatten, torch.Size([64, 12800])\n",
      "necessary shape for last linear layer = 128000, 600\n",
      "shape encoder_gru output, torch.Size([64, 300])\n",
      "shape encoder_gru output after view, torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 1D\n",
      "shape of input, torch.Size([64, 100, 48])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "Shape after transpose: torch.Size([64, 48, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 100])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 100])\n",
      "Shape after pool: torch.Size([64, 64, 50])\n",
      "Shape after conv3 relu: torch.Size([64, 64, 50])\n",
      "Shape after conv4 relu: torch.Size([64, 64, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 50])\n",
      "Shape after pool: torch.Size([64, 64, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 128])\n",
      "Shape after dropout1: torch.Size([64, 25, 128])\n",
      "necessary shape for fc2 is 25*64, 25*64\n",
      "Shape after fc2: torch.Size([64, 25, 1600])\n",
      "Shape after dropout2: torch.Size([64, 25, 1600])\n",
      "Shape after flatten: torch.Size([64, 40000])\n",
      "Shape of model_2_output: torch.Size([64, 12800])\n",
      "Shape of model_2_output_1 (concactenating last dimension of model_2_output and x): torch.Size([64, 52800])\n",
      "necessary shape for linear 528 * w, 6*w\n",
      "Shape of output_C2: torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 2D\n",
      "shape of input x_1, torch.Size([64, 100, 48])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "shape of input x, torch.Size([64, 100, 6, 8])\n",
      "inputs_2D_N: torch.Size([64, 8, 6, 100])\n",
      " inputs_2D_N: torch.Size([64, 8, 6, 100])\n",
      "inputs_2D_N: torch.Size([64, 100, 6, 8])\n",
      "Shape after transpose: torch.Size([64, 8, 6, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 6, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 6, 100])\n",
      "Shape after pool: torch.Size([64, 64, 3, 50])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 3, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 3, 50])\n",
      "Shape after pool: torch.Size([64, 64, 1, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 1, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 1, 1600])\n",
      "Shape after dropout: torch.Size([64, 25, 1, 1600])\n",
      "necessary shape for fc2 is 64, 32\n",
      "Shape after dropout2: torch.Size([64, 25, 1, 64])\n",
      "Shape after flatten: torch.Size([64, 1600])\n",
      "Shape of model_3_output: torch.Size([64, 12800])\n",
      "Shape of model_3_output after cat: torch.Size([64, 14400])\n",
      "Shape of output_C1: torch.Size([64, 100, 3])\n",
      "shape of out_gru, torch.Size([64, 100, 3])\n",
      "shape of out_C2, torch.Size([64, 100, 3])\n",
      "shape of out_C1, torch.Size([64, 100, 3])\n",
      "##################################\n",
      "first passage through encoder gru\n",
      "input to encoder_gru, torch.Size([64, 100, 48])\n",
      "shape of out_1 after lstm1, torch.Size([64, 100, 256])\n",
      "shape of out_1 dropout1, torch.Size([64, 100, 256])\n",
      "shape of out_2 after lstm1, torch.Size([64, 100, 128])\n",
      "shape of out_2 dropout2, torch.Size([64, 100, 128])\n",
      "shape of out_2 after flatten, torch.Size([64, 12800])\n",
      "necessary shape for last linear layer = 128000, 600\n",
      "shape encoder_gru output, torch.Size([64, 300])\n",
      "shape encoder_gru output after view, torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 1D\n",
      "shape of input, torch.Size([64, 100, 48])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "Shape after transpose: torch.Size([64, 48, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 100])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 100])\n",
      "Shape after pool: torch.Size([64, 64, 50])\n",
      "Shape after conv3 relu: torch.Size([64, 64, 50])\n",
      "Shape after conv4 relu: torch.Size([64, 64, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 50])\n",
      "Shape after pool: torch.Size([64, 64, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 128])\n",
      "Shape after dropout1: torch.Size([64, 25, 128])\n",
      "necessary shape for fc2 is 25*64, 25*64\n",
      "Shape after fc2: torch.Size([64, 25, 1600])\n",
      "Shape after dropout2: torch.Size([64, 25, 1600])\n",
      "Shape after flatten: torch.Size([64, 40000])\n",
      "Shape of model_2_output: torch.Size([64, 12800])\n",
      "Shape of model_2_output_1 (concactenating last dimension of model_2_output and x): torch.Size([64, 52800])\n",
      "necessary shape for linear 528 * w, 6*w\n",
      "Shape of output_C2: torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 2D\n",
      "shape of input x_1, torch.Size([64, 100, 48])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "shape of input x, torch.Size([64, 100, 6, 8])\n",
      "inputs_2D_N: torch.Size([64, 8, 6, 100])\n",
      " inputs_2D_N: torch.Size([64, 8, 6, 100])\n",
      "inputs_2D_N: torch.Size([64, 100, 6, 8])\n",
      "Shape after transpose: torch.Size([64, 8, 6, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 6, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 6, 100])\n",
      "Shape after pool: torch.Size([64, 64, 3, 50])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 3, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 3, 50])\n",
      "Shape after pool: torch.Size([64, 64, 1, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 1, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 1, 1600])\n",
      "Shape after dropout: torch.Size([64, 25, 1, 1600])\n",
      "necessary shape for fc2 is 64, 32\n",
      "Shape after dropout2: torch.Size([64, 25, 1, 64])\n",
      "Shape after flatten: torch.Size([64, 1600])\n",
      "Shape of model_3_output: torch.Size([64, 12800])\n",
      "Shape of model_3_output after cat: torch.Size([64, 14400])\n",
      "Shape of output_C1: torch.Size([64, 100, 3])\n",
      "shape of out_gru, torch.Size([64, 100, 3])\n",
      "shape of out_C2, torch.Size([64, 100, 3])\n",
      "shape of out_C1, torch.Size([64, 100, 3])\n",
      "##################################\n",
      "first passage through encoder gru\n",
      "input to encoder_gru, torch.Size([64, 100, 48])\n",
      "shape of out_1 after lstm1, torch.Size([64, 100, 256])\n",
      "shape of out_1 dropout1, torch.Size([64, 100, 256])\n",
      "shape of out_2 after lstm1, torch.Size([64, 100, 128])\n",
      "shape of out_2 dropout2, torch.Size([64, 100, 128])\n",
      "shape of out_2 after flatten, torch.Size([64, 12800])\n",
      "necessary shape for last linear layer = 128000, 600\n",
      "shape encoder_gru output, torch.Size([64, 300])\n",
      "shape encoder_gru output after view, torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 1D\n",
      "shape of input, torch.Size([64, 100, 48])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "Shape after transpose: torch.Size([64, 48, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 100])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 100])\n",
      "Shape after pool: torch.Size([64, 64, 50])\n",
      "Shape after conv3 relu: torch.Size([64, 64, 50])\n",
      "Shape after conv4 relu: torch.Size([64, 64, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 50])\n",
      "Shape after pool: torch.Size([64, 64, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 128])\n",
      "Shape after dropout1: torch.Size([64, 25, 128])\n",
      "necessary shape for fc2 is 25*64, 25*64\n",
      "Shape after fc2: torch.Size([64, 25, 1600])\n",
      "Shape after dropout2: torch.Size([64, 25, 1600])\n",
      "Shape after flatten: torch.Size([64, 40000])\n",
      "Shape of model_2_output: torch.Size([64, 12800])\n",
      "Shape of model_2_output_1 (concactenating last dimension of model_2_output and x): torch.Size([64, 52800])\n",
      "necessary shape for linear 528 * w, 6*w\n",
      "Shape of output_C2: torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 2D\n",
      "shape of input x_1, torch.Size([64, 100, 48])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "shape of input x, torch.Size([64, 100, 6, 8])\n",
      "inputs_2D_N: torch.Size([64, 8, 6, 100])\n",
      " inputs_2D_N: torch.Size([64, 8, 6, 100])\n",
      "inputs_2D_N: torch.Size([64, 100, 6, 8])\n",
      "Shape after transpose: torch.Size([64, 8, 6, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 6, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 6, 100])\n",
      "Shape after pool: torch.Size([64, 64, 3, 50])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 3, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 3, 50])\n",
      "Shape after pool: torch.Size([64, 64, 1, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 1, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 1, 1600])\n",
      "Shape after dropout: torch.Size([64, 25, 1, 1600])\n",
      "necessary shape for fc2 is 64, 32\n",
      "Shape after dropout2: torch.Size([64, 25, 1, 64])\n",
      "Shape after flatten: torch.Size([64, 1600])\n",
      "Shape of model_3_output: torch.Size([64, 12800])\n",
      "Shape of model_3_output after cat: torch.Size([64, 14400])\n",
      "Shape of output_C1: torch.Size([64, 100, 3])\n",
      "shape of out_gru, torch.Size([64, 100, 3])\n",
      "shape of out_C2, torch.Size([64, 100, 3])\n",
      "shape of out_C1, torch.Size([64, 100, 3])\n",
      "##################################\n",
      "first passage through encoder gru\n",
      "input to encoder_gru, torch.Size([64, 100, 48])\n",
      "shape of out_1 after lstm1, torch.Size([64, 100, 256])\n",
      "shape of out_1 dropout1, torch.Size([64, 100, 256])\n",
      "shape of out_2 after lstm1, torch.Size([64, 100, 128])\n",
      "shape of out_2 dropout2, torch.Size([64, 100, 128])\n",
      "shape of out_2 after flatten, torch.Size([64, 12800])\n",
      "necessary shape for last linear layer = 128000, 600\n",
      "shape encoder_gru output, torch.Size([64, 300])\n",
      "shape encoder_gru output after view, torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 1D\n",
      "shape of input, torch.Size([64, 100, 48])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "Shape after transpose: torch.Size([64, 48, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 100])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 100])\n",
      "Shape after pool: torch.Size([64, 64, 50])\n",
      "Shape after conv3 relu: torch.Size([64, 64, 50])\n",
      "Shape after conv4 relu: torch.Size([64, 64, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 50])\n",
      "Shape after pool: torch.Size([64, 64, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 128])\n",
      "Shape after dropout1: torch.Size([64, 25, 128])\n",
      "necessary shape for fc2 is 25*64, 25*64\n",
      "Shape after fc2: torch.Size([64, 25, 1600])\n",
      "Shape after dropout2: torch.Size([64, 25, 1600])\n",
      "Shape after flatten: torch.Size([64, 40000])\n",
      "Shape of model_2_output: torch.Size([64, 12800])\n",
      "Shape of model_2_output_1 (concactenating last dimension of model_2_output and x): torch.Size([64, 52800])\n",
      "necessary shape for linear 528 * w, 6*w\n",
      "Shape of output_C2: torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 2D\n",
      "shape of input x_1, torch.Size([64, 100, 48])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "shape of input x, torch.Size([64, 100, 6, 8])\n",
      "inputs_2D_N: torch.Size([64, 8, 6, 100])\n",
      " inputs_2D_N: torch.Size([64, 8, 6, 100])\n",
      "inputs_2D_N: torch.Size([64, 100, 6, 8])\n",
      "Shape after transpose: torch.Size([64, 8, 6, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 6, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 6, 100])\n",
      "Shape after pool: torch.Size([64, 64, 3, 50])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 3, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 3, 50])\n",
      "Shape after pool: torch.Size([64, 64, 1, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 1, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 1, 1600])\n",
      "Shape after dropout: torch.Size([64, 25, 1, 1600])\n",
      "necessary shape for fc2 is 64, 32\n",
      "Shape after dropout2: torch.Size([64, 25, 1, 64])\n",
      "Shape after flatten: torch.Size([64, 1600])\n",
      "Shape of model_3_output: torch.Size([64, 12800])\n",
      "Shape of model_3_output after cat: torch.Size([64, 14400])\n",
      "Shape of output_C1: torch.Size([64, 100, 3])\n",
      "shape of out_gru, torch.Size([64, 100, 3])\n",
      "shape of out_C2, torch.Size([64, 100, 3])\n",
      "shape of out_C1, torch.Size([64, 100, 3])\n",
      "##################################\n",
      "first passage through encoder gru\n",
      "input to encoder_gru, torch.Size([64, 100, 48])\n",
      "shape of out_1 after lstm1, torch.Size([64, 100, 256])\n",
      "shape of out_1 dropout1, torch.Size([64, 100, 256])\n",
      "shape of out_2 after lstm1, torch.Size([64, 100, 128])\n",
      "shape of out_2 dropout2, torch.Size([64, 100, 128])\n",
      "shape of out_2 after flatten, torch.Size([64, 12800])\n",
      "necessary shape for last linear layer = 128000, 600\n",
      "shape encoder_gru output, torch.Size([64, 300])\n",
      "shape encoder_gru output after view, torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 1D\n",
      "shape of input, torch.Size([64, 100, 48])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "Shape after transpose: torch.Size([64, 48, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 100])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 100])\n",
      "Shape after pool: torch.Size([64, 64, 50])\n",
      "Shape after conv3 relu: torch.Size([64, 64, 50])\n",
      "Shape after conv4 relu: torch.Size([64, 64, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 50])\n",
      "Shape after pool: torch.Size([64, 64, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 128])\n",
      "Shape after dropout1: torch.Size([64, 25, 128])\n",
      "necessary shape for fc2 is 25*64, 25*64\n",
      "Shape after fc2: torch.Size([64, 25, 1600])\n",
      "Shape after dropout2: torch.Size([64, 25, 1600])\n",
      "Shape after flatten: torch.Size([64, 40000])\n",
      "Shape of model_2_output: torch.Size([64, 12800])\n",
      "Shape of model_2_output_1 (concactenating last dimension of model_2_output and x): torch.Size([64, 52800])\n",
      "necessary shape for linear 528 * w, 6*w\n",
      "Shape of output_C2: torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 2D\n",
      "shape of input x_1, torch.Size([64, 100, 48])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "shape of input x, torch.Size([64, 100, 6, 8])\n",
      "inputs_2D_N: torch.Size([64, 8, 6, 100])\n",
      " inputs_2D_N: torch.Size([64, 8, 6, 100])\n",
      "inputs_2D_N: torch.Size([64, 100, 6, 8])\n",
      "Shape after transpose: torch.Size([64, 8, 6, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 6, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 6, 100])\n",
      "Shape after pool: torch.Size([64, 64, 3, 50])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 3, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 3, 50])\n",
      "Shape after pool: torch.Size([64, 64, 1, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 1, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 1, 1600])\n",
      "Shape after dropout: torch.Size([64, 25, 1, 1600])\n",
      "necessary shape for fc2 is 64, 32\n",
      "Shape after dropout2: torch.Size([64, 25, 1, 64])\n",
      "Shape after flatten: torch.Size([64, 1600])\n",
      "Shape of model_3_output: torch.Size([64, 12800])\n",
      "Shape of model_3_output after cat: torch.Size([64, 14400])\n",
      "Shape of output_C1: torch.Size([64, 100, 3])\n",
      "shape of out_gru, torch.Size([64, 100, 3])\n",
      "shape of out_C2, torch.Size([64, 100, 3])\n",
      "shape of out_C1, torch.Size([64, 100, 3])\n",
      "##################################\n",
      "first passage through encoder gru\n",
      "input to encoder_gru, torch.Size([64, 100, 48])\n",
      "shape of out_1 after lstm1, torch.Size([64, 100, 256])\n",
      "shape of out_1 dropout1, torch.Size([64, 100, 256])\n",
      "shape of out_2 after lstm1, torch.Size([64, 100, 128])\n",
      "shape of out_2 dropout2, torch.Size([64, 100, 128])\n",
      "shape of out_2 after flatten, torch.Size([64, 12800])\n",
      "necessary shape for last linear layer = 128000, 600\n",
      "shape encoder_gru output, torch.Size([64, 300])\n",
      "shape encoder_gru output after view, torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 1D\n",
      "shape of input, torch.Size([64, 100, 48])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "Shape after transpose: torch.Size([64, 48, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 100])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 100])\n",
      "Shape after pool: torch.Size([64, 64, 50])\n",
      "Shape after conv3 relu: torch.Size([64, 64, 50])\n",
      "Shape after conv4 relu: torch.Size([64, 64, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 50])\n",
      "Shape after pool: torch.Size([64, 64, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 128])\n",
      "Shape after dropout1: torch.Size([64, 25, 128])\n",
      "necessary shape for fc2 is 25*64, 25*64\n",
      "Shape after fc2: torch.Size([64, 25, 1600])\n",
      "Shape after dropout2: torch.Size([64, 25, 1600])\n",
      "Shape after flatten: torch.Size([64, 40000])\n",
      "Shape of model_2_output: torch.Size([64, 12800])\n",
      "Shape of model_2_output_1 (concactenating last dimension of model_2_output and x): torch.Size([64, 52800])\n",
      "necessary shape for linear 528 * w, 6*w\n",
      "Shape of output_C2: torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 2D\n",
      "shape of input x_1, torch.Size([64, 100, 48])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "shape of input x, torch.Size([64, 100, 6, 8])\n",
      "inputs_2D_N: torch.Size([64, 8, 6, 100])\n",
      " inputs_2D_N: torch.Size([64, 8, 6, 100])\n",
      "inputs_2D_N: torch.Size([64, 100, 6, 8])\n",
      "Shape after transpose: torch.Size([64, 8, 6, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 6, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 6, 100])\n",
      "Shape after pool: torch.Size([64, 64, 3, 50])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 3, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 3, 50])\n",
      "Shape after pool: torch.Size([64, 64, 1, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 1, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 1, 1600])\n",
      "Shape after dropout: torch.Size([64, 25, 1, 1600])\n",
      "necessary shape for fc2 is 64, 32\n",
      "Shape after dropout2: torch.Size([64, 25, 1, 64])\n",
      "Shape after flatten: torch.Size([64, 1600])\n",
      "Shape of model_3_output: torch.Size([64, 12800])\n",
      "Shape of model_3_output after cat: torch.Size([64, 14400])\n",
      "Shape of output_C1: torch.Size([64, 100, 3])\n",
      "shape of out_gru, torch.Size([64, 100, 3])\n",
      "shape of out_C2, torch.Size([64, 100, 3])\n",
      "shape of out_C1, torch.Size([64, 100, 3])\n",
      "##################################\n",
      "first passage through encoder gru\n",
      "input to encoder_gru, torch.Size([64, 100, 48])\n",
      "shape of out_1 after lstm1, torch.Size([64, 100, 256])\n",
      "shape of out_1 dropout1, torch.Size([64, 100, 256])\n",
      "shape of out_2 after lstm1, torch.Size([64, 100, 128])\n",
      "shape of out_2 dropout2, torch.Size([64, 100, 128])\n",
      "shape of out_2 after flatten, torch.Size([64, 12800])\n",
      "necessary shape for last linear layer = 128000, 600\n",
      "shape encoder_gru output, torch.Size([64, 300])\n",
      "shape encoder_gru output after view, torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 1D\n",
      "shape of input, torch.Size([64, 100, 48])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "Shape after transpose: torch.Size([64, 48, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 100])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 100])\n",
      "Shape after pool: torch.Size([64, 64, 50])\n",
      "Shape after conv3 relu: torch.Size([64, 64, 50])\n",
      "Shape after conv4 relu: torch.Size([64, 64, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 50])\n",
      "Shape after pool: torch.Size([64, 64, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 128])\n",
      "Shape after dropout1: torch.Size([64, 25, 128])\n",
      "necessary shape for fc2 is 25*64, 25*64\n",
      "Shape after fc2: torch.Size([64, 25, 1600])\n",
      "Shape after dropout2: torch.Size([64, 25, 1600])\n",
      "Shape after flatten: torch.Size([64, 40000])\n",
      "Shape of model_2_output: torch.Size([64, 12800])\n",
      "Shape of model_2_output_1 (concactenating last dimension of model_2_output and x): torch.Size([64, 52800])\n",
      "necessary shape for linear 528 * w, 6*w\n",
      "Shape of output_C2: torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 2D\n",
      "shape of input x_1, torch.Size([64, 100, 48])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "shape of input x, torch.Size([64, 100, 6, 8])\n",
      "inputs_2D_N: torch.Size([64, 8, 6, 100])\n",
      " inputs_2D_N: torch.Size([64, 8, 6, 100])\n",
      "inputs_2D_N: torch.Size([64, 100, 6, 8])\n",
      "Shape after transpose: torch.Size([64, 8, 6, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 6, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 6, 100])\n",
      "Shape after pool: torch.Size([64, 64, 3, 50])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 3, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 3, 50])\n",
      "Shape after pool: torch.Size([64, 64, 1, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 1, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 1, 1600])\n",
      "Shape after dropout: torch.Size([64, 25, 1, 1600])\n",
      "necessary shape for fc2 is 64, 32\n",
      "Shape after dropout2: torch.Size([64, 25, 1, 64])\n",
      "Shape after flatten: torch.Size([64, 1600])\n",
      "Shape of model_3_output: torch.Size([64, 12800])\n",
      "Shape of model_3_output after cat: torch.Size([64, 14400])\n",
      "Shape of output_C1: torch.Size([64, 100, 3])\n",
      "shape of out_gru, torch.Size([64, 100, 3])\n",
      "shape of out_C2, torch.Size([64, 100, 3])\n",
      "shape of out_C1, torch.Size([64, 100, 3])\n",
      "##################################\n",
      "first passage through encoder gru\n",
      "input to encoder_gru, torch.Size([64, 100, 48])\n",
      "shape of out_1 after lstm1, torch.Size([64, 100, 256])\n",
      "shape of out_1 dropout1, torch.Size([64, 100, 256])\n",
      "shape of out_2 after lstm1, torch.Size([64, 100, 128])\n",
      "shape of out_2 dropout2, torch.Size([64, 100, 128])\n",
      "shape of out_2 after flatten, torch.Size([64, 12800])\n",
      "necessary shape for last linear layer = 128000, 600\n",
      "shape encoder_gru output, torch.Size([64, 300])\n",
      "shape encoder_gru output after view, torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 1D\n",
      "shape of input, torch.Size([64, 100, 48])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "Shape after transpose: torch.Size([64, 48, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 100])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 100])\n",
      "Shape after pool: torch.Size([64, 64, 50])\n",
      "Shape after conv3 relu: torch.Size([64, 64, 50])\n",
      "Shape after conv4 relu: torch.Size([64, 64, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 50])\n",
      "Shape after pool: torch.Size([64, 64, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 128])\n",
      "Shape after dropout1: torch.Size([64, 25, 128])\n",
      "necessary shape for fc2 is 25*64, 25*64\n",
      "Shape after fc2: torch.Size([64, 25, 1600])\n",
      "Shape after dropout2: torch.Size([64, 25, 1600])\n",
      "Shape after flatten: torch.Size([64, 40000])\n",
      "Shape of model_2_output: torch.Size([64, 12800])\n",
      "Shape of model_2_output_1 (concactenating last dimension of model_2_output and x): torch.Size([64, 52800])\n",
      "necessary shape for linear 528 * w, 6*w\n",
      "Shape of output_C2: torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 2D\n",
      "shape of input x_1, torch.Size([64, 100, 48])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "shape of input x, torch.Size([64, 100, 6, 8])\n",
      "inputs_2D_N: torch.Size([64, 8, 6, 100])\n",
      " inputs_2D_N: torch.Size([64, 8, 6, 100])\n",
      "inputs_2D_N: torch.Size([64, 100, 6, 8])\n",
      "Shape after transpose: torch.Size([64, 8, 6, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 6, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 6, 100])\n",
      "Shape after pool: torch.Size([64, 64, 3, 50])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 3, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 3, 50])\n",
      "Shape after pool: torch.Size([64, 64, 1, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 1, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 1, 1600])\n",
      "Shape after dropout: torch.Size([64, 25, 1, 1600])\n",
      "necessary shape for fc2 is 64, 32\n",
      "Shape after dropout2: torch.Size([64, 25, 1, 64])\n",
      "Shape after flatten: torch.Size([64, 1600])\n",
      "Shape of model_3_output: torch.Size([64, 12800])\n",
      "Shape of model_3_output after cat: torch.Size([64, 14400])\n",
      "Shape of output_C1: torch.Size([64, 100, 3])\n",
      "shape of out_gru, torch.Size([64, 100, 3])\n",
      "shape of out_C2, torch.Size([64, 100, 3])\n",
      "shape of out_C1, torch.Size([64, 100, 3])\n",
      "##################################\n",
      "first passage through encoder gru\n",
      "input to encoder_gru, torch.Size([64, 100, 48])\n",
      "shape of out_1 after lstm1, torch.Size([64, 100, 256])\n",
      "shape of out_1 dropout1, torch.Size([64, 100, 256])\n",
      "shape of out_2 after lstm1, torch.Size([64, 100, 128])\n",
      "shape of out_2 dropout2, torch.Size([64, 100, 128])\n",
      "shape of out_2 after flatten, torch.Size([64, 12800])\n",
      "necessary shape for last linear layer = 128000, 600\n",
      "shape encoder_gru output, torch.Size([64, 300])\n",
      "shape encoder_gru output after view, torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 1D\n",
      "shape of input, torch.Size([64, 100, 48])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "Shape after transpose: torch.Size([64, 48, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 100])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 100])\n",
      "Shape after pool: torch.Size([64, 64, 50])\n",
      "Shape after conv3 relu: torch.Size([64, 64, 50])\n",
      "Shape after conv4 relu: torch.Size([64, 64, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 50])\n",
      "Shape after pool: torch.Size([64, 64, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 128])\n",
      "Shape after dropout1: torch.Size([64, 25, 128])\n",
      "necessary shape for fc2 is 25*64, 25*64\n",
      "Shape after fc2: torch.Size([64, 25, 1600])\n",
      "Shape after dropout2: torch.Size([64, 25, 1600])\n",
      "Shape after flatten: torch.Size([64, 40000])\n",
      "Shape of model_2_output: torch.Size([64, 12800])\n",
      "Shape of model_2_output_1 (concactenating last dimension of model_2_output and x): torch.Size([64, 52800])\n",
      "necessary shape for linear 528 * w, 6*w\n",
      "Shape of output_C2: torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 2D\n",
      "shape of input x_1, torch.Size([64, 100, 48])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "shape of input x, torch.Size([64, 100, 6, 8])\n",
      "inputs_2D_N: torch.Size([64, 8, 6, 100])\n",
      " inputs_2D_N: torch.Size([64, 8, 6, 100])\n",
      "inputs_2D_N: torch.Size([64, 100, 6, 8])\n",
      "Shape after transpose: torch.Size([64, 8, 6, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 6, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 6, 100])\n",
      "Shape after pool: torch.Size([64, 64, 3, 50])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 3, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 3, 50])\n",
      "Shape after pool: torch.Size([64, 64, 1, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 1, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 1, 1600])\n",
      "Shape after dropout: torch.Size([64, 25, 1, 1600])\n",
      "necessary shape for fc2 is 64, 32\n",
      "Shape after dropout2: torch.Size([64, 25, 1, 64])\n",
      "Shape after flatten: torch.Size([64, 1600])\n",
      "Shape of model_3_output: torch.Size([64, 12800])\n",
      "Shape of model_3_output after cat: torch.Size([64, 14400])\n",
      "Shape of output_C1: torch.Size([64, 100, 3])\n",
      "shape of out_gru, torch.Size([64, 100, 3])\n",
      "shape of out_C2, torch.Size([64, 100, 3])\n",
      "shape of out_C1, torch.Size([64, 100, 3])\n",
      "##################################\n",
      "first passage through encoder gru\n",
      "input to encoder_gru, torch.Size([64, 100, 48])\n",
      "shape of out_1 after lstm1, torch.Size([64, 100, 256])\n",
      "shape of out_1 dropout1, torch.Size([64, 100, 256])\n",
      "shape of out_2 after lstm1, torch.Size([64, 100, 128])\n",
      "shape of out_2 dropout2, torch.Size([64, 100, 128])\n",
      "shape of out_2 after flatten, torch.Size([64, 12800])\n",
      "necessary shape for last linear layer = 128000, 600\n",
      "shape encoder_gru output, torch.Size([64, 300])\n",
      "shape encoder_gru output after view, torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 1D\n",
      "shape of input, torch.Size([64, 100, 48])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "Shape after transpose: torch.Size([64, 48, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 100])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 100])\n",
      "Shape after pool: torch.Size([64, 64, 50])\n",
      "Shape after conv3 relu: torch.Size([64, 64, 50])\n",
      "Shape after conv4 relu: torch.Size([64, 64, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 50])\n",
      "Shape after pool: torch.Size([64, 64, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 128])\n",
      "Shape after dropout1: torch.Size([64, 25, 128])\n",
      "necessary shape for fc2 is 25*64, 25*64\n",
      "Shape after fc2: torch.Size([64, 25, 1600])\n",
      "Shape after dropout2: torch.Size([64, 25, 1600])\n",
      "Shape after flatten: torch.Size([64, 40000])\n",
      "Shape of model_2_output: torch.Size([64, 12800])\n",
      "Shape of model_2_output_1 (concactenating last dimension of model_2_output and x): torch.Size([64, 52800])\n",
      "necessary shape for linear 528 * w, 6*w\n",
      "Shape of output_C2: torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 2D\n",
      "shape of input x_1, torch.Size([64, 100, 48])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "shape of input x, torch.Size([64, 100, 6, 8])\n",
      "inputs_2D_N: torch.Size([64, 8, 6, 100])\n",
      " inputs_2D_N: torch.Size([64, 8, 6, 100])\n",
      "inputs_2D_N: torch.Size([64, 100, 6, 8])\n",
      "Shape after transpose: torch.Size([64, 8, 6, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 6, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 6, 100])\n",
      "Shape after pool: torch.Size([64, 64, 3, 50])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 3, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 3, 50])\n",
      "Shape after pool: torch.Size([64, 64, 1, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 1, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 1, 1600])\n",
      "Shape after dropout: torch.Size([64, 25, 1, 1600])\n",
      "necessary shape for fc2 is 64, 32\n",
      "Shape after dropout2: torch.Size([64, 25, 1, 64])\n",
      "Shape after flatten: torch.Size([64, 1600])\n",
      "Shape of model_3_output: torch.Size([64, 12800])\n",
      "Shape of model_3_output after cat: torch.Size([64, 14400])\n",
      "Shape of output_C1: torch.Size([64, 100, 3])\n",
      "shape of out_gru, torch.Size([64, 100, 3])\n",
      "shape of out_C2, torch.Size([64, 100, 3])\n",
      "shape of out_C1, torch.Size([64, 100, 3])\n",
      "##################################\n",
      "first passage through encoder gru\n",
      "input to encoder_gru, torch.Size([64, 100, 48])\n",
      "shape of out_1 after lstm1, torch.Size([64, 100, 256])\n",
      "shape of out_1 dropout1, torch.Size([64, 100, 256])\n",
      "shape of out_2 after lstm1, torch.Size([64, 100, 128])\n",
      "shape of out_2 dropout2, torch.Size([64, 100, 128])\n",
      "shape of out_2 after flatten, torch.Size([64, 12800])\n",
      "necessary shape for last linear layer = 128000, 600\n",
      "shape encoder_gru output, torch.Size([64, 300])\n",
      "shape encoder_gru output after view, torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 1D\n",
      "shape of input, torch.Size([64, 100, 48])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "Shape after transpose: torch.Size([64, 48, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 100])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 100])\n",
      "Shape after pool: torch.Size([64, 64, 50])\n",
      "Shape after conv3 relu: torch.Size([64, 64, 50])\n",
      "Shape after conv4 relu: torch.Size([64, 64, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 50])\n",
      "Shape after pool: torch.Size([64, 64, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 128])\n",
      "Shape after dropout1: torch.Size([64, 25, 128])\n",
      "necessary shape for fc2 is 25*64, 25*64\n",
      "Shape after fc2: torch.Size([64, 25, 1600])\n",
      "Shape after dropout2: torch.Size([64, 25, 1600])\n",
      "Shape after flatten: torch.Size([64, 40000])\n",
      "Shape of model_2_output: torch.Size([64, 12800])\n",
      "Shape of model_2_output_1 (concactenating last dimension of model_2_output and x): torch.Size([64, 52800])\n",
      "necessary shape for linear 528 * w, 6*w\n",
      "Shape of output_C2: torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 2D\n",
      "shape of input x_1, torch.Size([64, 100, 48])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "shape of input x, torch.Size([64, 100, 6, 8])\n",
      "inputs_2D_N: torch.Size([64, 8, 6, 100])\n",
      " inputs_2D_N: torch.Size([64, 8, 6, 100])\n",
      "inputs_2D_N: torch.Size([64, 100, 6, 8])\n",
      "Shape after transpose: torch.Size([64, 8, 6, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 6, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 6, 100])\n",
      "Shape after pool: torch.Size([64, 64, 3, 50])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 3, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 3, 50])\n",
      "Shape after pool: torch.Size([64, 64, 1, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 1, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 1, 1600])\n",
      "Shape after dropout: torch.Size([64, 25, 1, 1600])\n",
      "necessary shape for fc2 is 64, 32\n",
      "Shape after dropout2: torch.Size([64, 25, 1, 64])\n",
      "Shape after flatten: torch.Size([64, 1600])\n",
      "Shape of model_3_output: torch.Size([64, 12800])\n",
      "Shape of model_3_output after cat: torch.Size([64, 14400])\n",
      "Shape of output_C1: torch.Size([64, 100, 3])\n",
      "shape of out_gru, torch.Size([64, 100, 3])\n",
      "shape of out_C2, torch.Size([64, 100, 3])\n",
      "shape of out_C1, torch.Size([64, 100, 3])\n",
      "##################################\n",
      "first passage through encoder gru\n",
      "input to encoder_gru, torch.Size([64, 100, 48])\n",
      "shape of out_1 after lstm1, torch.Size([64, 100, 256])\n",
      "shape of out_1 dropout1, torch.Size([64, 100, 256])\n",
      "shape of out_2 after lstm1, torch.Size([64, 100, 128])\n",
      "shape of out_2 dropout2, torch.Size([64, 100, 128])\n",
      "shape of out_2 after flatten, torch.Size([64, 12800])\n",
      "necessary shape for last linear layer = 128000, 600\n",
      "shape encoder_gru output, torch.Size([64, 300])\n",
      "shape encoder_gru output after view, torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 1D\n",
      "shape of input, torch.Size([64, 100, 48])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "Shape after transpose: torch.Size([64, 48, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 100])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 100])\n",
      "Shape after pool: torch.Size([64, 64, 50])\n",
      "Shape after conv3 relu: torch.Size([64, 64, 50])\n",
      "Shape after conv4 relu: torch.Size([64, 64, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 50])\n",
      "Shape after pool: torch.Size([64, 64, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 128])\n",
      "Shape after dropout1: torch.Size([64, 25, 128])\n",
      "necessary shape for fc2 is 25*64, 25*64\n",
      "Shape after fc2: torch.Size([64, 25, 1600])\n",
      "Shape after dropout2: torch.Size([64, 25, 1600])\n",
      "Shape after flatten: torch.Size([64, 40000])\n",
      "Shape of model_2_output: torch.Size([64, 12800])\n",
      "Shape of model_2_output_1 (concactenating last dimension of model_2_output and x): torch.Size([64, 52800])\n",
      "necessary shape for linear 528 * w, 6*w\n",
      "Shape of output_C2: torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 2D\n",
      "shape of input x_1, torch.Size([64, 100, 48])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "shape of input x, torch.Size([64, 100, 6, 8])\n",
      "inputs_2D_N: torch.Size([64, 8, 6, 100])\n",
      " inputs_2D_N: torch.Size([64, 8, 6, 100])\n",
      "inputs_2D_N: torch.Size([64, 100, 6, 8])\n",
      "Shape after transpose: torch.Size([64, 8, 6, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 6, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 6, 100])\n",
      "Shape after pool: torch.Size([64, 64, 3, 50])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 3, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 3, 50])\n",
      "Shape after pool: torch.Size([64, 64, 1, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 1, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 1, 1600])\n",
      "Shape after dropout: torch.Size([64, 25, 1, 1600])\n",
      "necessary shape for fc2 is 64, 32\n",
      "Shape after dropout2: torch.Size([64, 25, 1, 64])\n",
      "Shape after flatten: torch.Size([64, 1600])\n",
      "Shape of model_3_output: torch.Size([64, 12800])\n",
      "Shape of model_3_output after cat: torch.Size([64, 14400])\n",
      "Shape of output_C1: torch.Size([64, 100, 3])\n",
      "shape of out_gru, torch.Size([64, 100, 3])\n",
      "shape of out_C2, torch.Size([64, 100, 3])\n",
      "shape of out_C1, torch.Size([64, 100, 3])\n",
      "##################################\n",
      "first passage through encoder gru\n",
      "input to encoder_gru, torch.Size([64, 100, 48])\n",
      "shape of out_1 after lstm1, torch.Size([64, 100, 256])\n",
      "shape of out_1 dropout1, torch.Size([64, 100, 256])\n",
      "shape of out_2 after lstm1, torch.Size([64, 100, 128])\n",
      "shape of out_2 dropout2, torch.Size([64, 100, 128])\n",
      "shape of out_2 after flatten, torch.Size([64, 12800])\n",
      "necessary shape for last linear layer = 128000, 600\n",
      "shape encoder_gru output, torch.Size([64, 300])\n",
      "shape encoder_gru output after view, torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 1D\n",
      "shape of input, torch.Size([64, 100, 48])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "Shape after transpose: torch.Size([64, 48, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 100])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 100])\n",
      "Shape after pool: torch.Size([64, 64, 50])\n",
      "Shape after conv3 relu: torch.Size([64, 64, 50])\n",
      "Shape after conv4 relu: torch.Size([64, 64, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 50])\n",
      "Shape after pool: torch.Size([64, 64, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 128])\n",
      "Shape after dropout1: torch.Size([64, 25, 128])\n",
      "necessary shape for fc2 is 25*64, 25*64\n",
      "Shape after fc2: torch.Size([64, 25, 1600])\n",
      "Shape after dropout2: torch.Size([64, 25, 1600])\n",
      "Shape after flatten: torch.Size([64, 40000])\n",
      "Shape of model_2_output: torch.Size([64, 12800])\n",
      "Shape of model_2_output_1 (concactenating last dimension of model_2_output and x): torch.Size([64, 52800])\n",
      "necessary shape for linear 528 * w, 6*w\n",
      "Shape of output_C2: torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 2D\n",
      "shape of input x_1, torch.Size([64, 100, 48])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "shape of input x, torch.Size([64, 100, 6, 8])\n",
      "inputs_2D_N: torch.Size([64, 8, 6, 100])\n",
      " inputs_2D_N: torch.Size([64, 8, 6, 100])\n",
      "inputs_2D_N: torch.Size([64, 100, 6, 8])\n",
      "Shape after transpose: torch.Size([64, 8, 6, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 6, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 6, 100])\n",
      "Shape after pool: torch.Size([64, 64, 3, 50])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 3, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 3, 50])\n",
      "Shape after pool: torch.Size([64, 64, 1, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 1, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 1, 1600])\n",
      "Shape after dropout: torch.Size([64, 25, 1, 1600])\n",
      "necessary shape for fc2 is 64, 32\n",
      "Shape after dropout2: torch.Size([64, 25, 1, 64])\n",
      "Shape after flatten: torch.Size([64, 1600])\n",
      "Shape of model_3_output: torch.Size([64, 12800])\n",
      "Shape of model_3_output after cat: torch.Size([64, 14400])\n",
      "Shape of output_C1: torch.Size([64, 100, 3])\n",
      "shape of out_gru, torch.Size([64, 100, 3])\n",
      "shape of out_C2, torch.Size([64, 100, 3])\n",
      "shape of out_C1, torch.Size([64, 100, 3])\n",
      "##################################\n",
      "first passage through encoder gru\n",
      "input to encoder_gru, torch.Size([64, 100, 48])\n",
      "shape of out_1 after lstm1, torch.Size([64, 100, 256])\n",
      "shape of out_1 dropout1, torch.Size([64, 100, 256])\n",
      "shape of out_2 after lstm1, torch.Size([64, 100, 128])\n",
      "shape of out_2 dropout2, torch.Size([64, 100, 128])\n",
      "shape of out_2 after flatten, torch.Size([64, 12800])\n",
      "necessary shape for last linear layer = 128000, 600\n",
      "shape encoder_gru output, torch.Size([64, 300])\n",
      "shape encoder_gru output after view, torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 1D\n",
      "shape of input, torch.Size([64, 100, 48])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "Shape after transpose: torch.Size([64, 48, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 100])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 100])\n",
      "Shape after pool: torch.Size([64, 64, 50])\n",
      "Shape after conv3 relu: torch.Size([64, 64, 50])\n",
      "Shape after conv4 relu: torch.Size([64, 64, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 50])\n",
      "Shape after pool: torch.Size([64, 64, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 128])\n",
      "Shape after dropout1: torch.Size([64, 25, 128])\n",
      "necessary shape for fc2 is 25*64, 25*64\n",
      "Shape after fc2: torch.Size([64, 25, 1600])\n",
      "Shape after dropout2: torch.Size([64, 25, 1600])\n",
      "Shape after flatten: torch.Size([64, 40000])\n",
      "Shape of model_2_output: torch.Size([64, 12800])\n",
      "Shape of model_2_output_1 (concactenating last dimension of model_2_output and x): torch.Size([64, 52800])\n",
      "necessary shape for linear 528 * w, 6*w\n",
      "Shape of output_C2: torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 2D\n",
      "shape of input x_1, torch.Size([64, 100, 48])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "shape of input x, torch.Size([64, 100, 6, 8])\n",
      "inputs_2D_N: torch.Size([64, 8, 6, 100])\n",
      " inputs_2D_N: torch.Size([64, 8, 6, 100])\n",
      "inputs_2D_N: torch.Size([64, 100, 6, 8])\n",
      "Shape after transpose: torch.Size([64, 8, 6, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 6, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 6, 100])\n",
      "Shape after pool: torch.Size([64, 64, 3, 50])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 3, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 3, 50])\n",
      "Shape after pool: torch.Size([64, 64, 1, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 1, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 1, 1600])\n",
      "Shape after dropout: torch.Size([64, 25, 1, 1600])\n",
      "necessary shape for fc2 is 64, 32\n",
      "Shape after dropout2: torch.Size([64, 25, 1, 64])\n",
      "Shape after flatten: torch.Size([64, 1600])\n",
      "Shape of model_3_output: torch.Size([64, 12800])\n",
      "Shape of model_3_output after cat: torch.Size([64, 14400])\n",
      "Shape of output_C1: torch.Size([64, 100, 3])\n",
      "shape of out_gru, torch.Size([64, 100, 3])\n",
      "shape of out_C2, torch.Size([64, 100, 3])\n",
      "shape of out_C1, torch.Size([64, 100, 3])\n",
      "##################################\n",
      "first passage through encoder gru\n",
      "input to encoder_gru, torch.Size([10, 100, 48])\n",
      "shape of out_1 after lstm1, torch.Size([10, 100, 256])\n",
      "shape of out_1 dropout1, torch.Size([10, 100, 256])\n",
      "shape of out_2 after lstm1, torch.Size([10, 100, 128])\n",
      "shape of out_2 dropout2, torch.Size([10, 100, 128])\n",
      "shape of out_2 after flatten, torch.Size([10, 12800])\n",
      "necessary shape for last linear layer = 128000, 600\n",
      "shape encoder_gru output, torch.Size([10, 300])\n",
      "shape encoder_gru output after view, torch.Size([10, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 1D\n",
      "shape of input, torch.Size([10, 100, 48])\n",
      "input_1D_N_1: torch.Size([10, 48, 100])\n",
      "input_1D_N_1: torch.Size([10, 48, 100])\n",
      "input_1D_N_1: torch.Size([10, 48, 100])\n",
      "Shape after transpose: torch.Size([10, 48, 100])\n",
      "Shape after conv1 relu: torch.Size([10, 64, 100])\n",
      "Shape after conv2 relu: torch.Size([10, 64, 100])\n",
      "Shape after BN_2: torch.Size([10, 64, 100])\n",
      "Shape after pool: torch.Size([10, 64, 50])\n",
      "Shape after conv3 relu: torch.Size([10, 64, 50])\n",
      "Shape after conv4 relu: torch.Size([10, 64, 50])\n",
      "Shape after BN_4: torch.Size([10, 64, 50])\n",
      "Shape after pool: torch.Size([10, 64, 25])\n",
      "Shape after transpose: torch.Size([10, 25, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([10, 25, 128])\n",
      "Shape after dropout1: torch.Size([10, 25, 128])\n",
      "necessary shape for fc2 is 25*64, 25*64\n",
      "Shape after fc2: torch.Size([10, 25, 1600])\n",
      "Shape after dropout2: torch.Size([10, 25, 1600])\n",
      "Shape after flatten: torch.Size([10, 40000])\n",
      "Shape of model_2_output: torch.Size([10, 12800])\n",
      "Shape of model_2_output_1 (concactenating last dimension of model_2_output and x): torch.Size([10, 52800])\n",
      "necessary shape for linear 528 * w, 6*w\n",
      "Shape of output_C2: torch.Size([10, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 2D\n",
      "shape of input x_1, torch.Size([10, 100, 48])\n",
      "input_1D_N_1: torch.Size([10, 48, 100])\n",
      "input_1D_N_1: torch.Size([10, 48, 100])\n",
      "input_1D_N_1: torch.Size([10, 48, 100])\n",
      "shape of input x, torch.Size([10, 100, 6, 8])\n",
      "inputs_2D_N: torch.Size([10, 8, 6, 100])\n",
      " inputs_2D_N: torch.Size([10, 8, 6, 100])\n",
      "inputs_2D_N: torch.Size([10, 100, 6, 8])\n",
      "Shape after transpose: torch.Size([10, 8, 6, 100])\n",
      "Shape after conv1 relu: torch.Size([10, 64, 6, 100])\n",
      "Shape after BN_2: torch.Size([10, 64, 6, 100])\n",
      "Shape after pool: torch.Size([10, 64, 3, 50])\n",
      "Shape after conv2 relu: torch.Size([10, 64, 3, 50])\n",
      "Shape after BN_4: torch.Size([10, 64, 3, 50])\n",
      "Shape after pool: torch.Size([10, 64, 1, 25])\n",
      "Shape after transpose: torch.Size([10, 25, 1, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([10, 25, 1, 1600])\n",
      "Shape after dropout: torch.Size([10, 25, 1, 1600])\n",
      "necessary shape for fc2 is 64, 32\n",
      "Shape after dropout2: torch.Size([10, 25, 1, 64])\n",
      "Shape after flatten: torch.Size([10, 1600])\n",
      "Shape of model_3_output: torch.Size([10, 12800])\n",
      "Shape of model_3_output after cat: torch.Size([10, 14400])\n",
      "Shape of output_C1: torch.Size([10, 100, 3])\n",
      "shape of out_gru, torch.Size([10, 100, 3])\n",
      "shape of out_C2, torch.Size([10, 100, 3])\n",
      "shape of out_C1, torch.Size([10, 100, 3])\n",
      "(1290, 100, 3)\n",
      "(1290, 100, 3)\n",
      "(193500,) (193500,)\n",
      "(64500, 3) (64500, 3)\n",
      "(64500, 3)\n",
      "11.682516840209876\n",
      "\n",
      "\n",
      "11.682516840209876\n",
      "\n",
      "\n",
      "0.9047990327154244\n",
      "\n",
      "\n",
      "0.9047990327154244\n"
     ]
    }
   ],
   "source": [
    "gait_Net_teacher= teacher(k_2-k_1,k_4-k_3,100)\n",
    "gait_Net_teacher.load_state_dict(torch.load(\"../../datacollection/vicon/subject_1/test/test.pth\"))\n",
    "gait_Net_teacher.to(device)\n",
    "\n",
    "gait_Net_teacher.eval()\n",
    "\n",
    "# iterate through batches of test data\n",
    "with torch.no_grad():\n",
    "    for i, (data_features_1D, data_features_2D, data_targets) in enumerate(test_loader):\n",
    "        output_1,output_2,output_3,output = gait_Net_teacher(data_features_1D[:,:,k_1:k_2].to(device).float(),data_features_2D[:,:,:,k_3:k_4].to(device).float())\n",
    "        if i==0:\n",
    "          yhat_5=output\n",
    "          test_target=data_targets\n",
    "\n",
    "        else:\n",
    "          yhat_5=torch.cat((yhat_5,output),dim=0)\n",
    "          test_target=torch.cat((test_target,data_targets),dim=0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "yhat_4 = yhat_5.detach().cpu().numpy()\n",
    "test_y = test_target.detach().cpu().numpy()\n",
    "print(yhat_4.shape)\n",
    "\n",
    " ### Present ###\n",
    "yhat_5=yhat_4.reshape((yhat_4.shape[0]*w,3))\n",
    "test_y_r=test_y.reshape((test_y.shape[0]*w,3))\n",
    "\n",
    "print(yhat_4.shape)\n",
    "\n",
    "### Unpack ###\n",
    "yhat_up=unpack_dataset_present(np.array(yhat_5))\n",
    "test_y_up=unpack_dataset_present(np.array(test_y_r))\n",
    "\n",
    "print(yhat_up.shape,test_y_up.shape)\n",
    "\n",
    "### Present ###\n",
    "\n",
    "yhat_up=yhat_up.reshape(int(len(yhat_up)/3),3)\n",
    "test_y_up=test_y_up.reshape(int(len(test_y_up)/3),3)\n",
    "\n",
    "print(yhat_up.shape,test_y_up.shape)\n",
    "\n",
    "print(yhat_up.shape)\n",
    "\n",
    "\n",
    "### Present ###\n",
    "\n",
    "rmse,p= prediction_test(np.array(yhat_up),np.array(test_y_up))\n",
    "\n",
    "print(rmse)\n",
    "\n",
    "m=rmse\n",
    "\n",
    "print('\\n')\n",
    "print(m)\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "print(p)\n",
    "print('\\n')\n",
    "\n",
    "print(p)\n",
    "\n",
    "ablation_1=np.hstack([rmse,p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_features_1D shape: torch.Size([64, 100, 48])\n",
      "data_features_2D shape: torch.Size([64, 100, 6, 8])\n",
      "data_targets shape: torch.Size([64, 100, 3])\n",
      "##################################\n",
      "first passage through encoder gru\n",
      "input to encoder_gru, torch.Size([64, 100, 12])\n",
      "shape of out_1 after lstm1, torch.Size([64, 100, 256])\n",
      "shape of out_1 dropout1, torch.Size([64, 100, 256])\n",
      "shape of out_2 after lstm1, torch.Size([64, 100, 128])\n",
      "shape of out_2 dropout2, torch.Size([64, 100, 128])\n",
      "shape of out_2 after flatten, torch.Size([64, 12800])\n",
      "necessary shape for last linear layer = 128000, 600\n",
      "shape encoder_gru output, torch.Size([64, 300])\n",
      "shape encoder_gru output after view, torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 1D\n",
      "shape of input, torch.Size([64, 100, 12])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "Shape after transpose: torch.Size([64, 12, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 100])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 100])\n",
      "Shape after pool: torch.Size([64, 64, 50])\n",
      "Shape after conv3 relu: torch.Size([64, 64, 50])\n",
      "Shape after conv4 relu: torch.Size([64, 64, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 50])\n",
      "Shape after pool: torch.Size([64, 64, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 128])\n",
      "Shape after dropout1: torch.Size([64, 25, 128])\n",
      "necessary shape for fc2 is 25*64, 25*64\n",
      "Shape after fc2: torch.Size([64, 25, 1600])\n",
      "Shape after dropout2: torch.Size([64, 25, 1600])\n",
      "Shape after flatten: torch.Size([64, 40000])\n",
      "Shape of model_2_output: torch.Size([64, 12800])\n",
      "Shape of model_2_output_1 (concactenating last dimension of model_2_output and x): torch.Size([64, 52800])\n",
      "necessary shape for linear 528 * w, 6*w\n",
      "Shape of output_C2: torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 2D\n",
      "shape of input x_1, torch.Size([64, 100, 12])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "shape of input x, torch.Size([64, 100, 6, 2])\n",
      "inputs_2D_N: torch.Size([64, 2, 6, 100])\n",
      " inputs_2D_N: torch.Size([64, 2, 6, 100])\n",
      "inputs_2D_N: torch.Size([64, 100, 6, 2])\n",
      "Shape after transpose: torch.Size([64, 2, 6, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 6, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 6, 100])\n",
      "Shape after pool: torch.Size([64, 64, 3, 50])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 3, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 3, 50])\n",
      "Shape after pool: torch.Size([64, 64, 1, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 1, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 1, 1600])\n",
      "Shape after dropout: torch.Size([64, 25, 1, 1600])\n",
      "necessary shape for fc2 is 64, 32\n",
      "Shape after dropout2: torch.Size([64, 25, 1, 64])\n",
      "Shape after flatten: torch.Size([64, 1600])\n",
      "Shape of model_3_output: torch.Size([64, 12800])\n",
      "Shape of model_3_output after cat: torch.Size([64, 14400])\n",
      "Shape of output_C1: torch.Size([64, 100, 3])\n",
      "output: torch.Size([64, 100, 3])\n",
      "data_features_1D shape: torch.Size([64, 100, 48])\n",
      "data_features_2D shape: torch.Size([64, 100, 6, 8])\n",
      "data_targets shape: torch.Size([64, 100, 3])\n",
      "##################################\n",
      "first passage through encoder gru\n",
      "input to encoder_gru, torch.Size([64, 100, 12])\n",
      "shape of out_1 after lstm1, torch.Size([64, 100, 256])\n",
      "shape of out_1 dropout1, torch.Size([64, 100, 256])\n",
      "shape of out_2 after lstm1, torch.Size([64, 100, 128])\n",
      "shape of out_2 dropout2, torch.Size([64, 100, 128])\n",
      "shape of out_2 after flatten, torch.Size([64, 12800])\n",
      "necessary shape for last linear layer = 128000, 600\n",
      "shape encoder_gru output, torch.Size([64, 300])\n",
      "shape encoder_gru output after view, torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 1D\n",
      "shape of input, torch.Size([64, 100, 12])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "Shape after transpose: torch.Size([64, 12, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 100])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 100])\n",
      "Shape after pool: torch.Size([64, 64, 50])\n",
      "Shape after conv3 relu: torch.Size([64, 64, 50])\n",
      "Shape after conv4 relu: torch.Size([64, 64, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 50])\n",
      "Shape after pool: torch.Size([64, 64, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 128])\n",
      "Shape after dropout1: torch.Size([64, 25, 128])\n",
      "necessary shape for fc2 is 25*64, 25*64\n",
      "Shape after fc2: torch.Size([64, 25, 1600])\n",
      "Shape after dropout2: torch.Size([64, 25, 1600])\n",
      "Shape after flatten: torch.Size([64, 40000])\n",
      "Shape of model_2_output: torch.Size([64, 12800])\n",
      "Shape of model_2_output_1 (concactenating last dimension of model_2_output and x): torch.Size([64, 52800])\n",
      "necessary shape for linear 528 * w, 6*w\n",
      "Shape of output_C2: torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 2D\n",
      "shape of input x_1, torch.Size([64, 100, 12])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "shape of input x, torch.Size([64, 100, 6, 2])\n",
      "inputs_2D_N: torch.Size([64, 2, 6, 100])\n",
      " inputs_2D_N: torch.Size([64, 2, 6, 100])\n",
      "inputs_2D_N: torch.Size([64, 100, 6, 2])\n",
      "Shape after transpose: torch.Size([64, 2, 6, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 6, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 6, 100])\n",
      "Shape after pool: torch.Size([64, 64, 3, 50])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 3, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 3, 50])\n",
      "Shape after pool: torch.Size([64, 64, 1, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 1, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 1, 1600])\n",
      "Shape after dropout: torch.Size([64, 25, 1, 1600])\n",
      "necessary shape for fc2 is 64, 32\n",
      "Shape after dropout2: torch.Size([64, 25, 1, 64])\n",
      "Shape after flatten: torch.Size([64, 1600])\n",
      "Shape of model_3_output: torch.Size([64, 12800])\n",
      "Shape of model_3_output after cat: torch.Size([64, 14400])\n",
      "Shape of output_C1: torch.Size([64, 100, 3])\n",
      "output: torch.Size([64, 100, 3])\n",
      "data_features_1D shape: torch.Size([64, 100, 48])\n",
      "data_features_2D shape: torch.Size([64, 100, 6, 8])\n",
      "data_targets shape: torch.Size([64, 100, 3])\n",
      "##################################\n",
      "first passage through encoder gru\n",
      "input to encoder_gru, torch.Size([64, 100, 12])\n",
      "shape of out_1 after lstm1, torch.Size([64, 100, 256])\n",
      "shape of out_1 dropout1, torch.Size([64, 100, 256])\n",
      "shape of out_2 after lstm1, torch.Size([64, 100, 128])\n",
      "shape of out_2 dropout2, torch.Size([64, 100, 128])\n",
      "shape of out_2 after flatten, torch.Size([64, 12800])\n",
      "necessary shape for last linear layer = 128000, 600\n",
      "shape encoder_gru output, torch.Size([64, 300])\n",
      "shape encoder_gru output after view, torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 1D\n",
      "shape of input, torch.Size([64, 100, 12])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "Shape after transpose: torch.Size([64, 12, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 100])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 100])\n",
      "Shape after pool: torch.Size([64, 64, 50])\n",
      "Shape after conv3 relu: torch.Size([64, 64, 50])\n",
      "Shape after conv4 relu: torch.Size([64, 64, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 50])\n",
      "Shape after pool: torch.Size([64, 64, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 128])\n",
      "Shape after dropout1: torch.Size([64, 25, 128])\n",
      "necessary shape for fc2 is 25*64, 25*64\n",
      "Shape after fc2: torch.Size([64, 25, 1600])\n",
      "Shape after dropout2: torch.Size([64, 25, 1600])\n",
      "Shape after flatten: torch.Size([64, 40000])\n",
      "Shape of model_2_output: torch.Size([64, 12800])\n",
      "Shape of model_2_output_1 (concactenating last dimension of model_2_output and x): torch.Size([64, 52800])\n",
      "necessary shape for linear 528 * w, 6*w\n",
      "Shape of output_C2: torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 2D\n",
      "shape of input x_1, torch.Size([64, 100, 12])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "shape of input x, torch.Size([64, 100, 6, 2])\n",
      "inputs_2D_N: torch.Size([64, 2, 6, 100])\n",
      " inputs_2D_N: torch.Size([64, 2, 6, 100])\n",
      "inputs_2D_N: torch.Size([64, 100, 6, 2])\n",
      "Shape after transpose: torch.Size([64, 2, 6, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 6, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 6, 100])\n",
      "Shape after pool: torch.Size([64, 64, 3, 50])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 3, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 3, 50])\n",
      "Shape after pool: torch.Size([64, 64, 1, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 1, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 1, 1600])\n",
      "Shape after dropout: torch.Size([64, 25, 1, 1600])\n",
      "necessary shape for fc2 is 64, 32\n",
      "Shape after dropout2: torch.Size([64, 25, 1, 64])\n",
      "Shape after flatten: torch.Size([64, 1600])\n",
      "Shape of model_3_output: torch.Size([64, 12800])\n",
      "Shape of model_3_output after cat: torch.Size([64, 14400])\n",
      "Shape of output_C1: torch.Size([64, 100, 3])\n",
      "output: torch.Size([64, 100, 3])\n",
      "data_features_1D shape: torch.Size([64, 100, 48])\n",
      "data_features_2D shape: torch.Size([64, 100, 6, 8])\n",
      "data_targets shape: torch.Size([64, 100, 3])\n",
      "##################################\n",
      "first passage through encoder gru\n",
      "input to encoder_gru, torch.Size([64, 100, 12])\n",
      "shape of out_1 after lstm1, torch.Size([64, 100, 256])\n",
      "shape of out_1 dropout1, torch.Size([64, 100, 256])\n",
      "shape of out_2 after lstm1, torch.Size([64, 100, 128])\n",
      "shape of out_2 dropout2, torch.Size([64, 100, 128])\n",
      "shape of out_2 after flatten, torch.Size([64, 12800])\n",
      "necessary shape for last linear layer = 128000, 600\n",
      "shape encoder_gru output, torch.Size([64, 300])\n",
      "shape encoder_gru output after view, torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 1D\n",
      "shape of input, torch.Size([64, 100, 12])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "Shape after transpose: torch.Size([64, 12, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 100])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 100])\n",
      "Shape after pool: torch.Size([64, 64, 50])\n",
      "Shape after conv3 relu: torch.Size([64, 64, 50])\n",
      "Shape after conv4 relu: torch.Size([64, 64, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 50])\n",
      "Shape after pool: torch.Size([64, 64, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 128])\n",
      "Shape after dropout1: torch.Size([64, 25, 128])\n",
      "necessary shape for fc2 is 25*64, 25*64\n",
      "Shape after fc2: torch.Size([64, 25, 1600])\n",
      "Shape after dropout2: torch.Size([64, 25, 1600])\n",
      "Shape after flatten: torch.Size([64, 40000])\n",
      "Shape of model_2_output: torch.Size([64, 12800])\n",
      "Shape of model_2_output_1 (concactenating last dimension of model_2_output and x): torch.Size([64, 52800])\n",
      "necessary shape for linear 528 * w, 6*w\n",
      "Shape of output_C2: torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 2D\n",
      "shape of input x_1, torch.Size([64, 100, 12])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "shape of input x, torch.Size([64, 100, 6, 2])\n",
      "inputs_2D_N: torch.Size([64, 2, 6, 100])\n",
      " inputs_2D_N: torch.Size([64, 2, 6, 100])\n",
      "inputs_2D_N: torch.Size([64, 100, 6, 2])\n",
      "Shape after transpose: torch.Size([64, 2, 6, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 6, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 6, 100])\n",
      "Shape after pool: torch.Size([64, 64, 3, 50])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 3, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 3, 50])\n",
      "Shape after pool: torch.Size([64, 64, 1, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 1, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 1, 1600])\n",
      "Shape after dropout: torch.Size([64, 25, 1, 1600])\n",
      "necessary shape for fc2 is 64, 32\n",
      "Shape after dropout2: torch.Size([64, 25, 1, 64])\n",
      "Shape after flatten: torch.Size([64, 1600])\n",
      "Shape of model_3_output: torch.Size([64, 12800])\n",
      "Shape of model_3_output after cat: torch.Size([64, 14400])\n",
      "Shape of output_C1: torch.Size([64, 100, 3])\n",
      "output: torch.Size([64, 100, 3])\n",
      "data_features_1D shape: torch.Size([64, 100, 48])\n",
      "data_features_2D shape: torch.Size([64, 100, 6, 8])\n",
      "data_targets shape: torch.Size([64, 100, 3])\n",
      "##################################\n",
      "first passage through encoder gru\n",
      "input to encoder_gru, torch.Size([64, 100, 12])\n",
      "shape of out_1 after lstm1, torch.Size([64, 100, 256])\n",
      "shape of out_1 dropout1, torch.Size([64, 100, 256])\n",
      "shape of out_2 after lstm1, torch.Size([64, 100, 128])\n",
      "shape of out_2 dropout2, torch.Size([64, 100, 128])\n",
      "shape of out_2 after flatten, torch.Size([64, 12800])\n",
      "necessary shape for last linear layer = 128000, 600\n",
      "shape encoder_gru output, torch.Size([64, 300])\n",
      "shape encoder_gru output after view, torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 1D\n",
      "shape of input, torch.Size([64, 100, 12])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "Shape after transpose: torch.Size([64, 12, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 100])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 100])\n",
      "Shape after pool: torch.Size([64, 64, 50])\n",
      "Shape after conv3 relu: torch.Size([64, 64, 50])\n",
      "Shape after conv4 relu: torch.Size([64, 64, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 50])\n",
      "Shape after pool: torch.Size([64, 64, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 128])\n",
      "Shape after dropout1: torch.Size([64, 25, 128])\n",
      "necessary shape for fc2 is 25*64, 25*64\n",
      "Shape after fc2: torch.Size([64, 25, 1600])\n",
      "Shape after dropout2: torch.Size([64, 25, 1600])\n",
      "Shape after flatten: torch.Size([64, 40000])\n",
      "Shape of model_2_output: torch.Size([64, 12800])\n",
      "Shape of model_2_output_1 (concactenating last dimension of model_2_output and x): torch.Size([64, 52800])\n",
      "necessary shape for linear 528 * w, 6*w\n",
      "Shape of output_C2: torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 2D\n",
      "shape of input x_1, torch.Size([64, 100, 12])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "shape of input x, torch.Size([64, 100, 6, 2])\n",
      "inputs_2D_N: torch.Size([64, 2, 6, 100])\n",
      " inputs_2D_N: torch.Size([64, 2, 6, 100])\n",
      "inputs_2D_N: torch.Size([64, 100, 6, 2])\n",
      "Shape after transpose: torch.Size([64, 2, 6, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 6, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 6, 100])\n",
      "Shape after pool: torch.Size([64, 64, 3, 50])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 3, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 3, 50])\n",
      "Shape after pool: torch.Size([64, 64, 1, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 1, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 1, 1600])\n",
      "Shape after dropout: torch.Size([64, 25, 1, 1600])\n",
      "necessary shape for fc2 is 64, 32\n",
      "Shape after dropout2: torch.Size([64, 25, 1, 64])\n",
      "Shape after flatten: torch.Size([64, 1600])\n",
      "Shape of model_3_output: torch.Size([64, 12800])\n",
      "Shape of model_3_output after cat: torch.Size([64, 14400])\n",
      "Shape of output_C1: torch.Size([64, 100, 3])\n",
      "output: torch.Size([64, 100, 3])\n",
      "data_features_1D shape: torch.Size([64, 100, 48])\n",
      "data_features_2D shape: torch.Size([64, 100, 6, 8])\n",
      "data_targets shape: torch.Size([64, 100, 3])\n",
      "##################################\n",
      "first passage through encoder gru\n",
      "input to encoder_gru, torch.Size([64, 100, 12])\n",
      "shape of out_1 after lstm1, torch.Size([64, 100, 256])\n",
      "shape of out_1 dropout1, torch.Size([64, 100, 256])\n",
      "shape of out_2 after lstm1, torch.Size([64, 100, 128])\n",
      "shape of out_2 dropout2, torch.Size([64, 100, 128])\n",
      "shape of out_2 after flatten, torch.Size([64, 12800])\n",
      "necessary shape for last linear layer = 128000, 600\n",
      "shape encoder_gru output, torch.Size([64, 300])\n",
      "shape encoder_gru output after view, torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 1D\n",
      "shape of input, torch.Size([64, 100, 12])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "Shape after transpose: torch.Size([64, 12, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 100])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 100])\n",
      "Shape after pool: torch.Size([64, 64, 50])\n",
      "Shape after conv3 relu: torch.Size([64, 64, 50])\n",
      "Shape after conv4 relu: torch.Size([64, 64, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 50])\n",
      "Shape after pool: torch.Size([64, 64, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 128])\n",
      "Shape after dropout1: torch.Size([64, 25, 128])\n",
      "necessary shape for fc2 is 25*64, 25*64\n",
      "Shape after fc2: torch.Size([64, 25, 1600])\n",
      "Shape after dropout2: torch.Size([64, 25, 1600])\n",
      "Shape after flatten: torch.Size([64, 40000])\n",
      "Shape of model_2_output: torch.Size([64, 12800])\n",
      "Shape of model_2_output_1 (concactenating last dimension of model_2_output and x): torch.Size([64, 52800])\n",
      "necessary shape for linear 528 * w, 6*w\n",
      "Shape of output_C2: torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 2D\n",
      "shape of input x_1, torch.Size([64, 100, 12])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "shape of input x, torch.Size([64, 100, 6, 2])\n",
      "inputs_2D_N: torch.Size([64, 2, 6, 100])\n",
      " inputs_2D_N: torch.Size([64, 2, 6, 100])\n",
      "inputs_2D_N: torch.Size([64, 100, 6, 2])\n",
      "Shape after transpose: torch.Size([64, 2, 6, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 6, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 6, 100])\n",
      "Shape after pool: torch.Size([64, 64, 3, 50])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 3, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 3, 50])\n",
      "Shape after pool: torch.Size([64, 64, 1, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 1, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 1, 1600])\n",
      "Shape after dropout: torch.Size([64, 25, 1, 1600])\n",
      "necessary shape for fc2 is 64, 32\n",
      "Shape after dropout2: torch.Size([64, 25, 1, 64])\n",
      "Shape after flatten: torch.Size([64, 1600])\n",
      "Shape of model_3_output: torch.Size([64, 12800])\n",
      "Shape of model_3_output after cat: torch.Size([64, 14400])\n",
      "Shape of output_C1: torch.Size([64, 100, 3])\n",
      "output: torch.Size([64, 100, 3])\n",
      "data_features_1D shape: torch.Size([64, 100, 48])\n",
      "data_features_2D shape: torch.Size([64, 100, 6, 8])\n",
      "data_targets shape: torch.Size([64, 100, 3])\n",
      "##################################\n",
      "first passage through encoder gru\n",
      "input to encoder_gru, torch.Size([64, 100, 12])\n",
      "shape of out_1 after lstm1, torch.Size([64, 100, 256])\n",
      "shape of out_1 dropout1, torch.Size([64, 100, 256])\n",
      "shape of out_2 after lstm1, torch.Size([64, 100, 128])\n",
      "shape of out_2 dropout2, torch.Size([64, 100, 128])\n",
      "shape of out_2 after flatten, torch.Size([64, 12800])\n",
      "necessary shape for last linear layer = 128000, 600\n",
      "shape encoder_gru output, torch.Size([64, 300])\n",
      "shape encoder_gru output after view, torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 1D\n",
      "shape of input, torch.Size([64, 100, 12])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "Shape after transpose: torch.Size([64, 12, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 100])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 100])\n",
      "Shape after pool: torch.Size([64, 64, 50])\n",
      "Shape after conv3 relu: torch.Size([64, 64, 50])\n",
      "Shape after conv4 relu: torch.Size([64, 64, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 50])\n",
      "Shape after pool: torch.Size([64, 64, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 128])\n",
      "Shape after dropout1: torch.Size([64, 25, 128])\n",
      "necessary shape for fc2 is 25*64, 25*64\n",
      "Shape after fc2: torch.Size([64, 25, 1600])\n",
      "Shape after dropout2: torch.Size([64, 25, 1600])\n",
      "Shape after flatten: torch.Size([64, 40000])\n",
      "Shape of model_2_output: torch.Size([64, 12800])\n",
      "Shape of model_2_output_1 (concactenating last dimension of model_2_output and x): torch.Size([64, 52800])\n",
      "necessary shape for linear 528 * w, 6*w\n",
      "Shape of output_C2: torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 2D\n",
      "shape of input x_1, torch.Size([64, 100, 12])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "shape of input x, torch.Size([64, 100, 6, 2])\n",
      "inputs_2D_N: torch.Size([64, 2, 6, 100])\n",
      " inputs_2D_N: torch.Size([64, 2, 6, 100])\n",
      "inputs_2D_N: torch.Size([64, 100, 6, 2])\n",
      "Shape after transpose: torch.Size([64, 2, 6, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 6, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 6, 100])\n",
      "Shape after pool: torch.Size([64, 64, 3, 50])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 3, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 3, 50])\n",
      "Shape after pool: torch.Size([64, 64, 1, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 1, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 1, 1600])\n",
      "Shape after dropout: torch.Size([64, 25, 1, 1600])\n",
      "necessary shape for fc2 is 64, 32\n",
      "Shape after dropout2: torch.Size([64, 25, 1, 64])\n",
      "Shape after flatten: torch.Size([64, 1600])\n",
      "Shape of model_3_output: torch.Size([64, 12800])\n",
      "Shape of model_3_output after cat: torch.Size([64, 14400])\n",
      "Shape of output_C1: torch.Size([64, 100, 3])\n",
      "output: torch.Size([64, 100, 3])\n",
      "data_features_1D shape: torch.Size([64, 100, 48])\n",
      "data_features_2D shape: torch.Size([64, 100, 6, 8])\n",
      "data_targets shape: torch.Size([64, 100, 3])\n",
      "##################################\n",
      "first passage through encoder gru\n",
      "input to encoder_gru, torch.Size([64, 100, 12])\n",
      "shape of out_1 after lstm1, torch.Size([64, 100, 256])\n",
      "shape of out_1 dropout1, torch.Size([64, 100, 256])\n",
      "shape of out_2 after lstm1, torch.Size([64, 100, 128])\n",
      "shape of out_2 dropout2, torch.Size([64, 100, 128])\n",
      "shape of out_2 after flatten, torch.Size([64, 12800])\n",
      "necessary shape for last linear layer = 128000, 600\n",
      "shape encoder_gru output, torch.Size([64, 300])\n",
      "shape encoder_gru output after view, torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 1D\n",
      "shape of input, torch.Size([64, 100, 12])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "Shape after transpose: torch.Size([64, 12, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 100])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 100])\n",
      "Shape after pool: torch.Size([64, 64, 50])\n",
      "Shape after conv3 relu: torch.Size([64, 64, 50])\n",
      "Shape after conv4 relu: torch.Size([64, 64, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 50])\n",
      "Shape after pool: torch.Size([64, 64, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 128])\n",
      "Shape after dropout1: torch.Size([64, 25, 128])\n",
      "necessary shape for fc2 is 25*64, 25*64\n",
      "Shape after fc2: torch.Size([64, 25, 1600])\n",
      "Shape after dropout2: torch.Size([64, 25, 1600])\n",
      "Shape after flatten: torch.Size([64, 40000])\n",
      "Shape of model_2_output: torch.Size([64, 12800])\n",
      "Shape of model_2_output_1 (concactenating last dimension of model_2_output and x): torch.Size([64, 52800])\n",
      "necessary shape for linear 528 * w, 6*w\n",
      "Shape of output_C2: torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 2D\n",
      "shape of input x_1, torch.Size([64, 100, 12])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "shape of input x, torch.Size([64, 100, 6, 2])\n",
      "inputs_2D_N: torch.Size([64, 2, 6, 100])\n",
      " inputs_2D_N: torch.Size([64, 2, 6, 100])\n",
      "inputs_2D_N: torch.Size([64, 100, 6, 2])\n",
      "Shape after transpose: torch.Size([64, 2, 6, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 6, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 6, 100])\n",
      "Shape after pool: torch.Size([64, 64, 3, 50])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 3, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 3, 50])\n",
      "Shape after pool: torch.Size([64, 64, 1, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 1, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 1, 1600])\n",
      "Shape after dropout: torch.Size([64, 25, 1, 1600])\n",
      "necessary shape for fc2 is 64, 32\n",
      "Shape after dropout2: torch.Size([64, 25, 1, 64])\n",
      "Shape after flatten: torch.Size([64, 1600])\n",
      "Shape of model_3_output: torch.Size([64, 12800])\n",
      "Shape of model_3_output after cat: torch.Size([64, 14400])\n",
      "Shape of output_C1: torch.Size([64, 100, 3])\n",
      "output: torch.Size([64, 100, 3])\n",
      "data_features_1D shape: torch.Size([64, 100, 48])\n",
      "data_features_2D shape: torch.Size([64, 100, 6, 8])\n",
      "data_targets shape: torch.Size([64, 100, 3])\n",
      "##################################\n",
      "first passage through encoder gru\n",
      "input to encoder_gru, torch.Size([64, 100, 12])\n",
      "shape of out_1 after lstm1, torch.Size([64, 100, 256])\n",
      "shape of out_1 dropout1, torch.Size([64, 100, 256])\n",
      "shape of out_2 after lstm1, torch.Size([64, 100, 128])\n",
      "shape of out_2 dropout2, torch.Size([64, 100, 128])\n",
      "shape of out_2 after flatten, torch.Size([64, 12800])\n",
      "necessary shape for last linear layer = 128000, 600\n",
      "shape encoder_gru output, torch.Size([64, 300])\n",
      "shape encoder_gru output after view, torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 1D\n",
      "shape of input, torch.Size([64, 100, 12])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "Shape after transpose: torch.Size([64, 12, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 100])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 100])\n",
      "Shape after pool: torch.Size([64, 64, 50])\n",
      "Shape after conv3 relu: torch.Size([64, 64, 50])\n",
      "Shape after conv4 relu: torch.Size([64, 64, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 50])\n",
      "Shape after pool: torch.Size([64, 64, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 128])\n",
      "Shape after dropout1: torch.Size([64, 25, 128])\n",
      "necessary shape for fc2 is 25*64, 25*64\n",
      "Shape after fc2: torch.Size([64, 25, 1600])\n",
      "Shape after dropout2: torch.Size([64, 25, 1600])\n",
      "Shape after flatten: torch.Size([64, 40000])\n",
      "Shape of model_2_output: torch.Size([64, 12800])\n",
      "Shape of model_2_output_1 (concactenating last dimension of model_2_output and x): torch.Size([64, 52800])\n",
      "necessary shape for linear 528 * w, 6*w\n",
      "Shape of output_C2: torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 2D\n",
      "shape of input x_1, torch.Size([64, 100, 12])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "shape of input x, torch.Size([64, 100, 6, 2])\n",
      "inputs_2D_N: torch.Size([64, 2, 6, 100])\n",
      " inputs_2D_N: torch.Size([64, 2, 6, 100])\n",
      "inputs_2D_N: torch.Size([64, 100, 6, 2])\n",
      "Shape after transpose: torch.Size([64, 2, 6, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 6, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 6, 100])\n",
      "Shape after pool: torch.Size([64, 64, 3, 50])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 3, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 3, 50])\n",
      "Shape after pool: torch.Size([64, 64, 1, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 1, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 1, 1600])\n",
      "Shape after dropout: torch.Size([64, 25, 1, 1600])\n",
      "necessary shape for fc2 is 64, 32\n",
      "Shape after dropout2: torch.Size([64, 25, 1, 64])\n",
      "Shape after flatten: torch.Size([64, 1600])\n",
      "Shape of model_3_output: torch.Size([64, 12800])\n",
      "Shape of model_3_output after cat: torch.Size([64, 14400])\n",
      "Shape of output_C1: torch.Size([64, 100, 3])\n",
      "output: torch.Size([64, 100, 3])\n",
      "data_features_1D shape: torch.Size([64, 100, 48])\n",
      "data_features_2D shape: torch.Size([64, 100, 6, 8])\n",
      "data_targets shape: torch.Size([64, 100, 3])\n",
      "##################################\n",
      "first passage through encoder gru\n",
      "input to encoder_gru, torch.Size([64, 100, 12])\n",
      "shape of out_1 after lstm1, torch.Size([64, 100, 256])\n",
      "shape of out_1 dropout1, torch.Size([64, 100, 256])\n",
      "shape of out_2 after lstm1, torch.Size([64, 100, 128])\n",
      "shape of out_2 dropout2, torch.Size([64, 100, 128])\n",
      "shape of out_2 after flatten, torch.Size([64, 12800])\n",
      "necessary shape for last linear layer = 128000, 600\n",
      "shape encoder_gru output, torch.Size([64, 300])\n",
      "shape encoder_gru output after view, torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 1D\n",
      "shape of input, torch.Size([64, 100, 12])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "Shape after transpose: torch.Size([64, 12, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 100])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 100])\n",
      "Shape after pool: torch.Size([64, 64, 50])\n",
      "Shape after conv3 relu: torch.Size([64, 64, 50])\n",
      "Shape after conv4 relu: torch.Size([64, 64, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 50])\n",
      "Shape after pool: torch.Size([64, 64, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 128])\n",
      "Shape after dropout1: torch.Size([64, 25, 128])\n",
      "necessary shape for fc2 is 25*64, 25*64\n",
      "Shape after fc2: torch.Size([64, 25, 1600])\n",
      "Shape after dropout2: torch.Size([64, 25, 1600])\n",
      "Shape after flatten: torch.Size([64, 40000])\n",
      "Shape of model_2_output: torch.Size([64, 12800])\n",
      "Shape of model_2_output_1 (concactenating last dimension of model_2_output and x): torch.Size([64, 52800])\n",
      "necessary shape for linear 528 * w, 6*w\n",
      "Shape of output_C2: torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 2D\n",
      "shape of input x_1, torch.Size([64, 100, 12])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "shape of input x, torch.Size([64, 100, 6, 2])\n",
      "inputs_2D_N: torch.Size([64, 2, 6, 100])\n",
      " inputs_2D_N: torch.Size([64, 2, 6, 100])\n",
      "inputs_2D_N: torch.Size([64, 100, 6, 2])\n",
      "Shape after transpose: torch.Size([64, 2, 6, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 6, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 6, 100])\n",
      "Shape after pool: torch.Size([64, 64, 3, 50])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 3, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 3, 50])\n",
      "Shape after pool: torch.Size([64, 64, 1, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 1, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 1, 1600])\n",
      "Shape after dropout: torch.Size([64, 25, 1, 1600])\n",
      "necessary shape for fc2 is 64, 32\n",
      "Shape after dropout2: torch.Size([64, 25, 1, 64])\n",
      "Shape after flatten: torch.Size([64, 1600])\n",
      "Shape of model_3_output: torch.Size([64, 12800])\n",
      "Shape of model_3_output after cat: torch.Size([64, 14400])\n",
      "Shape of output_C1: torch.Size([64, 100, 3])\n",
      "output: torch.Size([64, 100, 3])\n",
      "data_features_1D shape: torch.Size([64, 100, 48])\n",
      "data_features_2D shape: torch.Size([64, 100, 6, 8])\n",
      "data_targets shape: torch.Size([64, 100, 3])\n",
      "##################################\n",
      "first passage through encoder gru\n",
      "input to encoder_gru, torch.Size([64, 100, 12])\n",
      "shape of out_1 after lstm1, torch.Size([64, 100, 256])\n",
      "shape of out_1 dropout1, torch.Size([64, 100, 256])\n",
      "shape of out_2 after lstm1, torch.Size([64, 100, 128])\n",
      "shape of out_2 dropout2, torch.Size([64, 100, 128])\n",
      "shape of out_2 after flatten, torch.Size([64, 12800])\n",
      "necessary shape for last linear layer = 128000, 600\n",
      "shape encoder_gru output, torch.Size([64, 300])\n",
      "shape encoder_gru output after view, torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 1D\n",
      "shape of input, torch.Size([64, 100, 12])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "Shape after transpose: torch.Size([64, 12, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 100])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 100])\n",
      "Shape after pool: torch.Size([64, 64, 50])\n",
      "Shape after conv3 relu: torch.Size([64, 64, 50])\n",
      "Shape after conv4 relu: torch.Size([64, 64, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 50])\n",
      "Shape after pool: torch.Size([64, 64, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 128])\n",
      "Shape after dropout1: torch.Size([64, 25, 128])\n",
      "necessary shape for fc2 is 25*64, 25*64\n",
      "Shape after fc2: torch.Size([64, 25, 1600])\n",
      "Shape after dropout2: torch.Size([64, 25, 1600])\n",
      "Shape after flatten: torch.Size([64, 40000])\n",
      "Shape of model_2_output: torch.Size([64, 12800])\n",
      "Shape of model_2_output_1 (concactenating last dimension of model_2_output and x): torch.Size([64, 52800])\n",
      "necessary shape for linear 528 * w, 6*w\n",
      "Shape of output_C2: torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 2D\n",
      "shape of input x_1, torch.Size([64, 100, 12])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "shape of input x, torch.Size([64, 100, 6, 2])\n",
      "inputs_2D_N: torch.Size([64, 2, 6, 100])\n",
      " inputs_2D_N: torch.Size([64, 2, 6, 100])\n",
      "inputs_2D_N: torch.Size([64, 100, 6, 2])\n",
      "Shape after transpose: torch.Size([64, 2, 6, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 6, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 6, 100])\n",
      "Shape after pool: torch.Size([64, 64, 3, 50])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 3, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 3, 50])\n",
      "Shape after pool: torch.Size([64, 64, 1, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 1, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 1, 1600])\n",
      "Shape after dropout: torch.Size([64, 25, 1, 1600])\n",
      "necessary shape for fc2 is 64, 32\n",
      "Shape after dropout2: torch.Size([64, 25, 1, 64])\n",
      "Shape after flatten: torch.Size([64, 1600])\n",
      "Shape of model_3_output: torch.Size([64, 12800])\n",
      "Shape of model_3_output after cat: torch.Size([64, 14400])\n",
      "Shape of output_C1: torch.Size([64, 100, 3])\n",
      "output: torch.Size([64, 100, 3])\n",
      "data_features_1D shape: torch.Size([64, 100, 48])\n",
      "data_features_2D shape: torch.Size([64, 100, 6, 8])\n",
      "data_targets shape: torch.Size([64, 100, 3])\n",
      "##################################\n",
      "first passage through encoder gru\n",
      "input to encoder_gru, torch.Size([64, 100, 12])\n",
      "shape of out_1 after lstm1, torch.Size([64, 100, 256])\n",
      "shape of out_1 dropout1, torch.Size([64, 100, 256])\n",
      "shape of out_2 after lstm1, torch.Size([64, 100, 128])\n",
      "shape of out_2 dropout2, torch.Size([64, 100, 128])\n",
      "shape of out_2 after flatten, torch.Size([64, 12800])\n",
      "necessary shape for last linear layer = 128000, 600\n",
      "shape encoder_gru output, torch.Size([64, 300])\n",
      "shape encoder_gru output after view, torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 1D\n",
      "shape of input, torch.Size([64, 100, 12])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "Shape after transpose: torch.Size([64, 12, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 100])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 100])\n",
      "Shape after pool: torch.Size([64, 64, 50])\n",
      "Shape after conv3 relu: torch.Size([64, 64, 50])\n",
      "Shape after conv4 relu: torch.Size([64, 64, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 50])\n",
      "Shape after pool: torch.Size([64, 64, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 128])\n",
      "Shape after dropout1: torch.Size([64, 25, 128])\n",
      "necessary shape for fc2 is 25*64, 25*64\n",
      "Shape after fc2: torch.Size([64, 25, 1600])\n",
      "Shape after dropout2: torch.Size([64, 25, 1600])\n",
      "Shape after flatten: torch.Size([64, 40000])\n",
      "Shape of model_2_output: torch.Size([64, 12800])\n",
      "Shape of model_2_output_1 (concactenating last dimension of model_2_output and x): torch.Size([64, 52800])\n",
      "necessary shape for linear 528 * w, 6*w\n",
      "Shape of output_C2: torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 2D\n",
      "shape of input x_1, torch.Size([64, 100, 12])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "shape of input x, torch.Size([64, 100, 6, 2])\n",
      "inputs_2D_N: torch.Size([64, 2, 6, 100])\n",
      " inputs_2D_N: torch.Size([64, 2, 6, 100])\n",
      "inputs_2D_N: torch.Size([64, 100, 6, 2])\n",
      "Shape after transpose: torch.Size([64, 2, 6, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 6, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 6, 100])\n",
      "Shape after pool: torch.Size([64, 64, 3, 50])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 3, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 3, 50])\n",
      "Shape after pool: torch.Size([64, 64, 1, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 1, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 1, 1600])\n",
      "Shape after dropout: torch.Size([64, 25, 1, 1600])\n",
      "necessary shape for fc2 is 64, 32\n",
      "Shape after dropout2: torch.Size([64, 25, 1, 64])\n",
      "Shape after flatten: torch.Size([64, 1600])\n",
      "Shape of model_3_output: torch.Size([64, 12800])\n",
      "Shape of model_3_output after cat: torch.Size([64, 14400])\n",
      "Shape of output_C1: torch.Size([64, 100, 3])\n",
      "output: torch.Size([64, 100, 3])\n",
      "data_features_1D shape: torch.Size([64, 100, 48])\n",
      "data_features_2D shape: torch.Size([64, 100, 6, 8])\n",
      "data_targets shape: torch.Size([64, 100, 3])\n",
      "##################################\n",
      "first passage through encoder gru\n",
      "input to encoder_gru, torch.Size([64, 100, 12])\n",
      "shape of out_1 after lstm1, torch.Size([64, 100, 256])\n",
      "shape of out_1 dropout1, torch.Size([64, 100, 256])\n",
      "shape of out_2 after lstm1, torch.Size([64, 100, 128])\n",
      "shape of out_2 dropout2, torch.Size([64, 100, 128])\n",
      "shape of out_2 after flatten, torch.Size([64, 12800])\n",
      "necessary shape for last linear layer = 128000, 600\n",
      "shape encoder_gru output, torch.Size([64, 300])\n",
      "shape encoder_gru output after view, torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 1D\n",
      "shape of input, torch.Size([64, 100, 12])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "Shape after transpose: torch.Size([64, 12, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 100])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 100])\n",
      "Shape after pool: torch.Size([64, 64, 50])\n",
      "Shape after conv3 relu: torch.Size([64, 64, 50])\n",
      "Shape after conv4 relu: torch.Size([64, 64, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 50])\n",
      "Shape after pool: torch.Size([64, 64, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 128])\n",
      "Shape after dropout1: torch.Size([64, 25, 128])\n",
      "necessary shape for fc2 is 25*64, 25*64\n",
      "Shape after fc2: torch.Size([64, 25, 1600])\n",
      "Shape after dropout2: torch.Size([64, 25, 1600])\n",
      "Shape after flatten: torch.Size([64, 40000])\n",
      "Shape of model_2_output: torch.Size([64, 12800])\n",
      "Shape of model_2_output_1 (concactenating last dimension of model_2_output and x): torch.Size([64, 52800])\n",
      "necessary shape for linear 528 * w, 6*w\n",
      "Shape of output_C2: torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 2D\n",
      "shape of input x_1, torch.Size([64, 100, 12])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "shape of input x, torch.Size([64, 100, 6, 2])\n",
      "inputs_2D_N: torch.Size([64, 2, 6, 100])\n",
      " inputs_2D_N: torch.Size([64, 2, 6, 100])\n",
      "inputs_2D_N: torch.Size([64, 100, 6, 2])\n",
      "Shape after transpose: torch.Size([64, 2, 6, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 6, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 6, 100])\n",
      "Shape after pool: torch.Size([64, 64, 3, 50])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 3, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 3, 50])\n",
      "Shape after pool: torch.Size([64, 64, 1, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 1, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 1, 1600])\n",
      "Shape after dropout: torch.Size([64, 25, 1, 1600])\n",
      "necessary shape for fc2 is 64, 32\n",
      "Shape after dropout2: torch.Size([64, 25, 1, 64])\n",
      "Shape after flatten: torch.Size([64, 1600])\n",
      "Shape of model_3_output: torch.Size([64, 12800])\n",
      "Shape of model_3_output after cat: torch.Size([64, 14400])\n",
      "Shape of output_C1: torch.Size([64, 100, 3])\n",
      "output: torch.Size([64, 100, 3])\n",
      "data_features_1D shape: torch.Size([64, 100, 48])\n",
      "data_features_2D shape: torch.Size([64, 100, 6, 8])\n",
      "data_targets shape: torch.Size([64, 100, 3])\n",
      "##################################\n",
      "first passage through encoder gru\n",
      "input to encoder_gru, torch.Size([64, 100, 12])\n",
      "shape of out_1 after lstm1, torch.Size([64, 100, 256])\n",
      "shape of out_1 dropout1, torch.Size([64, 100, 256])\n",
      "shape of out_2 after lstm1, torch.Size([64, 100, 128])\n",
      "shape of out_2 dropout2, torch.Size([64, 100, 128])\n",
      "shape of out_2 after flatten, torch.Size([64, 12800])\n",
      "necessary shape for last linear layer = 128000, 600\n",
      "shape encoder_gru output, torch.Size([64, 300])\n",
      "shape encoder_gru output after view, torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 1D\n",
      "shape of input, torch.Size([64, 100, 12])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "Shape after transpose: torch.Size([64, 12, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 100])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 100])\n",
      "Shape after pool: torch.Size([64, 64, 50])\n",
      "Shape after conv3 relu: torch.Size([64, 64, 50])\n",
      "Shape after conv4 relu: torch.Size([64, 64, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 50])\n",
      "Shape after pool: torch.Size([64, 64, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 128])\n",
      "Shape after dropout1: torch.Size([64, 25, 128])\n",
      "necessary shape for fc2 is 25*64, 25*64\n",
      "Shape after fc2: torch.Size([64, 25, 1600])\n",
      "Shape after dropout2: torch.Size([64, 25, 1600])\n",
      "Shape after flatten: torch.Size([64, 40000])\n",
      "Shape of model_2_output: torch.Size([64, 12800])\n",
      "Shape of model_2_output_1 (concactenating last dimension of model_2_output and x): torch.Size([64, 52800])\n",
      "necessary shape for linear 528 * w, 6*w\n",
      "Shape of output_C2: torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 2D\n",
      "shape of input x_1, torch.Size([64, 100, 12])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "shape of input x, torch.Size([64, 100, 6, 2])\n",
      "inputs_2D_N: torch.Size([64, 2, 6, 100])\n",
      " inputs_2D_N: torch.Size([64, 2, 6, 100])\n",
      "inputs_2D_N: torch.Size([64, 100, 6, 2])\n",
      "Shape after transpose: torch.Size([64, 2, 6, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 6, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 6, 100])\n",
      "Shape after pool: torch.Size([64, 64, 3, 50])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 3, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 3, 50])\n",
      "Shape after pool: torch.Size([64, 64, 1, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 1, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 1, 1600])\n",
      "Shape after dropout: torch.Size([64, 25, 1, 1600])\n",
      "necessary shape for fc2 is 64, 32\n",
      "Shape after dropout2: torch.Size([64, 25, 1, 64])\n",
      "Shape after flatten: torch.Size([64, 1600])\n",
      "Shape of model_3_output: torch.Size([64, 12800])\n",
      "Shape of model_3_output after cat: torch.Size([64, 14400])\n",
      "Shape of output_C1: torch.Size([64, 100, 3])\n",
      "output: torch.Size([64, 100, 3])\n",
      "data_features_1D shape: torch.Size([64, 100, 48])\n",
      "data_features_2D shape: torch.Size([64, 100, 6, 8])\n",
      "data_targets shape: torch.Size([64, 100, 3])\n",
      "##################################\n",
      "first passage through encoder gru\n",
      "input to encoder_gru, torch.Size([64, 100, 12])\n",
      "shape of out_1 after lstm1, torch.Size([64, 100, 256])\n",
      "shape of out_1 dropout1, torch.Size([64, 100, 256])\n",
      "shape of out_2 after lstm1, torch.Size([64, 100, 128])\n",
      "shape of out_2 dropout2, torch.Size([64, 100, 128])\n",
      "shape of out_2 after flatten, torch.Size([64, 12800])\n",
      "necessary shape for last linear layer = 128000, 600\n",
      "shape encoder_gru output, torch.Size([64, 300])\n",
      "shape encoder_gru output after view, torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 1D\n",
      "shape of input, torch.Size([64, 100, 12])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "Shape after transpose: torch.Size([64, 12, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 100])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 100])\n",
      "Shape after pool: torch.Size([64, 64, 50])\n",
      "Shape after conv3 relu: torch.Size([64, 64, 50])\n",
      "Shape after conv4 relu: torch.Size([64, 64, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 50])\n",
      "Shape after pool: torch.Size([64, 64, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 128])\n",
      "Shape after dropout1: torch.Size([64, 25, 128])\n",
      "necessary shape for fc2 is 25*64, 25*64\n",
      "Shape after fc2: torch.Size([64, 25, 1600])\n",
      "Shape after dropout2: torch.Size([64, 25, 1600])\n",
      "Shape after flatten: torch.Size([64, 40000])\n",
      "Shape of model_2_output: torch.Size([64, 12800])\n",
      "Shape of model_2_output_1 (concactenating last dimension of model_2_output and x): torch.Size([64, 52800])\n",
      "necessary shape for linear 528 * w, 6*w\n",
      "Shape of output_C2: torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 2D\n",
      "shape of input x_1, torch.Size([64, 100, 12])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "shape of input x, torch.Size([64, 100, 6, 2])\n",
      "inputs_2D_N: torch.Size([64, 2, 6, 100])\n",
      " inputs_2D_N: torch.Size([64, 2, 6, 100])\n",
      "inputs_2D_N: torch.Size([64, 100, 6, 2])\n",
      "Shape after transpose: torch.Size([64, 2, 6, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 6, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 6, 100])\n",
      "Shape after pool: torch.Size([64, 64, 3, 50])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 3, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 3, 50])\n",
      "Shape after pool: torch.Size([64, 64, 1, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 1, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 1, 1600])\n",
      "Shape after dropout: torch.Size([64, 25, 1, 1600])\n",
      "necessary shape for fc2 is 64, 32\n",
      "Shape after dropout2: torch.Size([64, 25, 1, 64])\n",
      "Shape after flatten: torch.Size([64, 1600])\n",
      "Shape of model_3_output: torch.Size([64, 12800])\n",
      "Shape of model_3_output after cat: torch.Size([64, 14400])\n",
      "Shape of output_C1: torch.Size([64, 100, 3])\n",
      "output: torch.Size([64, 100, 3])\n",
      "data_features_1D shape: torch.Size([64, 100, 48])\n",
      "data_features_2D shape: torch.Size([64, 100, 6, 8])\n",
      "data_targets shape: torch.Size([64, 100, 3])\n",
      "##################################\n",
      "first passage through encoder gru\n",
      "input to encoder_gru, torch.Size([64, 100, 12])\n",
      "shape of out_1 after lstm1, torch.Size([64, 100, 256])\n",
      "shape of out_1 dropout1, torch.Size([64, 100, 256])\n",
      "shape of out_2 after lstm1, torch.Size([64, 100, 128])\n",
      "shape of out_2 dropout2, torch.Size([64, 100, 128])\n",
      "shape of out_2 after flatten, torch.Size([64, 12800])\n",
      "necessary shape for last linear layer = 128000, 600\n",
      "shape encoder_gru output, torch.Size([64, 300])\n",
      "shape encoder_gru output after view, torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 1D\n",
      "shape of input, torch.Size([64, 100, 12])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "Shape after transpose: torch.Size([64, 12, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 100])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 100])\n",
      "Shape after pool: torch.Size([64, 64, 50])\n",
      "Shape after conv3 relu: torch.Size([64, 64, 50])\n",
      "Shape after conv4 relu: torch.Size([64, 64, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 50])\n",
      "Shape after pool: torch.Size([64, 64, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 128])\n",
      "Shape after dropout1: torch.Size([64, 25, 128])\n",
      "necessary shape for fc2 is 25*64, 25*64\n",
      "Shape after fc2: torch.Size([64, 25, 1600])\n",
      "Shape after dropout2: torch.Size([64, 25, 1600])\n",
      "Shape after flatten: torch.Size([64, 40000])\n",
      "Shape of model_2_output: torch.Size([64, 12800])\n",
      "Shape of model_2_output_1 (concactenating last dimension of model_2_output and x): torch.Size([64, 52800])\n",
      "necessary shape for linear 528 * w, 6*w\n",
      "Shape of output_C2: torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 2D\n",
      "shape of input x_1, torch.Size([64, 100, 12])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "shape of input x, torch.Size([64, 100, 6, 2])\n",
      "inputs_2D_N: torch.Size([64, 2, 6, 100])\n",
      " inputs_2D_N: torch.Size([64, 2, 6, 100])\n",
      "inputs_2D_N: torch.Size([64, 100, 6, 2])\n",
      "Shape after transpose: torch.Size([64, 2, 6, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 6, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 6, 100])\n",
      "Shape after pool: torch.Size([64, 64, 3, 50])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 3, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 3, 50])\n",
      "Shape after pool: torch.Size([64, 64, 1, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 1, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 1, 1600])\n",
      "Shape after dropout: torch.Size([64, 25, 1, 1600])\n",
      "necessary shape for fc2 is 64, 32\n",
      "Shape after dropout2: torch.Size([64, 25, 1, 64])\n",
      "Shape after flatten: torch.Size([64, 1600])\n",
      "Shape of model_3_output: torch.Size([64, 12800])\n",
      "Shape of model_3_output after cat: torch.Size([64, 14400])\n",
      "Shape of output_C1: torch.Size([64, 100, 3])\n",
      "output: torch.Size([64, 100, 3])\n",
      "data_features_1D shape: torch.Size([8, 100, 48])\n",
      "data_features_2D shape: torch.Size([8, 100, 6, 8])\n",
      "data_targets shape: torch.Size([8, 100, 3])\n",
      "##################################\n",
      "first passage through encoder gru\n",
      "input to encoder_gru, torch.Size([8, 100, 12])\n",
      "shape of out_1 after lstm1, torch.Size([8, 100, 256])\n",
      "shape of out_1 dropout1, torch.Size([8, 100, 256])\n",
      "shape of out_2 after lstm1, torch.Size([8, 100, 128])\n",
      "shape of out_2 dropout2, torch.Size([8, 100, 128])\n",
      "shape of out_2 after flatten, torch.Size([8, 12800])\n",
      "necessary shape for last linear layer = 128000, 600\n",
      "shape encoder_gru output, torch.Size([8, 300])\n",
      "shape encoder_gru output after view, torch.Size([8, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 1D\n",
      "shape of input, torch.Size([8, 100, 12])\n",
      "input_1D_N_1: torch.Size([8, 12, 100])\n",
      "input_1D_N_1: torch.Size([8, 12, 100])\n",
      "input_1D_N_1: torch.Size([8, 12, 100])\n",
      "Shape after transpose: torch.Size([8, 12, 100])\n",
      "Shape after conv1 relu: torch.Size([8, 64, 100])\n",
      "Shape after conv2 relu: torch.Size([8, 64, 100])\n",
      "Shape after BN_2: torch.Size([8, 64, 100])\n",
      "Shape after pool: torch.Size([8, 64, 50])\n",
      "Shape after conv3 relu: torch.Size([8, 64, 50])\n",
      "Shape after conv4 relu: torch.Size([8, 64, 50])\n",
      "Shape after BN_4: torch.Size([8, 64, 50])\n",
      "Shape after pool: torch.Size([8, 64, 25])\n",
      "Shape after transpose: torch.Size([8, 25, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([8, 25, 128])\n",
      "Shape after dropout1: torch.Size([8, 25, 128])\n",
      "necessary shape for fc2 is 25*64, 25*64\n",
      "Shape after fc2: torch.Size([8, 25, 1600])\n",
      "Shape after dropout2: torch.Size([8, 25, 1600])\n",
      "Shape after flatten: torch.Size([8, 40000])\n",
      "Shape of model_2_output: torch.Size([8, 12800])\n",
      "Shape of model_2_output_1 (concactenating last dimension of model_2_output and x): torch.Size([8, 52800])\n",
      "necessary shape for linear 528 * w, 6*w\n",
      "Shape of output_C2: torch.Size([8, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 2D\n",
      "shape of input x_1, torch.Size([8, 100, 12])\n",
      "input_1D_N_1: torch.Size([8, 12, 100])\n",
      "input_1D_N_1: torch.Size([8, 12, 100])\n",
      "input_1D_N_1: torch.Size([8, 12, 100])\n",
      "shape of input x, torch.Size([8, 100, 6, 2])\n",
      "inputs_2D_N: torch.Size([8, 2, 6, 100])\n",
      " inputs_2D_N: torch.Size([8, 2, 6, 100])\n",
      "inputs_2D_N: torch.Size([8, 100, 6, 2])\n",
      "Shape after transpose: torch.Size([8, 2, 6, 100])\n",
      "Shape after conv1 relu: torch.Size([8, 64, 6, 100])\n",
      "Shape after BN_2: torch.Size([8, 64, 6, 100])\n",
      "Shape after pool: torch.Size([8, 64, 3, 50])\n",
      "Shape after conv2 relu: torch.Size([8, 64, 3, 50])\n",
      "Shape after BN_4: torch.Size([8, 64, 3, 50])\n",
      "Shape after pool: torch.Size([8, 64, 1, 25])\n",
      "Shape after transpose: torch.Size([8, 25, 1, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([8, 25, 1, 1600])\n",
      "Shape after dropout: torch.Size([8, 25, 1, 1600])\n",
      "necessary shape for fc2 is 64, 32\n",
      "Shape after dropout2: torch.Size([8, 25, 1, 64])\n",
      "Shape after flatten: torch.Size([8, 1600])\n",
      "Shape of model_3_output: torch.Size([8, 12800])\n",
      "Shape of model_3_output after cat: torch.Size([8, 14400])\n",
      "Shape of output_C1: torch.Size([8, 100, 3])\n",
      "output: torch.Size([8, 100, 3])\n",
      "##################################\n",
      "first passage through encoder gru\n",
      "input to encoder_gru, torch.Size([64, 100, 12])\n",
      "shape of out_1 after lstm1, torch.Size([64, 100, 256])\n",
      "shape of out_1 dropout1, torch.Size([64, 100, 256])\n",
      "shape of out_2 after lstm1, torch.Size([64, 100, 128])\n",
      "shape of out_2 dropout2, torch.Size([64, 100, 128])\n",
      "shape of out_2 after flatten, torch.Size([64, 12800])\n",
      "necessary shape for last linear layer = 128000, 600\n",
      "shape encoder_gru output, torch.Size([64, 300])\n",
      "shape encoder_gru output after view, torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 1D\n",
      "shape of input, torch.Size([64, 100, 12])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "Shape after transpose: torch.Size([64, 12, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 100])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 100])\n",
      "Shape after pool: torch.Size([64, 64, 50])\n",
      "Shape after conv3 relu: torch.Size([64, 64, 50])\n",
      "Shape after conv4 relu: torch.Size([64, 64, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 50])\n",
      "Shape after pool: torch.Size([64, 64, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 128])\n",
      "Shape after dropout1: torch.Size([64, 25, 128])\n",
      "necessary shape for fc2 is 25*64, 25*64\n",
      "Shape after fc2: torch.Size([64, 25, 1600])\n",
      "Shape after dropout2: torch.Size([64, 25, 1600])\n",
      "Shape after flatten: torch.Size([64, 40000])\n",
      "Shape of model_2_output: torch.Size([64, 12800])\n",
      "Shape of model_2_output_1 (concactenating last dimension of model_2_output and x): torch.Size([64, 52800])\n",
      "necessary shape for linear 528 * w, 6*w\n",
      "Shape of output_C2: torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 2D\n",
      "shape of input x_1, torch.Size([64, 100, 12])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "shape of input x, torch.Size([64, 100, 6, 2])\n",
      "inputs_2D_N: torch.Size([64, 2, 6, 100])\n",
      " inputs_2D_N: torch.Size([64, 2, 6, 100])\n",
      "inputs_2D_N: torch.Size([64, 100, 6, 2])\n",
      "Shape after transpose: torch.Size([64, 2, 6, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 6, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 6, 100])\n",
      "Shape after pool: torch.Size([64, 64, 3, 50])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 3, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 3, 50])\n",
      "Shape after pool: torch.Size([64, 64, 1, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 1, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 1, 1600])\n",
      "Shape after dropout: torch.Size([64, 25, 1, 1600])\n",
      "necessary shape for fc2 is 64, 32\n",
      "Shape after dropout2: torch.Size([64, 25, 1, 64])\n",
      "Shape after flatten: torch.Size([64, 1600])\n",
      "Shape of model_3_output: torch.Size([64, 12800])\n",
      "Shape of model_3_output after cat: torch.Size([64, 14400])\n",
      "Shape of output_C1: torch.Size([64, 100, 3])\n",
      "##################################\n",
      "first passage through encoder gru\n",
      "input to encoder_gru, torch.Size([64, 100, 12])\n",
      "shape of out_1 after lstm1, torch.Size([64, 100, 256])\n",
      "shape of out_1 dropout1, torch.Size([64, 100, 256])\n",
      "shape of out_2 after lstm1, torch.Size([64, 100, 128])\n",
      "shape of out_2 dropout2, torch.Size([64, 100, 128])\n",
      "shape of out_2 after flatten, torch.Size([64, 12800])\n",
      "necessary shape for last linear layer = 128000, 600\n",
      "shape encoder_gru output, torch.Size([64, 300])\n",
      "shape encoder_gru output after view, torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 1D\n",
      "shape of input, torch.Size([64, 100, 12])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "Shape after transpose: torch.Size([64, 12, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 100])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 100])\n",
      "Shape after pool: torch.Size([64, 64, 50])\n",
      "Shape after conv3 relu: torch.Size([64, 64, 50])\n",
      "Shape after conv4 relu: torch.Size([64, 64, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 50])\n",
      "Shape after pool: torch.Size([64, 64, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 128])\n",
      "Shape after dropout1: torch.Size([64, 25, 128])\n",
      "necessary shape for fc2 is 25*64, 25*64\n",
      "Shape after fc2: torch.Size([64, 25, 1600])\n",
      "Shape after dropout2: torch.Size([64, 25, 1600])\n",
      "Shape after flatten: torch.Size([64, 40000])\n",
      "Shape of model_2_output: torch.Size([64, 12800])\n",
      "Shape of model_2_output_1 (concactenating last dimension of model_2_output and x): torch.Size([64, 52800])\n",
      "necessary shape for linear 528 * w, 6*w\n",
      "Shape of output_C2: torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 2D\n",
      "shape of input x_1, torch.Size([64, 100, 12])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "shape of input x, torch.Size([64, 100, 6, 2])\n",
      "inputs_2D_N: torch.Size([64, 2, 6, 100])\n",
      " inputs_2D_N: torch.Size([64, 2, 6, 100])\n",
      "inputs_2D_N: torch.Size([64, 100, 6, 2])\n",
      "Shape after transpose: torch.Size([64, 2, 6, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 6, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 6, 100])\n",
      "Shape after pool: torch.Size([64, 64, 3, 50])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 3, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 3, 50])\n",
      "Shape after pool: torch.Size([64, 64, 1, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 1, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 1, 1600])\n",
      "Shape after dropout: torch.Size([64, 25, 1, 1600])\n",
      "necessary shape for fc2 is 64, 32\n",
      "Shape after dropout2: torch.Size([64, 25, 1, 64])\n",
      "Shape after flatten: torch.Size([64, 1600])\n",
      "Shape of model_3_output: torch.Size([64, 12800])\n",
      "Shape of model_3_output after cat: torch.Size([64, 14400])\n",
      "Shape of output_C1: torch.Size([64, 100, 3])\n",
      "##################################\n",
      "first passage through encoder gru\n",
      "input to encoder_gru, torch.Size([64, 100, 12])\n",
      "shape of out_1 after lstm1, torch.Size([64, 100, 256])\n",
      "shape of out_1 dropout1, torch.Size([64, 100, 256])\n",
      "shape of out_2 after lstm1, torch.Size([64, 100, 128])\n",
      "shape of out_2 dropout2, torch.Size([64, 100, 128])\n",
      "shape of out_2 after flatten, torch.Size([64, 12800])\n",
      "necessary shape for last linear layer = 128000, 600\n",
      "shape encoder_gru output, torch.Size([64, 300])\n",
      "shape encoder_gru output after view, torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 1D\n",
      "shape of input, torch.Size([64, 100, 12])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "Shape after transpose: torch.Size([64, 12, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 100])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 100])\n",
      "Shape after pool: torch.Size([64, 64, 50])\n",
      "Shape after conv3 relu: torch.Size([64, 64, 50])\n",
      "Shape after conv4 relu: torch.Size([64, 64, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 50])\n",
      "Shape after pool: torch.Size([64, 64, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 128])\n",
      "Shape after dropout1: torch.Size([64, 25, 128])\n",
      "necessary shape for fc2 is 25*64, 25*64\n",
      "Shape after fc2: torch.Size([64, 25, 1600])\n",
      "Shape after dropout2: torch.Size([64, 25, 1600])\n",
      "Shape after flatten: torch.Size([64, 40000])\n",
      "Shape of model_2_output: torch.Size([64, 12800])\n",
      "Shape of model_2_output_1 (concactenating last dimension of model_2_output and x): torch.Size([64, 52800])\n",
      "necessary shape for linear 528 * w, 6*w\n",
      "Shape of output_C2: torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 2D\n",
      "shape of input x_1, torch.Size([64, 100, 12])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "shape of input x, torch.Size([64, 100, 6, 2])\n",
      "inputs_2D_N: torch.Size([64, 2, 6, 100])\n",
      " inputs_2D_N: torch.Size([64, 2, 6, 100])\n",
      "inputs_2D_N: torch.Size([64, 100, 6, 2])\n",
      "Shape after transpose: torch.Size([64, 2, 6, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 6, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 6, 100])\n",
      "Shape after pool: torch.Size([64, 64, 3, 50])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 3, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 3, 50])\n",
      "Shape after pool: torch.Size([64, 64, 1, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 1, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 1, 1600])\n",
      "Shape after dropout: torch.Size([64, 25, 1, 1600])\n",
      "necessary shape for fc2 is 64, 32\n",
      "Shape after dropout2: torch.Size([64, 25, 1, 64])\n",
      "Shape after flatten: torch.Size([64, 1600])\n",
      "Shape of model_3_output: torch.Size([64, 12800])\n",
      "Shape of model_3_output after cat: torch.Size([64, 14400])\n",
      "Shape of output_C1: torch.Size([64, 100, 3])\n",
      "##################################\n",
      "first passage through encoder gru\n",
      "input to encoder_gru, torch.Size([64, 100, 12])\n",
      "shape of out_1 after lstm1, torch.Size([64, 100, 256])\n",
      "shape of out_1 dropout1, torch.Size([64, 100, 256])\n",
      "shape of out_2 after lstm1, torch.Size([64, 100, 128])\n",
      "shape of out_2 dropout2, torch.Size([64, 100, 128])\n",
      "shape of out_2 after flatten, torch.Size([64, 12800])\n",
      "necessary shape for last linear layer = 128000, 600\n",
      "shape encoder_gru output, torch.Size([64, 300])\n",
      "shape encoder_gru output after view, torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 1D\n",
      "shape of input, torch.Size([64, 100, 12])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "Shape after transpose: torch.Size([64, 12, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 100])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 100])\n",
      "Shape after pool: torch.Size([64, 64, 50])\n",
      "Shape after conv3 relu: torch.Size([64, 64, 50])\n",
      "Shape after conv4 relu: torch.Size([64, 64, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 50])\n",
      "Shape after pool: torch.Size([64, 64, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 128])\n",
      "Shape after dropout1: torch.Size([64, 25, 128])\n",
      "necessary shape for fc2 is 25*64, 25*64\n",
      "Shape after fc2: torch.Size([64, 25, 1600])\n",
      "Shape after dropout2: torch.Size([64, 25, 1600])\n",
      "Shape after flatten: torch.Size([64, 40000])\n",
      "Shape of model_2_output: torch.Size([64, 12800])\n",
      "Shape of model_2_output_1 (concactenating last dimension of model_2_output and x): torch.Size([64, 52800])\n",
      "necessary shape for linear 528 * w, 6*w\n",
      "Shape of output_C2: torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 2D\n",
      "shape of input x_1, torch.Size([64, 100, 12])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "shape of input x, torch.Size([64, 100, 6, 2])\n",
      "inputs_2D_N: torch.Size([64, 2, 6, 100])\n",
      " inputs_2D_N: torch.Size([64, 2, 6, 100])\n",
      "inputs_2D_N: torch.Size([64, 100, 6, 2])\n",
      "Shape after transpose: torch.Size([64, 2, 6, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 6, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 6, 100])\n",
      "Shape after pool: torch.Size([64, 64, 3, 50])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 3, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 3, 50])\n",
      "Shape after pool: torch.Size([64, 64, 1, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 1, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 1, 1600])\n",
      "Shape after dropout: torch.Size([64, 25, 1, 1600])\n",
      "necessary shape for fc2 is 64, 32\n",
      "Shape after dropout2: torch.Size([64, 25, 1, 64])\n",
      "Shape after flatten: torch.Size([64, 1600])\n",
      "Shape of model_3_output: torch.Size([64, 12800])\n",
      "Shape of model_3_output after cat: torch.Size([64, 14400])\n",
      "Shape of output_C1: torch.Size([64, 100, 3])\n",
      "##################################\n",
      "first passage through encoder gru\n",
      "input to encoder_gru, torch.Size([2, 100, 12])\n",
      "shape of out_1 after lstm1, torch.Size([2, 100, 256])\n",
      "shape of out_1 dropout1, torch.Size([2, 100, 256])\n",
      "shape of out_2 after lstm1, torch.Size([2, 100, 128])\n",
      "shape of out_2 dropout2, torch.Size([2, 100, 128])\n",
      "shape of out_2 after flatten, torch.Size([2, 12800])\n",
      "necessary shape for last linear layer = 128000, 600\n",
      "shape encoder_gru output, torch.Size([2, 300])\n",
      "shape encoder_gru output after view, torch.Size([2, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 1D\n",
      "shape of input, torch.Size([2, 100, 12])\n",
      "input_1D_N_1: torch.Size([2, 12, 100])\n",
      "input_1D_N_1: torch.Size([2, 12, 100])\n",
      "input_1D_N_1: torch.Size([2, 12, 100])\n",
      "Shape after transpose: torch.Size([2, 12, 100])\n",
      "Shape after conv1 relu: torch.Size([2, 64, 100])\n",
      "Shape after conv2 relu: torch.Size([2, 64, 100])\n",
      "Shape after BN_2: torch.Size([2, 64, 100])\n",
      "Shape after pool: torch.Size([2, 64, 50])\n",
      "Shape after conv3 relu: torch.Size([2, 64, 50])\n",
      "Shape after conv4 relu: torch.Size([2, 64, 50])\n",
      "Shape after BN_4: torch.Size([2, 64, 50])\n",
      "Shape after pool: torch.Size([2, 64, 25])\n",
      "Shape after transpose: torch.Size([2, 25, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([2, 25, 128])\n",
      "Shape after dropout1: torch.Size([2, 25, 128])\n",
      "necessary shape for fc2 is 25*64, 25*64\n",
      "Shape after fc2: torch.Size([2, 25, 1600])\n",
      "Shape after dropout2: torch.Size([2, 25, 1600])\n",
      "Shape after flatten: torch.Size([2, 40000])\n",
      "Shape of model_2_output: torch.Size([2, 12800])\n",
      "Shape of model_2_output_1 (concactenating last dimension of model_2_output and x): torch.Size([2, 52800])\n",
      "necessary shape for linear 528 * w, 6*w\n",
      "Shape of output_C2: torch.Size([2, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 2D\n",
      "shape of input x_1, torch.Size([2, 100, 12])\n",
      "input_1D_N_1: torch.Size([2, 12, 100])\n",
      "input_1D_N_1: torch.Size([2, 12, 100])\n",
      "input_1D_N_1: torch.Size([2, 12, 100])\n",
      "shape of input x, torch.Size([2, 100, 6, 2])\n",
      "inputs_2D_N: torch.Size([2, 2, 6, 100])\n",
      " inputs_2D_N: torch.Size([2, 2, 6, 100])\n",
      "inputs_2D_N: torch.Size([2, 100, 6, 2])\n",
      "Shape after transpose: torch.Size([2, 2, 6, 100])\n",
      "Shape after conv1 relu: torch.Size([2, 64, 6, 100])\n",
      "Shape after BN_2: torch.Size([2, 64, 6, 100])\n",
      "Shape after pool: torch.Size([2, 64, 3, 50])\n",
      "Shape after conv2 relu: torch.Size([2, 64, 3, 50])\n",
      "Shape after BN_4: torch.Size([2, 64, 3, 50])\n",
      "Shape after pool: torch.Size([2, 64, 1, 25])\n",
      "Shape after transpose: torch.Size([2, 25, 1, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([2, 25, 1, 1600])\n",
      "Shape after dropout: torch.Size([2, 25, 1, 1600])\n",
      "necessary shape for fc2 is 64, 32\n",
      "Shape after dropout2: torch.Size([2, 25, 1, 64])\n",
      "Shape after flatten: torch.Size([2, 1600])\n",
      "Shape of model_3_output: torch.Size([2, 12800])\n",
      "Shape of model_3_output after cat: torch.Size([2, 14400])\n",
      "Shape of output_C1: torch.Size([2, 100, 3])\n",
      "Epoch: 1, time: 40.0478, Training Loss: 22.2606,  Validation loss: 17.3559\n",
      "Training time: 40.58330297470093 seconds\n"
     ]
    }
   ],
   "source": [
    "k_1=36\n",
    "k_2=48\n",
    "k_3=6\n",
    "k_4=8\n",
    "\n",
    "lr = 0.001\n",
    "model = student(k_2-k_1,k_4-k_3,100)\n",
    "\n",
    "gait_Net_student = train_kinematics(device, train_loader, val_loader, lr,num_epoch,model,\"../../datacollection/vicon/subject_1/test_student/test_student.pth\",k_1, k_2, k_3, k_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################\n",
      "first passage through encoder gru\n",
      "input to encoder_gru, torch.Size([64, 100, 12])\n",
      "shape of out_1 after lstm1, torch.Size([64, 100, 256])\n",
      "shape of out_1 dropout1, torch.Size([64, 100, 256])\n",
      "shape of out_2 after lstm1, torch.Size([64, 100, 128])\n",
      "shape of out_2 dropout2, torch.Size([64, 100, 128])\n",
      "shape of out_2 after flatten, torch.Size([64, 12800])\n",
      "necessary shape for last linear layer = 128000, 600\n",
      "shape encoder_gru output, torch.Size([64, 300])\n",
      "shape encoder_gru output after view, torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 1D\n",
      "shape of input, torch.Size([64, 100, 12])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "Shape after transpose: torch.Size([64, 12, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 100])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 100])\n",
      "Shape after pool: torch.Size([64, 64, 50])\n",
      "Shape after conv3 relu: torch.Size([64, 64, 50])\n",
      "Shape after conv4 relu: torch.Size([64, 64, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 50])\n",
      "Shape after pool: torch.Size([64, 64, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 128])\n",
      "Shape after dropout1: torch.Size([64, 25, 128])\n",
      "necessary shape for fc2 is 25*64, 25*64\n",
      "Shape after fc2: torch.Size([64, 25, 1600])\n",
      "Shape after dropout2: torch.Size([64, 25, 1600])\n",
      "Shape after flatten: torch.Size([64, 40000])\n",
      "Shape of model_2_output: torch.Size([64, 12800])\n",
      "Shape of model_2_output_1 (concactenating last dimension of model_2_output and x): torch.Size([64, 52800])\n",
      "necessary shape for linear 528 * w, 6*w\n",
      "Shape of output_C2: torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 2D\n",
      "shape of input x_1, torch.Size([64, 100, 12])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "shape of input x, torch.Size([64, 100, 6, 2])\n",
      "inputs_2D_N: torch.Size([64, 2, 6, 100])\n",
      " inputs_2D_N: torch.Size([64, 2, 6, 100])\n",
      "inputs_2D_N: torch.Size([64, 100, 6, 2])\n",
      "Shape after transpose: torch.Size([64, 2, 6, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 6, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 6, 100])\n",
      "Shape after pool: torch.Size([64, 64, 3, 50])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 3, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 3, 50])\n",
      "Shape after pool: torch.Size([64, 64, 1, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 1, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 1, 1600])\n",
      "Shape after dropout: torch.Size([64, 25, 1, 1600])\n",
      "necessary shape for fc2 is 64, 32\n",
      "Shape after dropout2: torch.Size([64, 25, 1, 64])\n",
      "Shape after flatten: torch.Size([64, 1600])\n",
      "Shape of model_3_output: torch.Size([64, 12800])\n",
      "Shape of model_3_output after cat: torch.Size([64, 14400])\n",
      "Shape of output_C1: torch.Size([64, 100, 3])\n",
      "##################################\n",
      "first passage through encoder gru\n",
      "input to encoder_gru, torch.Size([64, 100, 12])\n",
      "shape of out_1 after lstm1, torch.Size([64, 100, 256])\n",
      "shape of out_1 dropout1, torch.Size([64, 100, 256])\n",
      "shape of out_2 after lstm1, torch.Size([64, 100, 128])\n",
      "shape of out_2 dropout2, torch.Size([64, 100, 128])\n",
      "shape of out_2 after flatten, torch.Size([64, 12800])\n",
      "necessary shape for last linear layer = 128000, 600\n",
      "shape encoder_gru output, torch.Size([64, 300])\n",
      "shape encoder_gru output after view, torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 1D\n",
      "shape of input, torch.Size([64, 100, 12])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "Shape after transpose: torch.Size([64, 12, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 100])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 100])\n",
      "Shape after pool: torch.Size([64, 64, 50])\n",
      "Shape after conv3 relu: torch.Size([64, 64, 50])\n",
      "Shape after conv4 relu: torch.Size([64, 64, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 50])\n",
      "Shape after pool: torch.Size([64, 64, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 128])\n",
      "Shape after dropout1: torch.Size([64, 25, 128])\n",
      "necessary shape for fc2 is 25*64, 25*64\n",
      "Shape after fc2: torch.Size([64, 25, 1600])\n",
      "Shape after dropout2: torch.Size([64, 25, 1600])\n",
      "Shape after flatten: torch.Size([64, 40000])\n",
      "Shape of model_2_output: torch.Size([64, 12800])\n",
      "Shape of model_2_output_1 (concactenating last dimension of model_2_output and x): torch.Size([64, 52800])\n",
      "necessary shape for linear 528 * w, 6*w\n",
      "Shape of output_C2: torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 2D\n",
      "shape of input x_1, torch.Size([64, 100, 12])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "shape of input x, torch.Size([64, 100, 6, 2])\n",
      "inputs_2D_N: torch.Size([64, 2, 6, 100])\n",
      " inputs_2D_N: torch.Size([64, 2, 6, 100])\n",
      "inputs_2D_N: torch.Size([64, 100, 6, 2])\n",
      "Shape after transpose: torch.Size([64, 2, 6, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 6, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 6, 100])\n",
      "Shape after pool: torch.Size([64, 64, 3, 50])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 3, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 3, 50])\n",
      "Shape after pool: torch.Size([64, 64, 1, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 1, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 1, 1600])\n",
      "Shape after dropout: torch.Size([64, 25, 1, 1600])\n",
      "necessary shape for fc2 is 64, 32\n",
      "Shape after dropout2: torch.Size([64, 25, 1, 64])\n",
      "Shape after flatten: torch.Size([64, 1600])\n",
      "Shape of model_3_output: torch.Size([64, 12800])\n",
      "Shape of model_3_output after cat: torch.Size([64, 14400])\n",
      "Shape of output_C1: torch.Size([64, 100, 3])\n",
      "##################################\n",
      "first passage through encoder gru\n",
      "input to encoder_gru, torch.Size([64, 100, 12])\n",
      "shape of out_1 after lstm1, torch.Size([64, 100, 256])\n",
      "shape of out_1 dropout1, torch.Size([64, 100, 256])\n",
      "shape of out_2 after lstm1, torch.Size([64, 100, 128])\n",
      "shape of out_2 dropout2, torch.Size([64, 100, 128])\n",
      "shape of out_2 after flatten, torch.Size([64, 12800])\n",
      "necessary shape for last linear layer = 128000, 600\n",
      "shape encoder_gru output, torch.Size([64, 300])\n",
      "shape encoder_gru output after view, torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 1D\n",
      "shape of input, torch.Size([64, 100, 12])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "Shape after transpose: torch.Size([64, 12, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 100])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 100])\n",
      "Shape after pool: torch.Size([64, 64, 50])\n",
      "Shape after conv3 relu: torch.Size([64, 64, 50])\n",
      "Shape after conv4 relu: torch.Size([64, 64, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 50])\n",
      "Shape after pool: torch.Size([64, 64, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 128])\n",
      "Shape after dropout1: torch.Size([64, 25, 128])\n",
      "necessary shape for fc2 is 25*64, 25*64\n",
      "Shape after fc2: torch.Size([64, 25, 1600])\n",
      "Shape after dropout2: torch.Size([64, 25, 1600])\n",
      "Shape after flatten: torch.Size([64, 40000])\n",
      "Shape of model_2_output: torch.Size([64, 12800])\n",
      "Shape of model_2_output_1 (concactenating last dimension of model_2_output and x): torch.Size([64, 52800])\n",
      "necessary shape for linear 528 * w, 6*w\n",
      "Shape of output_C2: torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 2D\n",
      "shape of input x_1, torch.Size([64, 100, 12])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "shape of input x, torch.Size([64, 100, 6, 2])\n",
      "inputs_2D_N: torch.Size([64, 2, 6, 100])\n",
      " inputs_2D_N: torch.Size([64, 2, 6, 100])\n",
      "inputs_2D_N: torch.Size([64, 100, 6, 2])\n",
      "Shape after transpose: torch.Size([64, 2, 6, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 6, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 6, 100])\n",
      "Shape after pool: torch.Size([64, 64, 3, 50])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 3, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 3, 50])\n",
      "Shape after pool: torch.Size([64, 64, 1, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 1, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 1, 1600])\n",
      "Shape after dropout: torch.Size([64, 25, 1, 1600])\n",
      "necessary shape for fc2 is 64, 32\n",
      "Shape after dropout2: torch.Size([64, 25, 1, 64])\n",
      "Shape after flatten: torch.Size([64, 1600])\n",
      "Shape of model_3_output: torch.Size([64, 12800])\n",
      "Shape of model_3_output after cat: torch.Size([64, 14400])\n",
      "Shape of output_C1: torch.Size([64, 100, 3])\n",
      "##################################\n",
      "first passage through encoder gru\n",
      "input to encoder_gru, torch.Size([64, 100, 12])\n",
      "shape of out_1 after lstm1, torch.Size([64, 100, 256])\n",
      "shape of out_1 dropout1, torch.Size([64, 100, 256])\n",
      "shape of out_2 after lstm1, torch.Size([64, 100, 128])\n",
      "shape of out_2 dropout2, torch.Size([64, 100, 128])\n",
      "shape of out_2 after flatten, torch.Size([64, 12800])\n",
      "necessary shape for last linear layer = 128000, 600\n",
      "shape encoder_gru output, torch.Size([64, 300])\n",
      "shape encoder_gru output after view, torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 1D\n",
      "shape of input, torch.Size([64, 100, 12])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "Shape after transpose: torch.Size([64, 12, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 100])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 100])\n",
      "Shape after pool: torch.Size([64, 64, 50])\n",
      "Shape after conv3 relu: torch.Size([64, 64, 50])\n",
      "Shape after conv4 relu: torch.Size([64, 64, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 50])\n",
      "Shape after pool: torch.Size([64, 64, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 128])\n",
      "Shape after dropout1: torch.Size([64, 25, 128])\n",
      "necessary shape for fc2 is 25*64, 25*64\n",
      "Shape after fc2: torch.Size([64, 25, 1600])\n",
      "Shape after dropout2: torch.Size([64, 25, 1600])\n",
      "Shape after flatten: torch.Size([64, 40000])\n",
      "Shape of model_2_output: torch.Size([64, 12800])\n",
      "Shape of model_2_output_1 (concactenating last dimension of model_2_output and x): torch.Size([64, 52800])\n",
      "necessary shape for linear 528 * w, 6*w\n",
      "Shape of output_C2: torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 2D\n",
      "shape of input x_1, torch.Size([64, 100, 12])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "shape of input x, torch.Size([64, 100, 6, 2])\n",
      "inputs_2D_N: torch.Size([64, 2, 6, 100])\n",
      " inputs_2D_N: torch.Size([64, 2, 6, 100])\n",
      "inputs_2D_N: torch.Size([64, 100, 6, 2])\n",
      "Shape after transpose: torch.Size([64, 2, 6, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 6, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 6, 100])\n",
      "Shape after pool: torch.Size([64, 64, 3, 50])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 3, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 3, 50])\n",
      "Shape after pool: torch.Size([64, 64, 1, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 1, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 1, 1600])\n",
      "Shape after dropout: torch.Size([64, 25, 1, 1600])\n",
      "necessary shape for fc2 is 64, 32\n",
      "Shape after dropout2: torch.Size([64, 25, 1, 64])\n",
      "Shape after flatten: torch.Size([64, 1600])\n",
      "Shape of model_3_output: torch.Size([64, 12800])\n",
      "Shape of model_3_output after cat: torch.Size([64, 14400])\n",
      "Shape of output_C1: torch.Size([64, 100, 3])\n",
      "##################################\n",
      "first passage through encoder gru\n",
      "input to encoder_gru, torch.Size([64, 100, 12])\n",
      "shape of out_1 after lstm1, torch.Size([64, 100, 256])\n",
      "shape of out_1 dropout1, torch.Size([64, 100, 256])\n",
      "shape of out_2 after lstm1, torch.Size([64, 100, 128])\n",
      "shape of out_2 dropout2, torch.Size([64, 100, 128])\n",
      "shape of out_2 after flatten, torch.Size([64, 12800])\n",
      "necessary shape for last linear layer = 128000, 600\n",
      "shape encoder_gru output, torch.Size([64, 300])\n",
      "shape encoder_gru output after view, torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 1D\n",
      "shape of input, torch.Size([64, 100, 12])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "Shape after transpose: torch.Size([64, 12, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 100])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 100])\n",
      "Shape after pool: torch.Size([64, 64, 50])\n",
      "Shape after conv3 relu: torch.Size([64, 64, 50])\n",
      "Shape after conv4 relu: torch.Size([64, 64, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 50])\n",
      "Shape after pool: torch.Size([64, 64, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 128])\n",
      "Shape after dropout1: torch.Size([64, 25, 128])\n",
      "necessary shape for fc2 is 25*64, 25*64\n",
      "Shape after fc2: torch.Size([64, 25, 1600])\n",
      "Shape after dropout2: torch.Size([64, 25, 1600])\n",
      "Shape after flatten: torch.Size([64, 40000])\n",
      "Shape of model_2_output: torch.Size([64, 12800])\n",
      "Shape of model_2_output_1 (concactenating last dimension of model_2_output and x): torch.Size([64, 52800])\n",
      "necessary shape for linear 528 * w, 6*w\n",
      "Shape of output_C2: torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 2D\n",
      "shape of input x_1, torch.Size([64, 100, 12])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "shape of input x, torch.Size([64, 100, 6, 2])\n",
      "inputs_2D_N: torch.Size([64, 2, 6, 100])\n",
      " inputs_2D_N: torch.Size([64, 2, 6, 100])\n",
      "inputs_2D_N: torch.Size([64, 100, 6, 2])\n",
      "Shape after transpose: torch.Size([64, 2, 6, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 6, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 6, 100])\n",
      "Shape after pool: torch.Size([64, 64, 3, 50])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 3, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 3, 50])\n",
      "Shape after pool: torch.Size([64, 64, 1, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 1, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 1, 1600])\n",
      "Shape after dropout: torch.Size([64, 25, 1, 1600])\n",
      "necessary shape for fc2 is 64, 32\n",
      "Shape after dropout2: torch.Size([64, 25, 1, 64])\n",
      "Shape after flatten: torch.Size([64, 1600])\n",
      "Shape of model_3_output: torch.Size([64, 12800])\n",
      "Shape of model_3_output after cat: torch.Size([64, 14400])\n",
      "Shape of output_C1: torch.Size([64, 100, 3])\n",
      "##################################\n",
      "first passage through encoder gru\n",
      "input to encoder_gru, torch.Size([64, 100, 12])\n",
      "shape of out_1 after lstm1, torch.Size([64, 100, 256])\n",
      "shape of out_1 dropout1, torch.Size([64, 100, 256])\n",
      "shape of out_2 after lstm1, torch.Size([64, 100, 128])\n",
      "shape of out_2 dropout2, torch.Size([64, 100, 128])\n",
      "shape of out_2 after flatten, torch.Size([64, 12800])\n",
      "necessary shape for last linear layer = 128000, 600\n",
      "shape encoder_gru output, torch.Size([64, 300])\n",
      "shape encoder_gru output after view, torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 1D\n",
      "shape of input, torch.Size([64, 100, 12])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "Shape after transpose: torch.Size([64, 12, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 100])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 100])\n",
      "Shape after pool: torch.Size([64, 64, 50])\n",
      "Shape after conv3 relu: torch.Size([64, 64, 50])\n",
      "Shape after conv4 relu: torch.Size([64, 64, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 50])\n",
      "Shape after pool: torch.Size([64, 64, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 128])\n",
      "Shape after dropout1: torch.Size([64, 25, 128])\n",
      "necessary shape for fc2 is 25*64, 25*64\n",
      "Shape after fc2: torch.Size([64, 25, 1600])\n",
      "Shape after dropout2: torch.Size([64, 25, 1600])\n",
      "Shape after flatten: torch.Size([64, 40000])\n",
      "Shape of model_2_output: torch.Size([64, 12800])\n",
      "Shape of model_2_output_1 (concactenating last dimension of model_2_output and x): torch.Size([64, 52800])\n",
      "necessary shape for linear 528 * w, 6*w\n",
      "Shape of output_C2: torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 2D\n",
      "shape of input x_1, torch.Size([64, 100, 12])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "shape of input x, torch.Size([64, 100, 6, 2])\n",
      "inputs_2D_N: torch.Size([64, 2, 6, 100])\n",
      " inputs_2D_N: torch.Size([64, 2, 6, 100])\n",
      "inputs_2D_N: torch.Size([64, 100, 6, 2])\n",
      "Shape after transpose: torch.Size([64, 2, 6, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 6, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 6, 100])\n",
      "Shape after pool: torch.Size([64, 64, 3, 50])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 3, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 3, 50])\n",
      "Shape after pool: torch.Size([64, 64, 1, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 1, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 1, 1600])\n",
      "Shape after dropout: torch.Size([64, 25, 1, 1600])\n",
      "necessary shape for fc2 is 64, 32\n",
      "Shape after dropout2: torch.Size([64, 25, 1, 64])\n",
      "Shape after flatten: torch.Size([64, 1600])\n",
      "Shape of model_3_output: torch.Size([64, 12800])\n",
      "Shape of model_3_output after cat: torch.Size([64, 14400])\n",
      "Shape of output_C1: torch.Size([64, 100, 3])\n",
      "##################################\n",
      "first passage through encoder gru\n",
      "input to encoder_gru, torch.Size([64, 100, 12])\n",
      "shape of out_1 after lstm1, torch.Size([64, 100, 256])\n",
      "shape of out_1 dropout1, torch.Size([64, 100, 256])\n",
      "shape of out_2 after lstm1, torch.Size([64, 100, 128])\n",
      "shape of out_2 dropout2, torch.Size([64, 100, 128])\n",
      "shape of out_2 after flatten, torch.Size([64, 12800])\n",
      "necessary shape for last linear layer = 128000, 600\n",
      "shape encoder_gru output, torch.Size([64, 300])\n",
      "shape encoder_gru output after view, torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 1D\n",
      "shape of input, torch.Size([64, 100, 12])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "Shape after transpose: torch.Size([64, 12, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 100])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 100])\n",
      "Shape after pool: torch.Size([64, 64, 50])\n",
      "Shape after conv3 relu: torch.Size([64, 64, 50])\n",
      "Shape after conv4 relu: torch.Size([64, 64, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 50])\n",
      "Shape after pool: torch.Size([64, 64, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 128])\n",
      "Shape after dropout1: torch.Size([64, 25, 128])\n",
      "necessary shape for fc2 is 25*64, 25*64\n",
      "Shape after fc2: torch.Size([64, 25, 1600])\n",
      "Shape after dropout2: torch.Size([64, 25, 1600])\n",
      "Shape after flatten: torch.Size([64, 40000])\n",
      "Shape of model_2_output: torch.Size([64, 12800])\n",
      "Shape of model_2_output_1 (concactenating last dimension of model_2_output and x): torch.Size([64, 52800])\n",
      "necessary shape for linear 528 * w, 6*w\n",
      "Shape of output_C2: torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 2D\n",
      "shape of input x_1, torch.Size([64, 100, 12])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "shape of input x, torch.Size([64, 100, 6, 2])\n",
      "inputs_2D_N: torch.Size([64, 2, 6, 100])\n",
      " inputs_2D_N: torch.Size([64, 2, 6, 100])\n",
      "inputs_2D_N: torch.Size([64, 100, 6, 2])\n",
      "Shape after transpose: torch.Size([64, 2, 6, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 6, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 6, 100])\n",
      "Shape after pool: torch.Size([64, 64, 3, 50])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 3, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 3, 50])\n",
      "Shape after pool: torch.Size([64, 64, 1, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 1, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 1, 1600])\n",
      "Shape after dropout: torch.Size([64, 25, 1, 1600])\n",
      "necessary shape for fc2 is 64, 32\n",
      "Shape after dropout2: torch.Size([64, 25, 1, 64])\n",
      "Shape after flatten: torch.Size([64, 1600])\n",
      "Shape of model_3_output: torch.Size([64, 12800])\n",
      "Shape of model_3_output after cat: torch.Size([64, 14400])\n",
      "Shape of output_C1: torch.Size([64, 100, 3])\n",
      "##################################\n",
      "first passage through encoder gru\n",
      "input to encoder_gru, torch.Size([64, 100, 12])\n",
      "shape of out_1 after lstm1, torch.Size([64, 100, 256])\n",
      "shape of out_1 dropout1, torch.Size([64, 100, 256])\n",
      "shape of out_2 after lstm1, torch.Size([64, 100, 128])\n",
      "shape of out_2 dropout2, torch.Size([64, 100, 128])\n",
      "shape of out_2 after flatten, torch.Size([64, 12800])\n",
      "necessary shape for last linear layer = 128000, 600\n",
      "shape encoder_gru output, torch.Size([64, 300])\n",
      "shape encoder_gru output after view, torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 1D\n",
      "shape of input, torch.Size([64, 100, 12])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "Shape after transpose: torch.Size([64, 12, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 100])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 100])\n",
      "Shape after pool: torch.Size([64, 64, 50])\n",
      "Shape after conv3 relu: torch.Size([64, 64, 50])\n",
      "Shape after conv4 relu: torch.Size([64, 64, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 50])\n",
      "Shape after pool: torch.Size([64, 64, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 128])\n",
      "Shape after dropout1: torch.Size([64, 25, 128])\n",
      "necessary shape for fc2 is 25*64, 25*64\n",
      "Shape after fc2: torch.Size([64, 25, 1600])\n",
      "Shape after dropout2: torch.Size([64, 25, 1600])\n",
      "Shape after flatten: torch.Size([64, 40000])\n",
      "Shape of model_2_output: torch.Size([64, 12800])\n",
      "Shape of model_2_output_1 (concactenating last dimension of model_2_output and x): torch.Size([64, 52800])\n",
      "necessary shape for linear 528 * w, 6*w\n",
      "Shape of output_C2: torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 2D\n",
      "shape of input x_1, torch.Size([64, 100, 12])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "shape of input x, torch.Size([64, 100, 6, 2])\n",
      "inputs_2D_N: torch.Size([64, 2, 6, 100])\n",
      " inputs_2D_N: torch.Size([64, 2, 6, 100])\n",
      "inputs_2D_N: torch.Size([64, 100, 6, 2])\n",
      "Shape after transpose: torch.Size([64, 2, 6, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 6, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 6, 100])\n",
      "Shape after pool: torch.Size([64, 64, 3, 50])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 3, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 3, 50])\n",
      "Shape after pool: torch.Size([64, 64, 1, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 1, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 1, 1600])\n",
      "Shape after dropout: torch.Size([64, 25, 1, 1600])\n",
      "necessary shape for fc2 is 64, 32\n",
      "Shape after dropout2: torch.Size([64, 25, 1, 64])\n",
      "Shape after flatten: torch.Size([64, 1600])\n",
      "Shape of model_3_output: torch.Size([64, 12800])\n",
      "Shape of model_3_output after cat: torch.Size([64, 14400])\n",
      "Shape of output_C1: torch.Size([64, 100, 3])\n",
      "##################################\n",
      "first passage through encoder gru\n",
      "input to encoder_gru, torch.Size([64, 100, 12])\n",
      "shape of out_1 after lstm1, torch.Size([64, 100, 256])\n",
      "shape of out_1 dropout1, torch.Size([64, 100, 256])\n",
      "shape of out_2 after lstm1, torch.Size([64, 100, 128])\n",
      "shape of out_2 dropout2, torch.Size([64, 100, 128])\n",
      "shape of out_2 after flatten, torch.Size([64, 12800])\n",
      "necessary shape for last linear layer = 128000, 600\n",
      "shape encoder_gru output, torch.Size([64, 300])\n",
      "shape encoder_gru output after view, torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 1D\n",
      "shape of input, torch.Size([64, 100, 12])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "Shape after transpose: torch.Size([64, 12, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 100])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 100])\n",
      "Shape after pool: torch.Size([64, 64, 50])\n",
      "Shape after conv3 relu: torch.Size([64, 64, 50])\n",
      "Shape after conv4 relu: torch.Size([64, 64, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 50])\n",
      "Shape after pool: torch.Size([64, 64, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 128])\n",
      "Shape after dropout1: torch.Size([64, 25, 128])\n",
      "necessary shape for fc2 is 25*64, 25*64\n",
      "Shape after fc2: torch.Size([64, 25, 1600])\n",
      "Shape after dropout2: torch.Size([64, 25, 1600])\n",
      "Shape after flatten: torch.Size([64, 40000])\n",
      "Shape of model_2_output: torch.Size([64, 12800])\n",
      "Shape of model_2_output_1 (concactenating last dimension of model_2_output and x): torch.Size([64, 52800])\n",
      "necessary shape for linear 528 * w, 6*w\n",
      "Shape of output_C2: torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 2D\n",
      "shape of input x_1, torch.Size([64, 100, 12])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "shape of input x, torch.Size([64, 100, 6, 2])\n",
      "inputs_2D_N: torch.Size([64, 2, 6, 100])\n",
      " inputs_2D_N: torch.Size([64, 2, 6, 100])\n",
      "inputs_2D_N: torch.Size([64, 100, 6, 2])\n",
      "Shape after transpose: torch.Size([64, 2, 6, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 6, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 6, 100])\n",
      "Shape after pool: torch.Size([64, 64, 3, 50])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 3, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 3, 50])\n",
      "Shape after pool: torch.Size([64, 64, 1, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 1, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 1, 1600])\n",
      "Shape after dropout: torch.Size([64, 25, 1, 1600])\n",
      "necessary shape for fc2 is 64, 32\n",
      "Shape after dropout2: torch.Size([64, 25, 1, 64])\n",
      "Shape after flatten: torch.Size([64, 1600])\n",
      "Shape of model_3_output: torch.Size([64, 12800])\n",
      "Shape of model_3_output after cat: torch.Size([64, 14400])\n",
      "Shape of output_C1: torch.Size([64, 100, 3])\n",
      "##################################\n",
      "first passage through encoder gru\n",
      "input to encoder_gru, torch.Size([64, 100, 12])\n",
      "shape of out_1 after lstm1, torch.Size([64, 100, 256])\n",
      "shape of out_1 dropout1, torch.Size([64, 100, 256])\n",
      "shape of out_2 after lstm1, torch.Size([64, 100, 128])\n",
      "shape of out_2 dropout2, torch.Size([64, 100, 128])\n",
      "shape of out_2 after flatten, torch.Size([64, 12800])\n",
      "necessary shape for last linear layer = 128000, 600\n",
      "shape encoder_gru output, torch.Size([64, 300])\n",
      "shape encoder_gru output after view, torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 1D\n",
      "shape of input, torch.Size([64, 100, 12])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "Shape after transpose: torch.Size([64, 12, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 100])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 100])\n",
      "Shape after pool: torch.Size([64, 64, 50])\n",
      "Shape after conv3 relu: torch.Size([64, 64, 50])\n",
      "Shape after conv4 relu: torch.Size([64, 64, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 50])\n",
      "Shape after pool: torch.Size([64, 64, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 128])\n",
      "Shape after dropout1: torch.Size([64, 25, 128])\n",
      "necessary shape for fc2 is 25*64, 25*64\n",
      "Shape after fc2: torch.Size([64, 25, 1600])\n",
      "Shape after dropout2: torch.Size([64, 25, 1600])\n",
      "Shape after flatten: torch.Size([64, 40000])\n",
      "Shape of model_2_output: torch.Size([64, 12800])\n",
      "Shape of model_2_output_1 (concactenating last dimension of model_2_output and x): torch.Size([64, 52800])\n",
      "necessary shape for linear 528 * w, 6*w\n",
      "Shape of output_C2: torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 2D\n",
      "shape of input x_1, torch.Size([64, 100, 12])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "shape of input x, torch.Size([64, 100, 6, 2])\n",
      "inputs_2D_N: torch.Size([64, 2, 6, 100])\n",
      " inputs_2D_N: torch.Size([64, 2, 6, 100])\n",
      "inputs_2D_N: torch.Size([64, 100, 6, 2])\n",
      "Shape after transpose: torch.Size([64, 2, 6, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 6, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 6, 100])\n",
      "Shape after pool: torch.Size([64, 64, 3, 50])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 3, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 3, 50])\n",
      "Shape after pool: torch.Size([64, 64, 1, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 1, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 1, 1600])\n",
      "Shape after dropout: torch.Size([64, 25, 1, 1600])\n",
      "necessary shape for fc2 is 64, 32\n",
      "Shape after dropout2: torch.Size([64, 25, 1, 64])\n",
      "Shape after flatten: torch.Size([64, 1600])\n",
      "Shape of model_3_output: torch.Size([64, 12800])\n",
      "Shape of model_3_output after cat: torch.Size([64, 14400])\n",
      "Shape of output_C1: torch.Size([64, 100, 3])\n",
      "##################################\n",
      "first passage through encoder gru\n",
      "input to encoder_gru, torch.Size([64, 100, 12])\n",
      "shape of out_1 after lstm1, torch.Size([64, 100, 256])\n",
      "shape of out_1 dropout1, torch.Size([64, 100, 256])\n",
      "shape of out_2 after lstm1, torch.Size([64, 100, 128])\n",
      "shape of out_2 dropout2, torch.Size([64, 100, 128])\n",
      "shape of out_2 after flatten, torch.Size([64, 12800])\n",
      "necessary shape for last linear layer = 128000, 600\n",
      "shape encoder_gru output, torch.Size([64, 300])\n",
      "shape encoder_gru output after view, torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 1D\n",
      "shape of input, torch.Size([64, 100, 12])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "Shape after transpose: torch.Size([64, 12, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 100])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 100])\n",
      "Shape after pool: torch.Size([64, 64, 50])\n",
      "Shape after conv3 relu: torch.Size([64, 64, 50])\n",
      "Shape after conv4 relu: torch.Size([64, 64, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 50])\n",
      "Shape after pool: torch.Size([64, 64, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 128])\n",
      "Shape after dropout1: torch.Size([64, 25, 128])\n",
      "necessary shape for fc2 is 25*64, 25*64\n",
      "Shape after fc2: torch.Size([64, 25, 1600])\n",
      "Shape after dropout2: torch.Size([64, 25, 1600])\n",
      "Shape after flatten: torch.Size([64, 40000])\n",
      "Shape of model_2_output: torch.Size([64, 12800])\n",
      "Shape of model_2_output_1 (concactenating last dimension of model_2_output and x): torch.Size([64, 52800])\n",
      "necessary shape for linear 528 * w, 6*w\n",
      "Shape of output_C2: torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 2D\n",
      "shape of input x_1, torch.Size([64, 100, 12])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "shape of input x, torch.Size([64, 100, 6, 2])\n",
      "inputs_2D_N: torch.Size([64, 2, 6, 100])\n",
      " inputs_2D_N: torch.Size([64, 2, 6, 100])\n",
      "inputs_2D_N: torch.Size([64, 100, 6, 2])\n",
      "Shape after transpose: torch.Size([64, 2, 6, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 6, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 6, 100])\n",
      "Shape after pool: torch.Size([64, 64, 3, 50])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 3, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 3, 50])\n",
      "Shape after pool: torch.Size([64, 64, 1, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 1, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 1, 1600])\n",
      "Shape after dropout: torch.Size([64, 25, 1, 1600])\n",
      "necessary shape for fc2 is 64, 32\n",
      "Shape after dropout2: torch.Size([64, 25, 1, 64])\n",
      "Shape after flatten: torch.Size([64, 1600])\n",
      "Shape of model_3_output: torch.Size([64, 12800])\n",
      "Shape of model_3_output after cat: torch.Size([64, 14400])\n",
      "Shape of output_C1: torch.Size([64, 100, 3])\n",
      "##################################\n",
      "first passage through encoder gru\n",
      "input to encoder_gru, torch.Size([64, 100, 12])\n",
      "shape of out_1 after lstm1, torch.Size([64, 100, 256])\n",
      "shape of out_1 dropout1, torch.Size([64, 100, 256])\n",
      "shape of out_2 after lstm1, torch.Size([64, 100, 128])\n",
      "shape of out_2 dropout2, torch.Size([64, 100, 128])\n",
      "shape of out_2 after flatten, torch.Size([64, 12800])\n",
      "necessary shape for last linear layer = 128000, 600\n",
      "shape encoder_gru output, torch.Size([64, 300])\n",
      "shape encoder_gru output after view, torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 1D\n",
      "shape of input, torch.Size([64, 100, 12])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "Shape after transpose: torch.Size([64, 12, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 100])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 100])\n",
      "Shape after pool: torch.Size([64, 64, 50])\n",
      "Shape after conv3 relu: torch.Size([64, 64, 50])\n",
      "Shape after conv4 relu: torch.Size([64, 64, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 50])\n",
      "Shape after pool: torch.Size([64, 64, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 128])\n",
      "Shape after dropout1: torch.Size([64, 25, 128])\n",
      "necessary shape for fc2 is 25*64, 25*64\n",
      "Shape after fc2: torch.Size([64, 25, 1600])\n",
      "Shape after dropout2: torch.Size([64, 25, 1600])\n",
      "Shape after flatten: torch.Size([64, 40000])\n",
      "Shape of model_2_output: torch.Size([64, 12800])\n",
      "Shape of model_2_output_1 (concactenating last dimension of model_2_output and x): torch.Size([64, 52800])\n",
      "necessary shape for linear 528 * w, 6*w\n",
      "Shape of output_C2: torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 2D\n",
      "shape of input x_1, torch.Size([64, 100, 12])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "shape of input x, torch.Size([64, 100, 6, 2])\n",
      "inputs_2D_N: torch.Size([64, 2, 6, 100])\n",
      " inputs_2D_N: torch.Size([64, 2, 6, 100])\n",
      "inputs_2D_N: torch.Size([64, 100, 6, 2])\n",
      "Shape after transpose: torch.Size([64, 2, 6, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 6, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 6, 100])\n",
      "Shape after pool: torch.Size([64, 64, 3, 50])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 3, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 3, 50])\n",
      "Shape after pool: torch.Size([64, 64, 1, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 1, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 1, 1600])\n",
      "Shape after dropout: torch.Size([64, 25, 1, 1600])\n",
      "necessary shape for fc2 is 64, 32\n",
      "Shape after dropout2: torch.Size([64, 25, 1, 64])\n",
      "Shape after flatten: torch.Size([64, 1600])\n",
      "Shape of model_3_output: torch.Size([64, 12800])\n",
      "Shape of model_3_output after cat: torch.Size([64, 14400])\n",
      "Shape of output_C1: torch.Size([64, 100, 3])\n",
      "##################################\n",
      "first passage through encoder gru\n",
      "input to encoder_gru, torch.Size([64, 100, 12])\n",
      "shape of out_1 after lstm1, torch.Size([64, 100, 256])\n",
      "shape of out_1 dropout1, torch.Size([64, 100, 256])\n",
      "shape of out_2 after lstm1, torch.Size([64, 100, 128])\n",
      "shape of out_2 dropout2, torch.Size([64, 100, 128])\n",
      "shape of out_2 after flatten, torch.Size([64, 12800])\n",
      "necessary shape for last linear layer = 128000, 600\n",
      "shape encoder_gru output, torch.Size([64, 300])\n",
      "shape encoder_gru output after view, torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 1D\n",
      "shape of input, torch.Size([64, 100, 12])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "Shape after transpose: torch.Size([64, 12, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 100])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 100])\n",
      "Shape after pool: torch.Size([64, 64, 50])\n",
      "Shape after conv3 relu: torch.Size([64, 64, 50])\n",
      "Shape after conv4 relu: torch.Size([64, 64, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 50])\n",
      "Shape after pool: torch.Size([64, 64, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 128])\n",
      "Shape after dropout1: torch.Size([64, 25, 128])\n",
      "necessary shape for fc2 is 25*64, 25*64\n",
      "Shape after fc2: torch.Size([64, 25, 1600])\n",
      "Shape after dropout2: torch.Size([64, 25, 1600])\n",
      "Shape after flatten: torch.Size([64, 40000])\n",
      "Shape of model_2_output: torch.Size([64, 12800])\n",
      "Shape of model_2_output_1 (concactenating last dimension of model_2_output and x): torch.Size([64, 52800])\n",
      "necessary shape for linear 528 * w, 6*w\n",
      "Shape of output_C2: torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 2D\n",
      "shape of input x_1, torch.Size([64, 100, 12])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "shape of input x, torch.Size([64, 100, 6, 2])\n",
      "inputs_2D_N: torch.Size([64, 2, 6, 100])\n",
      " inputs_2D_N: torch.Size([64, 2, 6, 100])\n",
      "inputs_2D_N: torch.Size([64, 100, 6, 2])\n",
      "Shape after transpose: torch.Size([64, 2, 6, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 6, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 6, 100])\n",
      "Shape after pool: torch.Size([64, 64, 3, 50])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 3, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 3, 50])\n",
      "Shape after pool: torch.Size([64, 64, 1, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 1, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 1, 1600])\n",
      "Shape after dropout: torch.Size([64, 25, 1, 1600])\n",
      "necessary shape for fc2 is 64, 32\n",
      "Shape after dropout2: torch.Size([64, 25, 1, 64])\n",
      "Shape after flatten: torch.Size([64, 1600])\n",
      "Shape of model_3_output: torch.Size([64, 12800])\n",
      "Shape of model_3_output after cat: torch.Size([64, 14400])\n",
      "Shape of output_C1: torch.Size([64, 100, 3])\n",
      "##################################\n",
      "first passage through encoder gru\n",
      "input to encoder_gru, torch.Size([64, 100, 12])\n",
      "shape of out_1 after lstm1, torch.Size([64, 100, 256])\n",
      "shape of out_1 dropout1, torch.Size([64, 100, 256])\n",
      "shape of out_2 after lstm1, torch.Size([64, 100, 128])\n",
      "shape of out_2 dropout2, torch.Size([64, 100, 128])\n",
      "shape of out_2 after flatten, torch.Size([64, 12800])\n",
      "necessary shape for last linear layer = 128000, 600\n",
      "shape encoder_gru output, torch.Size([64, 300])\n",
      "shape encoder_gru output after view, torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 1D\n",
      "shape of input, torch.Size([64, 100, 12])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "Shape after transpose: torch.Size([64, 12, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 100])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 100])\n",
      "Shape after pool: torch.Size([64, 64, 50])\n",
      "Shape after conv3 relu: torch.Size([64, 64, 50])\n",
      "Shape after conv4 relu: torch.Size([64, 64, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 50])\n",
      "Shape after pool: torch.Size([64, 64, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 128])\n",
      "Shape after dropout1: torch.Size([64, 25, 128])\n",
      "necessary shape for fc2 is 25*64, 25*64\n",
      "Shape after fc2: torch.Size([64, 25, 1600])\n",
      "Shape after dropout2: torch.Size([64, 25, 1600])\n",
      "Shape after flatten: torch.Size([64, 40000])\n",
      "Shape of model_2_output: torch.Size([64, 12800])\n",
      "Shape of model_2_output_1 (concactenating last dimension of model_2_output and x): torch.Size([64, 52800])\n",
      "necessary shape for linear 528 * w, 6*w\n",
      "Shape of output_C2: torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 2D\n",
      "shape of input x_1, torch.Size([64, 100, 12])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "shape of input x, torch.Size([64, 100, 6, 2])\n",
      "inputs_2D_N: torch.Size([64, 2, 6, 100])\n",
      " inputs_2D_N: torch.Size([64, 2, 6, 100])\n",
      "inputs_2D_N: torch.Size([64, 100, 6, 2])\n",
      "Shape after transpose: torch.Size([64, 2, 6, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 6, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 6, 100])\n",
      "Shape after pool: torch.Size([64, 64, 3, 50])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 3, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 3, 50])\n",
      "Shape after pool: torch.Size([64, 64, 1, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 1, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 1, 1600])\n",
      "Shape after dropout: torch.Size([64, 25, 1, 1600])\n",
      "necessary shape for fc2 is 64, 32\n",
      "Shape after dropout2: torch.Size([64, 25, 1, 64])\n",
      "Shape after flatten: torch.Size([64, 1600])\n",
      "Shape of model_3_output: torch.Size([64, 12800])\n",
      "Shape of model_3_output after cat: torch.Size([64, 14400])\n",
      "Shape of output_C1: torch.Size([64, 100, 3])\n",
      "##################################\n",
      "first passage through encoder gru\n",
      "input to encoder_gru, torch.Size([64, 100, 12])\n",
      "shape of out_1 after lstm1, torch.Size([64, 100, 256])\n",
      "shape of out_1 dropout1, torch.Size([64, 100, 256])\n",
      "shape of out_2 after lstm1, torch.Size([64, 100, 128])\n",
      "shape of out_2 dropout2, torch.Size([64, 100, 128])\n",
      "shape of out_2 after flatten, torch.Size([64, 12800])\n",
      "necessary shape for last linear layer = 128000, 600\n",
      "shape encoder_gru output, torch.Size([64, 300])\n",
      "shape encoder_gru output after view, torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 1D\n",
      "shape of input, torch.Size([64, 100, 12])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "Shape after transpose: torch.Size([64, 12, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 100])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 100])\n",
      "Shape after pool: torch.Size([64, 64, 50])\n",
      "Shape after conv3 relu: torch.Size([64, 64, 50])\n",
      "Shape after conv4 relu: torch.Size([64, 64, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 50])\n",
      "Shape after pool: torch.Size([64, 64, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 128])\n",
      "Shape after dropout1: torch.Size([64, 25, 128])\n",
      "necessary shape for fc2 is 25*64, 25*64\n",
      "Shape after fc2: torch.Size([64, 25, 1600])\n",
      "Shape after dropout2: torch.Size([64, 25, 1600])\n",
      "Shape after flatten: torch.Size([64, 40000])\n",
      "Shape of model_2_output: torch.Size([64, 12800])\n",
      "Shape of model_2_output_1 (concactenating last dimension of model_2_output and x): torch.Size([64, 52800])\n",
      "necessary shape for linear 528 * w, 6*w\n",
      "Shape of output_C2: torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 2D\n",
      "shape of input x_1, torch.Size([64, 100, 12])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "shape of input x, torch.Size([64, 100, 6, 2])\n",
      "inputs_2D_N: torch.Size([64, 2, 6, 100])\n",
      " inputs_2D_N: torch.Size([64, 2, 6, 100])\n",
      "inputs_2D_N: torch.Size([64, 100, 6, 2])\n",
      "Shape after transpose: torch.Size([64, 2, 6, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 6, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 6, 100])\n",
      "Shape after pool: torch.Size([64, 64, 3, 50])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 3, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 3, 50])\n",
      "Shape after pool: torch.Size([64, 64, 1, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 1, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 1, 1600])\n",
      "Shape after dropout: torch.Size([64, 25, 1, 1600])\n",
      "necessary shape for fc2 is 64, 32\n",
      "Shape after dropout2: torch.Size([64, 25, 1, 64])\n",
      "Shape after flatten: torch.Size([64, 1600])\n",
      "Shape of model_3_output: torch.Size([64, 12800])\n",
      "Shape of model_3_output after cat: torch.Size([64, 14400])\n",
      "Shape of output_C1: torch.Size([64, 100, 3])\n",
      "##################################\n",
      "first passage through encoder gru\n",
      "input to encoder_gru, torch.Size([64, 100, 12])\n",
      "shape of out_1 after lstm1, torch.Size([64, 100, 256])\n",
      "shape of out_1 dropout1, torch.Size([64, 100, 256])\n",
      "shape of out_2 after lstm1, torch.Size([64, 100, 128])\n",
      "shape of out_2 dropout2, torch.Size([64, 100, 128])\n",
      "shape of out_2 after flatten, torch.Size([64, 12800])\n",
      "necessary shape for last linear layer = 128000, 600\n",
      "shape encoder_gru output, torch.Size([64, 300])\n",
      "shape encoder_gru output after view, torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 1D\n",
      "shape of input, torch.Size([64, 100, 12])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "Shape after transpose: torch.Size([64, 12, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 100])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 100])\n",
      "Shape after pool: torch.Size([64, 64, 50])\n",
      "Shape after conv3 relu: torch.Size([64, 64, 50])\n",
      "Shape after conv4 relu: torch.Size([64, 64, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 50])\n",
      "Shape after pool: torch.Size([64, 64, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 128])\n",
      "Shape after dropout1: torch.Size([64, 25, 128])\n",
      "necessary shape for fc2 is 25*64, 25*64\n",
      "Shape after fc2: torch.Size([64, 25, 1600])\n",
      "Shape after dropout2: torch.Size([64, 25, 1600])\n",
      "Shape after flatten: torch.Size([64, 40000])\n",
      "Shape of model_2_output: torch.Size([64, 12800])\n",
      "Shape of model_2_output_1 (concactenating last dimension of model_2_output and x): torch.Size([64, 52800])\n",
      "necessary shape for linear 528 * w, 6*w\n",
      "Shape of output_C2: torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 2D\n",
      "shape of input x_1, torch.Size([64, 100, 12])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "shape of input x, torch.Size([64, 100, 6, 2])\n",
      "inputs_2D_N: torch.Size([64, 2, 6, 100])\n",
      " inputs_2D_N: torch.Size([64, 2, 6, 100])\n",
      "inputs_2D_N: torch.Size([64, 100, 6, 2])\n",
      "Shape after transpose: torch.Size([64, 2, 6, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 6, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 6, 100])\n",
      "Shape after pool: torch.Size([64, 64, 3, 50])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 3, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 3, 50])\n",
      "Shape after pool: torch.Size([64, 64, 1, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 1, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 1, 1600])\n",
      "Shape after dropout: torch.Size([64, 25, 1, 1600])\n",
      "necessary shape for fc2 is 64, 32\n",
      "Shape after dropout2: torch.Size([64, 25, 1, 64])\n",
      "Shape after flatten: torch.Size([64, 1600])\n",
      "Shape of model_3_output: torch.Size([64, 12800])\n",
      "Shape of model_3_output after cat: torch.Size([64, 14400])\n",
      "Shape of output_C1: torch.Size([64, 100, 3])\n",
      "##################################\n",
      "first passage through encoder gru\n",
      "input to encoder_gru, torch.Size([64, 100, 12])\n",
      "shape of out_1 after lstm1, torch.Size([64, 100, 256])\n",
      "shape of out_1 dropout1, torch.Size([64, 100, 256])\n",
      "shape of out_2 after lstm1, torch.Size([64, 100, 128])\n",
      "shape of out_2 dropout2, torch.Size([64, 100, 128])\n",
      "shape of out_2 after flatten, torch.Size([64, 12800])\n",
      "necessary shape for last linear layer = 128000, 600\n",
      "shape encoder_gru output, torch.Size([64, 300])\n",
      "shape encoder_gru output after view, torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 1D\n",
      "shape of input, torch.Size([64, 100, 12])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "Shape after transpose: torch.Size([64, 12, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 100])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 100])\n",
      "Shape after pool: torch.Size([64, 64, 50])\n",
      "Shape after conv3 relu: torch.Size([64, 64, 50])\n",
      "Shape after conv4 relu: torch.Size([64, 64, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 50])\n",
      "Shape after pool: torch.Size([64, 64, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 128])\n",
      "Shape after dropout1: torch.Size([64, 25, 128])\n",
      "necessary shape for fc2 is 25*64, 25*64\n",
      "Shape after fc2: torch.Size([64, 25, 1600])\n",
      "Shape after dropout2: torch.Size([64, 25, 1600])\n",
      "Shape after flatten: torch.Size([64, 40000])\n",
      "Shape of model_2_output: torch.Size([64, 12800])\n",
      "Shape of model_2_output_1 (concactenating last dimension of model_2_output and x): torch.Size([64, 52800])\n",
      "necessary shape for linear 528 * w, 6*w\n",
      "Shape of output_C2: torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 2D\n",
      "shape of input x_1, torch.Size([64, 100, 12])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "shape of input x, torch.Size([64, 100, 6, 2])\n",
      "inputs_2D_N: torch.Size([64, 2, 6, 100])\n",
      " inputs_2D_N: torch.Size([64, 2, 6, 100])\n",
      "inputs_2D_N: torch.Size([64, 100, 6, 2])\n",
      "Shape after transpose: torch.Size([64, 2, 6, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 6, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 6, 100])\n",
      "Shape after pool: torch.Size([64, 64, 3, 50])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 3, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 3, 50])\n",
      "Shape after pool: torch.Size([64, 64, 1, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 1, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 1, 1600])\n",
      "Shape after dropout: torch.Size([64, 25, 1, 1600])\n",
      "necessary shape for fc2 is 64, 32\n",
      "Shape after dropout2: torch.Size([64, 25, 1, 64])\n",
      "Shape after flatten: torch.Size([64, 1600])\n",
      "Shape of model_3_output: torch.Size([64, 12800])\n",
      "Shape of model_3_output after cat: torch.Size([64, 14400])\n",
      "Shape of output_C1: torch.Size([64, 100, 3])\n",
      "##################################\n",
      "first passage through encoder gru\n",
      "input to encoder_gru, torch.Size([64, 100, 12])\n",
      "shape of out_1 after lstm1, torch.Size([64, 100, 256])\n",
      "shape of out_1 dropout1, torch.Size([64, 100, 256])\n",
      "shape of out_2 after lstm1, torch.Size([64, 100, 128])\n",
      "shape of out_2 dropout2, torch.Size([64, 100, 128])\n",
      "shape of out_2 after flatten, torch.Size([64, 12800])\n",
      "necessary shape for last linear layer = 128000, 600\n",
      "shape encoder_gru output, torch.Size([64, 300])\n",
      "shape encoder_gru output after view, torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 1D\n",
      "shape of input, torch.Size([64, 100, 12])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "Shape after transpose: torch.Size([64, 12, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 100])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 100])\n",
      "Shape after pool: torch.Size([64, 64, 50])\n",
      "Shape after conv3 relu: torch.Size([64, 64, 50])\n",
      "Shape after conv4 relu: torch.Size([64, 64, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 50])\n",
      "Shape after pool: torch.Size([64, 64, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 128])\n",
      "Shape after dropout1: torch.Size([64, 25, 128])\n",
      "necessary shape for fc2 is 25*64, 25*64\n",
      "Shape after fc2: torch.Size([64, 25, 1600])\n",
      "Shape after dropout2: torch.Size([64, 25, 1600])\n",
      "Shape after flatten: torch.Size([64, 40000])\n",
      "Shape of model_2_output: torch.Size([64, 12800])\n",
      "Shape of model_2_output_1 (concactenating last dimension of model_2_output and x): torch.Size([64, 52800])\n",
      "necessary shape for linear 528 * w, 6*w\n",
      "Shape of output_C2: torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 2D\n",
      "shape of input x_1, torch.Size([64, 100, 12])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "shape of input x, torch.Size([64, 100, 6, 2])\n",
      "inputs_2D_N: torch.Size([64, 2, 6, 100])\n",
      " inputs_2D_N: torch.Size([64, 2, 6, 100])\n",
      "inputs_2D_N: torch.Size([64, 100, 6, 2])\n",
      "Shape after transpose: torch.Size([64, 2, 6, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 6, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 6, 100])\n",
      "Shape after pool: torch.Size([64, 64, 3, 50])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 3, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 3, 50])\n",
      "Shape after pool: torch.Size([64, 64, 1, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 1, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 1, 1600])\n",
      "Shape after dropout: torch.Size([64, 25, 1, 1600])\n",
      "necessary shape for fc2 is 64, 32\n",
      "Shape after dropout2: torch.Size([64, 25, 1, 64])\n",
      "Shape after flatten: torch.Size([64, 1600])\n",
      "Shape of model_3_output: torch.Size([64, 12800])\n",
      "Shape of model_3_output after cat: torch.Size([64, 14400])\n",
      "Shape of output_C1: torch.Size([64, 100, 3])\n",
      "##################################\n",
      "first passage through encoder gru\n",
      "input to encoder_gru, torch.Size([64, 100, 12])\n",
      "shape of out_1 after lstm1, torch.Size([64, 100, 256])\n",
      "shape of out_1 dropout1, torch.Size([64, 100, 256])\n",
      "shape of out_2 after lstm1, torch.Size([64, 100, 128])\n",
      "shape of out_2 dropout2, torch.Size([64, 100, 128])\n",
      "shape of out_2 after flatten, torch.Size([64, 12800])\n",
      "necessary shape for last linear layer = 128000, 600\n",
      "shape encoder_gru output, torch.Size([64, 300])\n",
      "shape encoder_gru output after view, torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 1D\n",
      "shape of input, torch.Size([64, 100, 12])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "Shape after transpose: torch.Size([64, 12, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 100])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 100])\n",
      "Shape after pool: torch.Size([64, 64, 50])\n",
      "Shape after conv3 relu: torch.Size([64, 64, 50])\n",
      "Shape after conv4 relu: torch.Size([64, 64, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 50])\n",
      "Shape after pool: torch.Size([64, 64, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 128])\n",
      "Shape after dropout1: torch.Size([64, 25, 128])\n",
      "necessary shape for fc2 is 25*64, 25*64\n",
      "Shape after fc2: torch.Size([64, 25, 1600])\n",
      "Shape after dropout2: torch.Size([64, 25, 1600])\n",
      "Shape after flatten: torch.Size([64, 40000])\n",
      "Shape of model_2_output: torch.Size([64, 12800])\n",
      "Shape of model_2_output_1 (concactenating last dimension of model_2_output and x): torch.Size([64, 52800])\n",
      "necessary shape for linear 528 * w, 6*w\n",
      "Shape of output_C2: torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 2D\n",
      "shape of input x_1, torch.Size([64, 100, 12])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "shape of input x, torch.Size([64, 100, 6, 2])\n",
      "inputs_2D_N: torch.Size([64, 2, 6, 100])\n",
      " inputs_2D_N: torch.Size([64, 2, 6, 100])\n",
      "inputs_2D_N: torch.Size([64, 100, 6, 2])\n",
      "Shape after transpose: torch.Size([64, 2, 6, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 6, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 6, 100])\n",
      "Shape after pool: torch.Size([64, 64, 3, 50])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 3, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 3, 50])\n",
      "Shape after pool: torch.Size([64, 64, 1, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 1, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 1, 1600])\n",
      "Shape after dropout: torch.Size([64, 25, 1, 1600])\n",
      "necessary shape for fc2 is 64, 32\n",
      "Shape after dropout2: torch.Size([64, 25, 1, 64])\n",
      "Shape after flatten: torch.Size([64, 1600])\n",
      "Shape of model_3_output: torch.Size([64, 12800])\n",
      "Shape of model_3_output after cat: torch.Size([64, 14400])\n",
      "Shape of output_C1: torch.Size([64, 100, 3])\n",
      "##################################\n",
      "first passage through encoder gru\n",
      "input to encoder_gru, torch.Size([64, 100, 12])\n",
      "shape of out_1 after lstm1, torch.Size([64, 100, 256])\n",
      "shape of out_1 dropout1, torch.Size([64, 100, 256])\n",
      "shape of out_2 after lstm1, torch.Size([64, 100, 128])\n",
      "shape of out_2 dropout2, torch.Size([64, 100, 128])\n",
      "shape of out_2 after flatten, torch.Size([64, 12800])\n",
      "necessary shape for last linear layer = 128000, 600\n",
      "shape encoder_gru output, torch.Size([64, 300])\n",
      "shape encoder_gru output after view, torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 1D\n",
      "shape of input, torch.Size([64, 100, 12])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "Shape after transpose: torch.Size([64, 12, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 100])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 100])\n",
      "Shape after pool: torch.Size([64, 64, 50])\n",
      "Shape after conv3 relu: torch.Size([64, 64, 50])\n",
      "Shape after conv4 relu: torch.Size([64, 64, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 50])\n",
      "Shape after pool: torch.Size([64, 64, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 128])\n",
      "Shape after dropout1: torch.Size([64, 25, 128])\n",
      "necessary shape for fc2 is 25*64, 25*64\n",
      "Shape after fc2: torch.Size([64, 25, 1600])\n",
      "Shape after dropout2: torch.Size([64, 25, 1600])\n",
      "Shape after flatten: torch.Size([64, 40000])\n",
      "Shape of model_2_output: torch.Size([64, 12800])\n",
      "Shape of model_2_output_1 (concactenating last dimension of model_2_output and x): torch.Size([64, 52800])\n",
      "necessary shape for linear 528 * w, 6*w\n",
      "Shape of output_C2: torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 2D\n",
      "shape of input x_1, torch.Size([64, 100, 12])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "shape of input x, torch.Size([64, 100, 6, 2])\n",
      "inputs_2D_N: torch.Size([64, 2, 6, 100])\n",
      " inputs_2D_N: torch.Size([64, 2, 6, 100])\n",
      "inputs_2D_N: torch.Size([64, 100, 6, 2])\n",
      "Shape after transpose: torch.Size([64, 2, 6, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 6, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 6, 100])\n",
      "Shape after pool: torch.Size([64, 64, 3, 50])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 3, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 3, 50])\n",
      "Shape after pool: torch.Size([64, 64, 1, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 1, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 1, 1600])\n",
      "Shape after dropout: torch.Size([64, 25, 1, 1600])\n",
      "necessary shape for fc2 is 64, 32\n",
      "Shape after dropout2: torch.Size([64, 25, 1, 64])\n",
      "Shape after flatten: torch.Size([64, 1600])\n",
      "Shape of model_3_output: torch.Size([64, 12800])\n",
      "Shape of model_3_output after cat: torch.Size([64, 14400])\n",
      "Shape of output_C1: torch.Size([64, 100, 3])\n",
      "##################################\n",
      "first passage through encoder gru\n",
      "input to encoder_gru, torch.Size([10, 100, 12])\n",
      "shape of out_1 after lstm1, torch.Size([10, 100, 256])\n",
      "shape of out_1 dropout1, torch.Size([10, 100, 256])\n",
      "shape of out_2 after lstm1, torch.Size([10, 100, 128])\n",
      "shape of out_2 dropout2, torch.Size([10, 100, 128])\n",
      "shape of out_2 after flatten, torch.Size([10, 12800])\n",
      "necessary shape for last linear layer = 128000, 600\n",
      "shape encoder_gru output, torch.Size([10, 300])\n",
      "shape encoder_gru output after view, torch.Size([10, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 1D\n",
      "shape of input, torch.Size([10, 100, 12])\n",
      "input_1D_N_1: torch.Size([10, 12, 100])\n",
      "input_1D_N_1: torch.Size([10, 12, 100])\n",
      "input_1D_N_1: torch.Size([10, 12, 100])\n",
      "Shape after transpose: torch.Size([10, 12, 100])\n",
      "Shape after conv1 relu: torch.Size([10, 64, 100])\n",
      "Shape after conv2 relu: torch.Size([10, 64, 100])\n",
      "Shape after BN_2: torch.Size([10, 64, 100])\n",
      "Shape after pool: torch.Size([10, 64, 50])\n",
      "Shape after conv3 relu: torch.Size([10, 64, 50])\n",
      "Shape after conv4 relu: torch.Size([10, 64, 50])\n",
      "Shape after BN_4: torch.Size([10, 64, 50])\n",
      "Shape after pool: torch.Size([10, 64, 25])\n",
      "Shape after transpose: torch.Size([10, 25, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([10, 25, 128])\n",
      "Shape after dropout1: torch.Size([10, 25, 128])\n",
      "necessary shape for fc2 is 25*64, 25*64\n",
      "Shape after fc2: torch.Size([10, 25, 1600])\n",
      "Shape after dropout2: torch.Size([10, 25, 1600])\n",
      "Shape after flatten: torch.Size([10, 40000])\n",
      "Shape of model_2_output: torch.Size([10, 12800])\n",
      "Shape of model_2_output_1 (concactenating last dimension of model_2_output and x): torch.Size([10, 52800])\n",
      "necessary shape for linear 528 * w, 6*w\n",
      "Shape of output_C2: torch.Size([10, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 2D\n",
      "shape of input x_1, torch.Size([10, 100, 12])\n",
      "input_1D_N_1: torch.Size([10, 12, 100])\n",
      "input_1D_N_1: torch.Size([10, 12, 100])\n",
      "input_1D_N_1: torch.Size([10, 12, 100])\n",
      "shape of input x, torch.Size([10, 100, 6, 2])\n",
      "inputs_2D_N: torch.Size([10, 2, 6, 100])\n",
      " inputs_2D_N: torch.Size([10, 2, 6, 100])\n",
      "inputs_2D_N: torch.Size([10, 100, 6, 2])\n",
      "Shape after transpose: torch.Size([10, 2, 6, 100])\n",
      "Shape after conv1 relu: torch.Size([10, 64, 6, 100])\n",
      "Shape after BN_2: torch.Size([10, 64, 6, 100])\n",
      "Shape after pool: torch.Size([10, 64, 3, 50])\n",
      "Shape after conv2 relu: torch.Size([10, 64, 3, 50])\n",
      "Shape after BN_4: torch.Size([10, 64, 3, 50])\n",
      "Shape after pool: torch.Size([10, 64, 1, 25])\n",
      "Shape after transpose: torch.Size([10, 25, 1, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([10, 25, 1, 1600])\n",
      "Shape after dropout: torch.Size([10, 25, 1, 1600])\n",
      "necessary shape for fc2 is 64, 32\n",
      "Shape after dropout2: torch.Size([10, 25, 1, 64])\n",
      "Shape after flatten: torch.Size([10, 1600])\n",
      "Shape of model_3_output: torch.Size([10, 12800])\n",
      "Shape of model_3_output after cat: torch.Size([10, 14400])\n",
      "Shape of output_C1: torch.Size([10, 100, 3])\n",
      "(1290, 100, 3)\n",
      "(1290, 100, 3)\n",
      "(193500,) (193500,)\n",
      "(64500, 3) (64500, 3)\n",
      "(64500, 3)\n",
      "15.72005424591778\n",
      "\n",
      "\n",
      "15.72005424591778\n",
      "\n",
      "\n",
      "0.8600890856197217\n",
      "\n",
      "\n",
      "0.8600890856197217\n"
     ]
    }
   ],
   "source": [
    "ait_Net_student= student(k_2-k_1,k_4-k_3,100)\n",
    "gait_Net_student.load_state_dict(torch.load(\"../../datacollection/vicon/subject_1/test_student/test_student.pth\"))\n",
    "gait_Net_student.to(device)\n",
    "\n",
    "gait_Net_student.eval()\n",
    "\n",
    "# iterate through batches of test data\n",
    "with torch.no_grad():\n",
    "    for i, (data_features_1D, data_features_2D, data_targets) in enumerate(test_loader):\n",
    "        output_1,output_2,output_3,output = gait_Net_student(data_features_1D[:,:,k_1:k_2].to(device).float(),data_features_2D[:,:,:,k_3:k_4].to(device).float())\n",
    "        if i==0:\n",
    "          yhat_5=output\n",
    "          test_target=data_targets\n",
    "\n",
    "        else:\n",
    "          yhat_5=torch.cat((yhat_5,output),dim=0)\n",
    "          test_target=torch.cat((test_target,data_targets),dim=0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "yhat_4 = yhat_5.detach().cpu().numpy()\n",
    "test_y = test_target.detach().cpu().numpy()\n",
    "print(yhat_4.shape)\n",
    "\n",
    " ### Present ###\n",
    "yhat_5=yhat_4.reshape((yhat_4.shape[0]*w,3))\n",
    "test_y_r=test_y.reshape((test_y.shape[0]*w,3))\n",
    "\n",
    "print(yhat_4.shape)\n",
    "\n",
    "### Unpack ###\n",
    "yhat_up=unpack_dataset_present(np.array(yhat_5))\n",
    "test_y_up=unpack_dataset_present(np.array(test_y_r))\n",
    "\n",
    "print(yhat_up.shape,test_y_up.shape)\n",
    "\n",
    "### Present ###\n",
    "\n",
    "yhat_up=yhat_up.reshape(int(len(yhat_up)/3),3)\n",
    "test_y_up=test_y_up.reshape(int(len(test_y_up)/3),3)\n",
    "\n",
    "print(yhat_up.shape,test_y_up.shape)\n",
    "\n",
    "print(yhat_up.shape)\n",
    "\n",
    "\n",
    "### Present ###\n",
    "\n",
    "rmse,p= prediction_test(np.array(yhat_up),np.array(test_y_up))\n",
    "\n",
    "print(rmse)\n",
    "\n",
    "m=rmse\n",
    "\n",
    "print('\\n')\n",
    "print(m)\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "print(p)\n",
    "print('\\n')\n",
    "\n",
    "print(p)\n",
    "\n",
    "ablation_2=np.hstack([rmse,p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################\n",
      "first passage through encoder gru\n",
      "input to encoder_gru, torch.Size([64, 100, 12])\n",
      "shape of out_1 after lstm1, torch.Size([64, 100, 256])\n",
      "shape of out_1 dropout1, torch.Size([64, 100, 256])\n",
      "shape of out_2 after lstm1, torch.Size([64, 100, 128])\n",
      "shape of out_2 dropout2, torch.Size([64, 100, 128])\n",
      "shape of out_2 after flatten, torch.Size([64, 12800])\n",
      "necessary shape for last linear layer = 128000, 600\n",
      "shape encoder_gru output, torch.Size([64, 300])\n",
      "shape encoder_gru output after view, torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 1D\n",
      "shape of input, torch.Size([64, 100, 12])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "Shape after transpose: torch.Size([64, 12, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 100])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 100])\n",
      "Shape after pool: torch.Size([64, 64, 50])\n",
      "Shape after conv3 relu: torch.Size([64, 64, 50])\n",
      "Shape after conv4 relu: torch.Size([64, 64, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 50])\n",
      "Shape after pool: torch.Size([64, 64, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 128])\n",
      "Shape after dropout1: torch.Size([64, 25, 128])\n",
      "necessary shape for fc2 is 25*64, 25*64\n",
      "Shape after fc2: torch.Size([64, 25, 1600])\n",
      "Shape after dropout2: torch.Size([64, 25, 1600])\n",
      "Shape after flatten: torch.Size([64, 40000])\n",
      "Shape of model_2_output: torch.Size([64, 12800])\n",
      "Shape of model_2_output_1 (concactenating last dimension of model_2_output and x): torch.Size([64, 52800])\n",
      "necessary shape for linear 528 * w, 6*w\n",
      "Shape of output_C2: torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 2D\n",
      "shape of input x_1, torch.Size([64, 100, 12])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "input_1D_N_1: torch.Size([64, 12, 100])\n",
      "shape of input x, torch.Size([64, 100, 6, 2])\n",
      "inputs_2D_N: torch.Size([64, 2, 6, 100])\n",
      " inputs_2D_N: torch.Size([64, 2, 6, 100])\n",
      "inputs_2D_N: torch.Size([64, 100, 6, 2])\n",
      "Shape after transpose: torch.Size([64, 2, 6, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 6, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 6, 100])\n",
      "Shape after pool: torch.Size([64, 64, 3, 50])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 3, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 3, 50])\n",
      "Shape after pool: torch.Size([64, 64, 1, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 1, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 1, 1600])\n",
      "Shape after dropout: torch.Size([64, 25, 1, 1600])\n",
      "necessary shape for fc2 is 64, 32\n",
      "Shape after dropout2: torch.Size([64, 25, 1, 64])\n",
      "Shape after flatten: torch.Size([64, 1600])\n",
      "Shape of model_3_output: torch.Size([64, 12800])\n",
      "Shape of model_3_output after cat: torch.Size([64, 14400])\n",
      "Shape of output_C1: torch.Size([64, 100, 3])\n",
      "##################################\n",
      "first passage through encoder gru\n",
      "input to encoder_gru, torch.Size([64, 100, 48])\n",
      "shape of out_1 after lstm1, torch.Size([64, 100, 256])\n",
      "shape of out_1 dropout1, torch.Size([64, 100, 256])\n",
      "shape of out_2 after lstm1, torch.Size([64, 100, 128])\n",
      "shape of out_2 dropout2, torch.Size([64, 100, 128])\n",
      "shape of out_2 after flatten, torch.Size([64, 12800])\n",
      "necessary shape for last linear layer = 128000, 600\n",
      "shape encoder_gru output, torch.Size([64, 300])\n",
      "shape encoder_gru output after view, torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 1D\n",
      "shape of input, torch.Size([64, 100, 48])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "Shape after transpose: torch.Size([64, 48, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 100])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 100])\n",
      "Shape after pool: torch.Size([64, 64, 50])\n",
      "Shape after conv3 relu: torch.Size([64, 64, 50])\n",
      "Shape after conv4 relu: torch.Size([64, 64, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 50])\n",
      "Shape after pool: torch.Size([64, 64, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 128])\n",
      "Shape after dropout1: torch.Size([64, 25, 128])\n",
      "necessary shape for fc2 is 25*64, 25*64\n",
      "Shape after fc2: torch.Size([64, 25, 1600])\n",
      "Shape after dropout2: torch.Size([64, 25, 1600])\n",
      "Shape after flatten: torch.Size([64, 40000])\n",
      "Shape of model_2_output: torch.Size([64, 12800])\n",
      "Shape of model_2_output_1 (concactenating last dimension of model_2_output and x): torch.Size([64, 52800])\n",
      "necessary shape for linear 528 * w, 6*w\n",
      "Shape of output_C2: torch.Size([64, 100, 3])\n",
      "##################################\n",
      "forward pass encoder 2D\n",
      "shape of input x_1, torch.Size([64, 100, 48])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "input_1D_N_1: torch.Size([64, 48, 100])\n",
      "shape of input x, torch.Size([64, 100, 6, 8])\n",
      "inputs_2D_N: torch.Size([64, 8, 6, 100])\n",
      " inputs_2D_N: torch.Size([64, 8, 6, 100])\n",
      "inputs_2D_N: torch.Size([64, 100, 6, 8])\n",
      "Shape after transpose: torch.Size([64, 8, 6, 100])\n",
      "Shape after conv1 relu: torch.Size([64, 64, 6, 100])\n",
      "Shape after BN_2: torch.Size([64, 64, 6, 100])\n",
      "Shape after pool: torch.Size([64, 64, 3, 50])\n",
      "Shape after conv2 relu: torch.Size([64, 64, 3, 50])\n",
      "Shape after BN_4: torch.Size([64, 64, 3, 50])\n",
      "Shape after pool: torch.Size([64, 64, 1, 25])\n",
      "Shape after transpose: torch.Size([64, 25, 1, 64])\n",
      "necessary shape for fc1 is 64, 25*64\n",
      "Shape after fc1: torch.Size([64, 25, 1, 1600])\n",
      "Shape after dropout: torch.Size([64, 25, 1, 1600])\n",
      "necessary shape for fc2 is 64, 32\n",
      "Shape after dropout2: torch.Size([64, 25, 1, 64])\n",
      "Shape after flatten: torch.Size([64, 1600])\n",
      "Shape of model_3_output: torch.Size([64, 12800])\n",
      "Shape of model_3_output after cat: torch.Size([64, 14400])\n",
      "Shape of output_C1: torch.Size([64, 100, 3])\n",
      "shape of out_gru, torch.Size([64, 100, 3])\n",
      "shape of out_C2, torch.Size([64, 100, 3])\n",
      "shape of out_C1, torch.Size([64, 100, 3])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for *: 'student' and 'Tensor'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 21\u001b[0m\n\u001b[1;32m     16\u001b[0m Teacher\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../../datacollection/vicon/subject_1/test/test.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     17\u001b[0m Teacher\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 21\u001b[0m student_KD\u001b[38;5;241m=\u001b[39m \u001b[43mtrain_SD\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mStudent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnum_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mStudent\u001b[49m\u001b[43m,\u001b[49m\u001b[43mTeacher\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../../datacollection/vicon/subject_1/test_student/test_student.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk_1s\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk_2s\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk_3s\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk_4s\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk_1t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk_2t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk_3t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk_4t\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/SD/SD1-project/ml/examples/../Sensor_distillation_utils/sensor_distillation.py:106\u001b[0m, in \u001b[0;36mtrain_SD\u001b[0;34m(model, alpha, device, val_loader, train_loader, learn_rate, EPOCHS, student, teacher, filename, k_1s, k_2s, k_3s, k_4s, k_1t, k_2t, k_3t, k_4t)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m    104\u001b[0m  output_1t, output_2t, output_3t, outputt\u001b[38;5;241m=\u001b[39m teacher(data_features_1D[:,:,k_1t:k_2t]\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mfloat(),data_features_2D[:,:,:,k_3t:k_4t]\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mfloat())\n\u001b[0;32m--> 106\u001b[0m loss\u001b[38;5;241m=\u001b[39mcriterion(output, data_targets\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mfloat())\u001b[38;5;241m+\u001b[39m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    109\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'student' and 'Tensor'"
     ]
    }
   ],
   "source": [
    "k_1s=36\n",
    "k_2s=48\n",
    "k_3s=6\n",
    "k_4s=8\n",
    "\n",
    "lr = 0.001\n",
    "Student = student(k_2s-k_1s,k_4s-k_3s,100)\n",
    "\n",
    "k_1t=0\n",
    "k_2t=48\n",
    "k_3t=0\n",
    "k_4t=8\n",
    "\n",
    "lr = 0.001\n",
    "Teacher = teacher(k_2t-k_1t,k_4t-k_3t,100)\n",
    "Teacher.load_state_dict(torch.load(\"../../datacollection/vicon/subject_1/test/test.pth\"))\n",
    "Teacher.to(device)\n",
    "\n",
    "\n",
    "\n",
    "student_KD= train_SD(device, alpha, val_loader, train_loader, lr,num_epoch, Student,Teacher, \"../../datacollection/vicon/subject_1/test_student/test_student.pth\", k_1s, k_2s, k_3s, k_4s, k_1t, k_2t, k_3t, k_4t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
