{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports from Sanzid\n",
    "import h5py\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import numpy\n",
    "import statistics\n",
    "from numpy import loadtxt\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas\n",
    "import math\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from statistics import stdev\n",
    "import math\n",
    "import h5py\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from scipy.signal import butter,filtfilt\n",
    "import sys\n",
    "import numpy as np # linear algebra\n",
    "from scipy.stats import randint\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv), data manipulation as in SQL\n",
    "import matplotlib.pyplot as plt # this is used for the plot the graph\n",
    "import seaborn as sns # used for plot interactive graph.\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# from tsf.model import TransformerForecaster\n",
    "\n",
    "\n",
    "# from tensorflow.keras.utils import np_utils\n",
    "import itertools\n",
    "###  Library for attention layers\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "#from tqdm import tqdm # Processing time measurement\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import statistics\n",
    "import gc\n",
    "import torch.nn.init as init\n",
    "\n",
    "############################################################################################################################################################################\n",
    "############################################################################################################################################################################\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.utils.weight_norm as weight_norm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "#from torchsummary import summary\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "#from torchsummary import summary\n",
    "\n",
    "\n",
    "\n",
    "import torch.optim as optim\n",
    "import gc\n",
    "\n",
    "#from tqdm import tqdm_notebook\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports from dataloader final\n",
    "import sys #you need this to add path to utils folder\n",
    "sys.path.append('../utils') #now you can import from utils folder\n",
    "# sys.path.append(\"../..\")\n",
    "from configs import config_general\n",
    "from datasharders import datasharder_imu_joints\n",
    "from datasets import ImuJointPairDataset\n",
    "from data_utils import fft_filter_signal, wavelet_filter_signal\n",
    "from model_utils import plot_predictions, plot_loss, train_model, evaluate_model\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split \n",
    "import matplotlib.pyplot as plt\n",
    "from LSTM import DeepLSTMModel\n",
    "import torch.nn as nn\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports for this notebook using sensor distillation utils\n",
    "import sys #you need this to add path to utils folder\n",
    "sys.path.append('../Sensor_distillation_utils') #now you can import from utils folder\n",
    "from sensor_distillation import train_SD\n",
    "from student_model import student\n",
    "from teacher_model import teacher\n",
    "from train import train_kinematics\n",
    "from predictions import prediction_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#config from dataloader final\n",
    "#some config errors\n",
    "\n",
    "# Initialize config object\n",
    "config = config_general(\n",
    "    batch_size=64,\n",
    "    epochs=1000,\n",
    "    lr=0.002,\n",
    "    scheduler=None,\n",
    "    num_channels_imu=3,\n",
    "    num_channels_joints=3,\n",
    "    num_sessions=1,\n",
    "    num_patients=3,\n",
    "    seed=42,\n",
    "    data_folder_name=\"../../datacollection/data\",\n",
    "    dataset_root=\"../../datasets\",\n",
    "    dataset_train_name=\"train_dataset\",\n",
    "    dataset_test_name=\"test_dataset\",\n",
    "    window_length=100,\n",
    "    imu_transforms=[fft_filter_signal],\n",
    "    joint_transforms=[], \n",
    "    hidden_size=256,\n",
    "    num_layers=6,\n",
    "    input_size=3,\n",
    "    output_size=3\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#main function that runs everything from dataloader final\n",
    "def main(config, remake_dataset):\n",
    "    \n",
    "    \n",
    "    #device definition?\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    model = DeepLSTMModel(input_size=config.input_size, hidden_size=config.hidden_size, num_layers=config.num_layers, output_size=config.output_size).to(device)\n",
    "    criterion = nn.MSELoss() #changes depending on model\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config.lr) #changes depending on model\n",
    "   \n",
    "    \n",
    "    #remake_dataset false if no changes made, true if changes need to be made \n",
    "    #remake_dataset = True\n",
    "    \n",
    "    if(remake_dataset):\n",
    "    \n",
    "        #call datasharder on config \n",
    "        datasharder = datasharder_imu_joints(config, sample_rate=16000)\n",
    "    \n",
    "        #access data from datasharder\n",
    "        training_data, testing_data = datasharder.load_data()\n",
    "\n",
    "    \n",
    "        #save windowed data to config specified destination\n",
    "        datasharder.save_windowed_data(training_data, \"train\")\n",
    "        datasharder.save_windowed_data(testing_data, \"test\")\n",
    "    \n",
    "   #split dataset into train and test\n",
    "    dataset_train = ImuJointPairDataset(config, \"train\")\n",
    "    dataset_test = ImuJointPairDataset(config, \"test\")\n",
    "    #print(len(dataset_train)) \n",
    "    #print(len(dataset_test))\n",
    "    #STOP \n",
    "    \n",
    "    \n",
    "    imu_data_tensor,joint_data_tensor=dataset_train.__getitem__(1)\n",
    "\n",
    "    print(imu_data_tensor.shape, joint_data_tensor.shape)\n",
    "    \n",
    "    #convert to numpy\n",
    "    imu_data = imu_data_tensor.numpy()\n",
    "    joint_data = joint_data_tensor.numpy()\n",
    "    sns.set(style=\"ticks\")\n",
    "    plt.plot(imu_data[:,1], label=\"imu\")\n",
    "    plt.plot(joint_data[:,1], label=\"joint\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    train_size = int(0.75 * len(dataset_train))\n",
    "    val_size = int(0.25 * len(dataset_train))\n",
    "    \n",
    "    train_dataset, val_dataset = random_split(dataset_train, [train_size, val_size])\n",
    "\n",
    "    test_dataset = dataset_test\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=config.batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=config.batch_size, shuffle=False)\n",
    "    \n",
    "    #train model\n",
    "    train_losses, val_losses = train_model(model, train_loader, val_loader, config.epochs, criterion, optimizer, device)\n",
    "    \n",
    "    # Plot training and validation loss history\n",
    "    plot_loss(train_losses, val_losses)\n",
    "    \n",
    "    # Evaluate model on validation and test sets and plot predictions\n",
    "    val_inputs, val_targets, val_predictions = evaluate_model(model, val_loader, device)\n",
    "    plot_predictions(val_inputs.squeeze(), val_targets.squeeze(), val_predictions.squeeze(), num_channels=3)\n",
    "\n",
    "    test_inputs, test_targets, test_predictions = evaluate_model(model, test_loader, device)\n",
    "    plot_predictions(test_inputs.squeeze(), test_targets.squeeze(), test_predictions.squeeze(), num_channels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use main function accessing imu and joint data files directly from dataloader final\n",
    "main(config, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading in data from lightweight kinematics net\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "#load all csvs from the folder and concatenate them into one dataframe by matching the columns\n",
    "def load_data(path):\n",
    "    df_list = []\n",
    "    for file in os.listdir(path):\n",
    "        if file.endswith('.csv'):\n",
    "            data = pd.read_csv(os.path.join(path, file))\n",
    "            df_list.append(data)\n",
    "    df = pd.concat(df_list, ignore_index=True, sort=False)\n",
    "    return df\n",
    "\n",
    "\n",
    "# Index 0: pelvis_tilt\n",
    "# Index 1: pelvis_list\n",
    "# Index 2: pelvis_rotation\n",
    "# Index 3: pelvis_tx\n",
    "# Index 4: pelvis_ty\n",
    "# Index 5: pelvis_tz\n",
    "# Index 6: hip_flexion_r\n",
    "# Index 7: hip_adduction_r\n",
    "# Index 8: hip_rotation_r\n",
    "# Index 9: knee_angle_r\n",
    "# Index 10: knee_angle_r_beta\n",
    "# Index 11: ankle_angle_r\n",
    "# Index 12: subtalar_angle_r\n",
    "# Index 13: mtp_angle_r\n",
    "# Index 14: hip_flexion_l\n",
    "# Index 15: hip_adduction_l\n",
    "# Index 16: hip_rotation_l\n",
    "# Index 17: knee_angle_l\n",
    "# Index 18: knee_angle_l_beta\n",
    "# Index 19: ankle_angle_l\n",
    "# Index 20: subtalar_angle_l\n",
    "# Index 21: mtp_angle_l\n",
    "# Index 22: lumbar_extension\n",
    "# Index 23: lumbar_bending\n",
    "# Index 24: lumbar_rotation\n",
    "# Index 25: arm_flex_r\n",
    "# Index 26: arm_add_r\n",
    "# Index 27: arm_rot_r\n",
    "# Index 28: elbow_flex_r\n",
    "# Index 29: pro_sup_r\n",
    "# Index 30: wrist_flex_r\n",
    "# Index 31: wrist_dev_r\n",
    "# Index 32: arm_flex_l\n",
    "# Index 33: arm_add_l\n",
    "# Index 34: arm_rot_l\n",
    "# Index 35: elbow_flex_l\n",
    "# Index 36: pro_sup_l\n",
    "# Index 37: wrist_flex_l\n",
    "# Index 38: wrist_dev_l\n",
    "# Index 39: time\n",
    "# Index 40: 1\n",
    "# Index 41: IM EMG1\n",
    "# Index 42: IM EMG2\n",
    "# Index 43: IM EMG3\n",
    "# Index 44: IM EMG4\n",
    "# Index 45: IM EMG5\n",
    "# Index 46: IM EMG6\n",
    "# Index 47: IM EMG7\n",
    "# Index 48: IM EMG8\n",
    "# Index 49: ACCX1\n",
    "# Index 50: ACCY1\n",
    "# Index 51: ACCZ1\n",
    "# Index 52: GYROX1\n",
    "# Index 53: GYROY1\n",
    "# Index 54: GYROZ1\n",
    "# Index 55: ACCX2\n",
    "# Index 56: ACCY2\n",
    "# Index 57: ACCZ2\n",
    "# Index 58: GYROX2\n",
    "# Index 59: GYROY2\n",
    "# Index 60: GYROZ2\n",
    "# Index 61: ACCX3\n",
    "# Index 62: ACCY3\n",
    "# Index 63: ACCZ3\n",
    "# Index 64: GYROX3\n",
    "# Index 65: GYROY3\n",
    "# Index 66: GYROZ3\n",
    "# Index 67: ACCX4\n",
    "# Index 68: ACCY4\n",
    "# Index 69: ACCZ4\n",
    "# Index 70: GYROX4\n",
    "# Index 71: GYROY4\n",
    "# Index 72: GYROZ4\n",
    "# Index 73: ACCX5\n",
    "# Index 74: ACCY5\n",
    "# Index 75: ACCZ5\n",
    "# Index 76: GYROX5\n",
    "# Index 77: GYROY5\n",
    "# Index 78: GYROZ5\n",
    "# Index 79: ACCX6\n",
    "# Index 80: ACCY6\n",
    "# Index 81: ACCZ6\n",
    "# Index 82: GYROX6\n",
    "# Index 83: GYROY6\n",
    "# Index 84: GYROZ6\n",
    "# Index 85: ACCX7\n",
    "# Index 86: ACCY7\n",
    "# Index 87: ACCZ7\n",
    "# Index 88: GYROX7\n",
    "# Index 89: GYROY7\n",
    "# Index 90: GYROZ7\n",
    "# Index 91: ACCX8\n",
    "# Index 92: ACCY8\n",
    "# Index 93: ACCZ8\n",
    "# Index 94: GYROX8\n",
    "# Index 95: GYROY8\n",
    "# Index 96: GYROZ8\n",
    "\n",
    "\n",
    "# gait_Net = train_kinematics_light(train_loader, lr,30,model,path+encoder+'_gait_net_kinematics_lightweight.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loader will return [batch_size, sequence_length, input_dim] for 3 vector, acc and gyro as well as the target \n",
    "# from kinematics net notebook\n",
    "class Sensor_Distillation_Dataset(Dataset):\n",
    "    def __init__(self, data_acc, data_gyr, target, w, overlap=200):\n",
    "        self.data_acc = data_acc\n",
    "        self.data_gyr = data_gyr\n",
    "        self.target = target\n",
    "        self.w = w\n",
    "        self.overlap = overlap\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.data_acc) - self.w) // (self.w - self.overlap) + 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        start = idx * (self.w - self.overlap)\n",
    "        end = start + self.w\n",
    "        return (torch.tensor(self.data_acc[start:end]),\n",
    "                torch.tensor(self.data_gyr[start:end]),\n",
    "                torch.tensor(self.target[start:end]))  # Updated to return the entire target sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'NoneType'>\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'isnull'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(combined_csv))\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Check for NaN values\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mcombined_csv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misnull\u001b[49m()\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39msum())\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Drop NaN values\u001b[39;00m\n\u001b[1;32m     13\u001b[0m combined_csv \u001b[38;5;241m=\u001b[39m combined_csv\u001b[38;5;241m.\u001b[39mdropna()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'isnull'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load and clean the combined CSV data\n",
    "import sys #you need this to add path to utils folder\n",
    "sys.path.append('../../datacollection') #now you can import from utils folder\n",
    "from batch_process import combine_and_save_data\n",
    "\n",
    "combine_and_save_data(\"../../datacollection/vicon/subject_1/motion\", \"../../datacollection/vicon/subject_1/processed\", \"../../datacollection/vicon/subject_1/combined\")\n",
    "\n",
    "combined_csv = \"../../datacollection/vicon/subject_1/combined\"\n",
    "\n",
    "print(type(combined_csv))\n",
    "# Check for NaN values\n",
    "print(combined_csv.isnull().sum().sum())\n",
    "\n",
    "# Drop NaN values\n",
    "combined_csv = combined_csv.dropna()\n",
    "\n",
    "print(combined_csv.isnull().sum().sum())\n",
    "\n",
    "# Plot ACCX8, ACCY8, ACCZ8, GYROX8, GYROY8, GYROZ8\n",
    "plt.plot(combined_csv['ACCX8'][:10000])\n",
    "plt.plot(combined_csv['ACCY8'][:10000])\n",
    "plt.plot(combined_csv['ACCZ8'][:10000])\n",
    "plt.plot(combined_csv['GYROX8'][:10000])\n",
    "plt.plot(combined_csv['GYROY8'][:10000])\n",
    "plt.plot(combined_csv['GYROZ8'][:10000])\n",
    "plt.show()\n",
    "\n",
    "# Get two 3-channel arrays for ACC and GYRO for sensor 2\n",
    "data_acc = combined_csv[['ACCX2', 'ACCY2', 'ACCZ2']].values\n",
    "data_gyr = combined_csv[['GYROX2', 'GYROY2', 'GYROZ2']].values\n",
    "\n",
    "# Normalize the data\n",
    "scaler_acc = StandardScaler()\n",
    "scaler_gyr = StandardScaler()\n",
    "scaler_joints = StandardScaler()\n",
    "\n",
    "data_acc = scaler_acc.fit_transform(data_acc)\n",
    "data_gyr = scaler_gyr.fit_transform(data_gyr)\n",
    "data_joints = combined_csv[['elbow_flex_r']].values\n",
    "data_joints = scaler_joints.fit_transform(data_joints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate model from kinematics net notebook\n",
    "model = Kinematics_lightweight(3, 3)\n",
    "\n",
    "model.to('cuda')\n",
    "# Calculate the total size of parameters in bytes\n",
    "total_size_bytes = sum(p.numel() * p.element_size() for p in model.parameters())\n",
    "print(f'Total size in bytes: {total_size_bytes}')\n",
    "\n",
    "# Convert total size in bytes to kilobytes\n",
    "total_size_kb = total_size_bytes / 1024\n",
    "print(f'Total size in kilobytes: {total_size_kb:.2f} KB')\n",
    "\n",
    "# Example alternative model\n",
    "# model = residual_net(6)\n",
    "\n",
    "# Calculate the total size of parameters in bytes\n",
    "total_size_bytes = sum(p.numel() * p.element_size() for p in model.parameters())\n",
    "print(f'Total size in bytes: {total_size_bytes}')\n",
    "\n",
    "# Convert total size in bytes to kilobytes\n",
    "total_size_kb = total_size_bytes / 1024\n",
    "print(f'Total size in kilobytes: {total_size_kb:.2f} KB')\n",
    "\n",
    "# Load and clean the combined CSV data\n",
    "combined_csv = load_data(\"../../datacollection/vicon/subject_1/combined\")\n",
    "\n",
    "# Check for NaN values\n",
    "print(combined_csv.isnull().sum().sum())\n",
    "\n",
    "# Drop NaN values\n",
    "combined_csv = combined_csv.dropna()\n",
    "\n",
    "print(combined_csv.isnull().sum().sum())\n",
    "\n",
    "# Plot ACCX8, ACCY8, ACCZ8, GYROX8, GYROY8, GYROZ8\n",
    "plt.plot(combined_csv['ACCX8'][:10000])\n",
    "plt.plot(combined_csv['ACCY8'][:10000])\n",
    "plt.plot(combined_csv['ACCZ8'][:10000])\n",
    "plt.plot(combined_csv['GYROX8'][:10000])\n",
    "plt.plot(combined_csv['GYROY8'][:10000])\n",
    "plt.plot(combined_csv['GYROZ8'][:10000])\n",
    "plt.show()\n",
    "\n",
    "# Get two 3-channel arrays for ACC and GYRO for sensor 2\n",
    "data_acc = combined_csv[['ACCX2', 'ACCY2', 'ACCZ2']].values\n",
    "data_gyr = combined_csv[['GYROX2', 'GYROY2', 'GYROZ2']].values\n",
    "\n",
    "# Normalize the data\n",
    "scaler_acc = StandardScaler()\n",
    "scaler_gyr = StandardScaler()\n",
    "scaler_joints = StandardScaler()\n",
    "\n",
    "data_acc = scaler_acc.fit_transform(data_acc)\n",
    "data_gyr = scaler_gyr.fit_transform(data_gyr)\n",
    "data_joints = combined_csv[['elbow_flex_r']].values\n",
    "data_joints = scaler_joints.fit_transform(data_joints)\n",
    "\n",
    "# Create dataset instances with overlapping windows\n",
    "dataset = KinematicsDataset(data_acc, data_gyr, data_joints, w=1000, overlap=500)\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get first batch\n",
    "data_acc, data_gyr, data_targets = next(iter(train_loader))\n",
    "\n",
    "#print the shapes of the tensors\n",
    "print(data_acc.shape)\n",
    "print(data_gyr.shape)\n",
    "print(data_targets.shape)\n",
    "\n",
    "\n",
    "#plot the first 1000 samples of the first batch\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(data_acc[0, :, 0], label='ACCX')\n",
    "plt.plot(data_acc[0, :, 1], label='ACCY')\n",
    "plt.plot(data_acc[0, :, 2], label='ACCZ')\n",
    "plt.plot(data_gyr[0, :, 0], label='GYROX')\n",
    "plt.plot(data_gyr[0, :, 1], label='GYROY')\n",
    "plt.plot(data_gyr[0, :, 2], label='GYROZ')\n",
    "plt.legend()\n",
    "plt.title('Sensor Data')\n",
    "plt.show()\n",
    "\n",
    "#plot the first 1000 samples of the first batch\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(data_targets[0,:1000])\n",
    "plt.title('Target')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(combined_csv[['elbow_flex_r']].values[:1000])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Train the model\n",
    "gait_Net = train_kinematics_light(train_loader, val_loader, learn_rate=0.001, EPOCHS=100, model=model, filename='gait_net_kinematics_lightweight.pth', device=device, w=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load in test data\n",
    "combined_csv_test = load_data(\"../../datacollection/vicon/subject_2/combined\")\n",
    "\n",
    "# Check for NaN values\n",
    "print(combined_csv_test.isnull().sum().sum())\n",
    "\n",
    "\n",
    "# Drop NaN values\n",
    "combined_csv_test = combined_csv_test.dropna()\n",
    "\n",
    "print(combined_csv_test.isnull().sum().sum())\n",
    "\n",
    "combined_csv_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ACCX8, ACCY8, ACCZ8, GYROX8, GYROY8, GYROZ8\n",
    "plt.plot(combined_csv_test['ACCX10'][:10000])\n",
    "plt.plot(combined_csv_test['ACCY10'][:10000])\n",
    "plt.plot(combined_csv_test['ACCZ10'][:10000])\n",
    "plt.plot(combined_csv_test['GYROX10'][:10000])\n",
    "plt.plot(combined_csv_test['GYROY10'][:10000])\n",
    "plt.plot(combined_csv_test['GYROZ10'][:10000])\n",
    "plt.show()\n",
    "\n",
    "# Get two 3-channel arrays for ACC and GYRO for sensor 2\n",
    "data_acc_test = combined_csv_test[['ACCX2', 'ACCY2', 'ACCZ2']].values\n",
    "data_gyr_test = combined_csv_test[['GYROX2', 'GYROY2', 'GYROZ2']].values\n",
    "\n",
    "# Normalize the data\n",
    "scaler_acc = StandardScaler()\n",
    "scaler_gyr = StandardScaler()\n",
    "scaler_joints = StandardScaler()\n",
    "\n",
    "data_acc_test = scaler_acc.fit_transform(data_acc_test)\n",
    "data_gyr_test = scaler_gyr.fit_transform(data_gyr_test)\n",
    "data_targets_test = combined_csv_test[['elbow_flex_r']].values\n",
    "data_targets_test = scaler_joints.fit_transform(data_targets_test)\n",
    "\n",
    "\n",
    "# Create dataset instances with overlapping windows for test data\n",
    "test_dataset = KinematicsDataset(data_acc_test, data_gyr_test, data_targets_test, w=1000, overlap=500)\n",
    "\n",
    "# Create DataLoader for test data\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Load the trained model\n",
    "model = Kinematics_lightweight(3, 3)\n",
    "model.load_state_dict(torch.load('gait_net_kinematics_lightweight.pth'))\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss = 0\n",
    "all_predictions = []\n",
    "all_targets = []\n",
    "\n",
    "criterion = RMSELoss()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data_acc, data_gyr, data_targets in test_loader:\n",
    "        data_acc = data_acc.to(device).float()\n",
    "        data_gyr = data_gyr.to(device).float()\n",
    "        data_targets = data_targets.to(device).float().squeeze(-1)\n",
    "\n",
    "        output = model(data_acc, data_gyr)\n",
    "        loss = criterion(output, data_targets)\n",
    "        test_loss += loss.item()\n",
    "\n",
    "        all_predictions.append(output.cpu().numpy())\n",
    "        all_targets.append(data_targets.cpu().numpy())\n",
    "\n",
    "test_loss /= len(test_loader)\n",
    "print(f'Test Loss: {test_loss:.4f}')\n",
    "\n",
    "# Combine all predictions and targets\n",
    "all_predictions = np.concatenate(all_predictions)\n",
    "all_targets = np.concatenate(all_targets)\n",
    "\n",
    "print(all_predictions.shape, all_targets.shape)  # Print shapes for debugging\n",
    "\n",
    "# Plot individual graphs for each prediction and target\n",
    "num_graphs = 10\n",
    "for i in range(num_graphs):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(all_targets[i], label='True Values')\n",
    "    plt.plot(all_predictions[i], label='Predictions')\n",
    "    plt.xlabel('Time Frames')\n",
    "    plt.ylabel('Elbow Flexion (r)')\n",
    "    plt.legend()\n",
    "    plt.title(f'Predictions vs True Values for Sample {i+1}')\n",
    "\n",
    "    #set limits for y axis\n",
    "    plt.ylim(-3,3)\n",
    "    \n",
    "   \n",
    "\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_predictions.shape, all_targets.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the trained model\n",
    "model = Kinematics_lightweight(3, 3)\n",
    "model.load_state_dict(torch.load('gait_net_kinematics_lightweight.pth'))\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss = 0\n",
    "all_predictions = []\n",
    "all_targets = []\n",
    "\n",
    "criterion = RMSELoss()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data_acc, data_gyr, data_targets in val_loader:\n",
    "        data_acc = data_acc.to(device).float()\n",
    "        data_gyr = data_gyr.to(device).float()\n",
    "        data_targets = data_targets.to(device).float().squeeze(-1)\n",
    "\n",
    "        output = model(data_acc, data_gyr)\n",
    "        loss = criterion(output, data_targets)\n",
    "        test_loss += loss.item()\n",
    "\n",
    "        all_predictions.append(output.cpu().numpy())\n",
    "        all_targets.append(data_targets.cpu().numpy())\n",
    "\n",
    "test_loss /= len(test_loader)\n",
    "print(f'Validation Loss: {test_loss:.4f}')\n",
    "\n",
    "# Combine all predictions and targets\n",
    "all_predictions = np.concatenate(all_predictions)\n",
    "all_targets = np.concatenate(all_targets)\n",
    "\n",
    "print(all_predictions.shape, all_targets.shape)  # Print shapes for debugging\n",
    "\n",
    "# Calculate slopes\n",
    "def calculate_slope(values):\n",
    "    return np.gradient(values, axis=1)\n",
    "\n",
    "slopes_predictions = calculate_slope(all_predictions)\n",
    "slopes_targets = calculate_slope(all_targets)\n",
    "\n",
    "# Plot individual graphs for each prediction and target\n",
    "num_graphs = 10\n",
    "for i in range(num_graphs):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(all_targets[i], label='True Values')\n",
    "    plt.plot(all_predictions[i], label='Predictions')\n",
    "    plt.xlabel('Time Frames')\n",
    "    plt.ylabel('Elbow Flexion (r)')\n",
    "    plt.legend()\n",
    "    plt.title(f'Predictions vs Validation Values for Sample {i+1}')\n",
    "    plt.ylim(-3, 3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_graphs = 10\n",
    "for i in range(num_graphs):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(slopes_targets[i], label='True Values Slope')\n",
    "    plt.plot(slopes_predictions[i], label='Predictions Slope')\n",
    "    plt.xlabel('Time Frames')\n",
    "    plt.ylabel('Slope')\n",
    "    plt.legend()\n",
    "    plt.title(f'Slope Comparison for Sample {i+1}')\n",
    "    plt.ylim(-.005, .005)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to calculate linear regression\n",
    "def linear_regression(values):\n",
    "    x = np.arange(values.shape[0])\n",
    "    slope, intercept, _, _, _ = linregress(x, values)\n",
    "    return slope * x + intercept, slope, intercept\n",
    "\n",
    "# Calculate linear regression lines and get their slopes and intercepts\n",
    "regression_lines_predictions = []\n",
    "regression_lines_targets = []\n",
    "slopes_predictions = []\n",
    "intercepts_predictions = []\n",
    "slopes_targets = []\n",
    "intercepts_targets = []\n",
    "\n",
    "for pred, target in zip(all_predictions, all_targets):\n",
    "    reg_pred, slope_pred, intercept_pred = linear_regression(pred)\n",
    "    reg_target, slope_target, intercept_target = linear_regression(target)\n",
    "    regression_lines_predictions.append(reg_pred)\n",
    "    regression_lines_targets.append(reg_target)\n",
    "    slopes_predictions.append(slope_pred)\n",
    "    intercepts_predictions.append(intercept_pred)\n",
    "    slopes_targets.append(slope_target)\n",
    "    intercepts_targets.append(intercept_target)\n",
    "\n",
    "# Convert to numpy arrays\n",
    "regression_lines_predictions = np.array(regression_lines_predictions)\n",
    "regression_lines_targets = np.array(regression_lines_targets)\n",
    "\n",
    "# Plot individual graphs for each prediction and target with linear regression lines\n",
    "num_graphs = 10\n",
    "for i in range(num_graphs):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(regression_lines_targets[i], label='True Values Regression', linestyle='--')\n",
    "    plt.plot(regression_lines_predictions[i], label='Predictions Regression', linestyle='--')\n",
    "    plt.xlabel('Time Frames')\n",
    "    plt.ylabel('Elbow Flexion (r)')\n",
    "    plt.legend()\n",
    "    plt.title(f'Predictions vs Validation Values with Linear Regression for Sample {i+1}')\n",
    "    plt.ylim(-3, 3)\n",
    "\n",
    "    # Add text annotations for the linear regression formulas\n",
    "    formula_target = f'True Values: y = {slopes_targets[i]:.2f}x + {intercepts_targets[i]:.2f}'\n",
    "    formula_pred = f'Predictions: y = {slopes_predictions[i]:.2f}x + {intercepts_predictions[i]:.2f}'\n",
    "    plt.text(0.05, 0.95, formula_target, transform=plt.gca().transAxes, fontsize=10, verticalalignment='top')\n",
    "    plt.text(0.05, 0.90, formula_pred, transform=plt.gca().transAxes, fontsize=10, verticalalignment='top')\n",
    "\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
