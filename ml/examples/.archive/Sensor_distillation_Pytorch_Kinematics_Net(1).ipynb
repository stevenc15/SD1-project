{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "W17jUoA2pwjm",
        "8A4Eoxgop1qX",
        "dbbpf3z7eCAf",
        "bRf9hcyFgZZ1",
        "ROjfjLaBfcBU",
        "9jBrhY101ZR1",
        "xKXUc_383Xqr",
        "EnimIe173UB5"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQ_iJPPKh7zX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af6c3c63-8d30-4093-ecc9-765a157d5969"
      },
      "source": [
        "import h5py\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import numpy\n",
        "import statistics\n",
        "from numpy import loadtxt\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas\n",
        "import math\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from statistics import stdev\n",
        "import math\n",
        "import h5py\n",
        "\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "from scipy.signal import butter,filtfilt\n",
        "import sys\n",
        "import numpy as np # linear algebra\n",
        "from scipy.stats import randint\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv), data manipulation as in SQL\n",
        "import matplotlib.pyplot as plt # this is used for the plot the graph\n",
        "import seaborn as sns # used for plot interactive graph.\n",
        "import pandas\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# from tsf.model import TransformerForecaster\n",
        "\n",
        "\n",
        "# from tensorflow.keras.utils import np_utils\n",
        "import itertools\n",
        "###  Library for attention layers\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "#from tqdm import tqdm # Processing time measurement\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import statistics\n",
        "import gc\n",
        "import torch.nn.init as init\n",
        "\n",
        "############################################################################################################################################################################\n",
        "############################################################################################################################################################################\n",
        "\n",
        "import os\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.utils.weight_norm as weight_norm\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "from torchsummary import summary\n",
        "from torch.nn.parameter import Parameter\n",
        "\n",
        "from torchsummary import summary\n",
        "\n",
        "\n",
        "\n",
        "import torch.optim as optim\n",
        "import gc\n",
        "\n",
        "from tqdm import tqdm_notebook\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W17jUoA2pwjm"
      },
      "source": [
        "# Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k1qJ-_XVjY3z"
      },
      "outputs": [],
      "source": [
        "##IMU, Kin-Kinematics, HoF-Histogram of Features\n",
        "\n",
        "def data_loader_WR(subject):\n",
        "  with h5py.File('/content/drive/My Drive/Kinematics_data_10 Subjects/All_subjects_data_kinematics.h5', 'r') as hf:\n",
        "    All_subjects = hf['All_subjects']\n",
        "    Subject = All_subjects[subject]\n",
        "\n",
        "    HOF=Subject['hof']\n",
        "    IMU_KIN=Subject['IMU_Kin']\n",
        "\n",
        "    treadmill_hof = HOF['Treadmill']\n",
        "    levelground_hof = HOF['Levelground']\n",
        "    slope_hof = HOF['Slope']\n",
        "    stair_hof = HOF['Stair']\n",
        "    round_hof = HOF['Round']\n",
        "    obstacles_hof = HOF['Obstacles']\n",
        "\n",
        "    treadmill_IMU_kin = IMU_KIN['Treadmill']\n",
        "    levelground_IMU_kin = IMU_KIN['Levelground']\n",
        "    slope_IMU_kin = IMU_KIN['Slope']\n",
        "    stair_IMU_kin = IMU_KIN['Stair']\n",
        "    round_IMU_kin= IMU_KIN['Round']\n",
        "    obstacles_IMU_kin = IMU_KIN['Obstacles']\n",
        "\n",
        "\n",
        "    hof_data=np.concatenate((treadmill_hof,levelground_hof,levelground_hof,levelground_hof,slope_hof,slope_hof,slope_hof,stair_hof),axis=0)\n",
        "    IMU_kin_data=np.concatenate((treadmill_IMU_kin,levelground_IMU_kin,slope_IMU_kin,stair_IMU_kin,round_IMU_kin,obstacles_IMU_kin),axis=0)\n",
        "\n",
        "    return np.array(hof_data), np.array(IMU_kin_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##IMU, Kin-Kinematics, HoF-Histogram of Features\n",
        "\n",
        "def data_loader(subject):\n",
        "  with h5py.File('/content/drive/My Drive/Kinematics_data_10 Subjects/All_subjects_data_kinematics.h5', 'r') as hf:\n",
        "    All_subjects = hf['All_subjects']\n",
        "    Subject = All_subjects[subject]\n",
        "\n",
        "    HOF=Subject['hof']\n",
        "    IMU_KIN=Subject['IMU_Kin']\n",
        "\n",
        "    treadmill_hof = HOF['Treadmill']\n",
        "    levelground_hof = HOF['Levelground']\n",
        "    slope_hof = HOF['Slope']\n",
        "    stair_hof = HOF['Stair']\n",
        "    round_hof = HOF['Round']\n",
        "    obstacles_hof = HOF['Obstacles']\n",
        "\n",
        "    treadmill_IMU_kin = IMU_KIN['Treadmill']\n",
        "    levelground_IMU_kin = IMU_KIN['Levelground']\n",
        "    slope_IMU_kin = IMU_KIN['Slope']\n",
        "    stair_IMU_kin = IMU_KIN['Stair']\n",
        "    round_IMU_kin= IMU_KIN['Round']\n",
        "    obstacles_IMU_kin = IMU_KIN['Obstacles']\n",
        "\n",
        "\n",
        "    hof_data=np.concatenate((treadmill_hof,levelground_hof,levelground_hof,levelground_hof,slope_hof,slope_hof,slope_hof,stair_hof,stair_hof,stair_hof,stair_hof),axis=0)\n",
        "    IMU_kin_data=np.concatenate((treadmill_IMU_kin,levelground_IMU_kin,levelground_IMU_kin,levelground_IMU_kin,slope_IMU_kin,slope_IMU_kin,slope_IMU_kin,\\\n",
        "                                 stair_IMU_kin,stair_IMU_kin,stair_IMU_kin,stair_IMU_kin),axis=0)\n",
        "\n",
        "    return np.array(hof_data), np.array(IMU_kin_data)\n"
      ],
      "metadata": {
        "id": "DxBKwycDrD0g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "53dWXPgXkt4w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37e2e7a2-1a1c-4286-f2dc-3e19b11fbd87"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "subject_1_data_hof, subject_1_data_IMU_Kin=data_loader_WR('Subject_1')\n",
        "gc.collect()\n",
        "subject_2_data_hof, subject_2_data_IMU_Kin=data_loader_WR('Subject_2')\n",
        "gc.collect()\n",
        "subject_3_data_hof, subject_3_data_IMU_Kin=data_loader_WR('Subject_3')\n",
        "gc.collect()\n",
        "subject_4_data_hof, subject_4_data_IMU_Kin=data_loader_WR('Subject_4')\n",
        "gc.collect()\n",
        "subject_5_data_hof, subject_5_data_IMU_Kin=data_loader_WR('Subject_5')\n",
        "gc.collect()\n",
        "subject_6_data_hof, subject_6_data_IMU_Kin=data_loader_WR('Subject_6')\n",
        "gc.collect()\n",
        "subject_7_data_hof, subject_7_data_IMU_Kin=data_loader_WR('Subject_7')\n",
        "gc.collect()\n",
        "subject_8_data_hof, subject_8_data_IMU_Kin=data_loader_WR('Subject_8')\n",
        "gc.collect()\n",
        "subject_9_data_hof, subject_9_data_IMU_Kin=data_loader_WR('Subject_9')\n",
        "gc.collect()\n",
        "subject_10_data_hof, subject_10_data_IMU_Kin=data_loader_WR('Subject_10')\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8A4Eoxgop1qX"
      },
      "source": [
        "# Subject Selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aUDPyqjRoDoZ"
      },
      "outputs": [],
      "source": [
        "main_dir = \"/content/drive/My Drive/public dataset/Public_dataset_2/Subject01\"\n",
        "# os.mkdir(main_dir)\n",
        "path=\"/content/\"\n",
        "subject='Subject_01'\n",
        "encoder='lstm'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8iMx41lxp4Jp"
      },
      "outputs": [],
      "source": [
        "train_data_hof=np.concatenate((subject_1_data_hof,subject_2_data_hof,subject_3_data_hof,subject_4_data_hof,subject_5_data_hof,\n",
        "                               subject_6_data_hof,subject_8_data_hof,subject_9_data_hof,subject_10_data_hof),axis=0)\n",
        "\n",
        "train_data_IMU_Kin=np.concatenate((subject_1_data_IMU_Kin,subject_2_data_IMU_Kin,subject_3_data_IMU_Kin,subject_4_data_IMU_Kin,subject_5_data_IMU_Kin,\n",
        "                               subject_6_data_IMU_Kin,subject_8_data_IMU_Kin,subject_9_data_IMU_Kin,subject_10_data_IMU_Kin),axis=0)\n",
        "\n",
        "\n",
        "test_data_hof=subject_7_data_hof\n",
        "test_data_IMU_Kin=subject_7_data_IMU_Kin"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "alpha=0.50\n",
        "num_epoch=30"
      ],
      "metadata": {
        "id": "NO0vq95nE0gU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Processing"
      ],
      "metadata": {
        "id": "dbbpf3z7eCAf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MGYu4d7pqFe3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8da0c2ed-6203-4600-fb43-30d2f523f54d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1595595, 48)\n",
            "(846983, 36)\n",
            "(1595595, 6)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "##### IMUs-0:48\n",
        "# Sensor 1- Sternum\n",
        "# Sensor 2-Sacrum\n",
        "# Sensor 3-R_thigh\n",
        "# Sensor 4-L_thigh\n",
        "# Sensor 5-R_shank\n",
        "# Sensor 6-L_shank\n",
        "# Sensor 7-R_dorsal\n",
        "# Sensor 8-L_dorsal\n",
        "\n",
        "train_dataset_IMU=train_data_IMU_Kin[:,0:48]\n",
        "train_dataset_hof=train_data_hof\n",
        "train_dataset_target=np.concatenate((train_data_IMU_Kin[:,55:56],train_data_IMU_Kin[:,58:60],train_data_IMU_Kin[:,62:63],train_data_IMU_Kin[:,65:67]),axis=1) ## Left and right leg hip, knee,ankle angle\n",
        "\n",
        "\n",
        "test_dataset_IMU=test_data_IMU_Kin[:,0:48]\n",
        "test_dataset_hof=test_data_hof\n",
        "test_dataset_target=np.concatenate((test_data_IMU_Kin[:,55:56],test_data_IMU_Kin[:,58:60],test_data_IMU_Kin[:,62:63],test_data_IMU_Kin[:,65:67]),axis=1)\n",
        "\n",
        "print(train_dataset_IMU.shape)\n",
        "print(train_dataset_hof.shape)\n",
        "print(train_dataset_target.shape)\n",
        "\n",
        "\n",
        "gc.collect()\n",
        "gc.collect()\n",
        "gc.collect()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRf9hcyFgZZ1"
      },
      "source": [
        "# Data creation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MF05NJYvUfuB"
      },
      "source": [
        "## Creating a dataset with overlapping window of 100 samples with overlap of 50 samples ##\n",
        "\n",
        "# # convert an array of values into a dataset matrix\n",
        "def create_dataset_present(dataset_1, window=100):\n",
        "  dataX= []\n",
        "  k=0\n",
        "  shift=50\n",
        "  for i in range(int(len(dataset_1)/shift)-1):\n",
        "    j=shift*k\n",
        "    a = dataset_1[j:j+window,:]\n",
        "    dataX.append(a)\n",
        "    k=k+1\n",
        "  return np.array(dataX)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNc_iwMpXkL4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0260b6a-675c-4ddc-e1e3-e345130a6759"
      },
      "source": [
        "import gc\n",
        "gc.collect()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PWYwyhYhVImc"
      },
      "source": [
        "### Reconstruction/Present Dataset ###\n",
        "w=100\n",
        "\n",
        "train_X_3=create_dataset_present(train_dataset_IMU,w)\n",
        "train_y_3=create_dataset_present(train_dataset_target,w)\n",
        "\n",
        "test_X_1D=create_dataset_present(test_dataset_IMU,w)\n",
        "test_y=create_dataset_present(test_dataset_target,w)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7R8lC06V537"
      },
      "source": [
        "train_y_3=train_y_3.reshape(train_y_3.shape[0],w,6)\n",
        "test_y=test_y.reshape(test_y.shape[0],w,6)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pUXjTNPG2LT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d35c7ea5-e3fe-4d7a-e518-807cb61563a7"
      },
      "source": [
        "train_X_1D, X_validation_1D, train_y_5, Y_validation = train_test_split(train_X_3,train_y_3, test_size=0.20, random_state=True)\n",
        "\n",
        "print(train_X_1D.shape,train_y_5.shape,X_validation_1D.shape,Y_validation.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(25528, 100, 48) (25528, 100, 6) (6382, 100, 48) (6382, 100, 6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBEpgkovHByF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dcbba289-a94b-4fa5-b10b-4950e325147a"
      },
      "source": [
        "features=6\n",
        "\n",
        "train_X_2D=train_X_1D.reshape(train_X_1D.shape[0],train_X_1D.shape[1],features,8)\n",
        "test_X_2D=test_X_1D.reshape(test_X_1D.shape[0],test_X_1D.shape[1],features,8)\n",
        "X_validation_2D= X_validation_1D.reshape(X_validation_1D.shape[0],\n",
        "                                                   X_validation_1D.shape[1],features,8)\n",
        "\n",
        "\n",
        "print(train_X_2D.shape,test_X_2D.shape,X_validation_2D.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(25528, 100, 6, 8) (3495, 100, 6, 8) (6382, 100, 6, 8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROjfjLaBfcBU"
      },
      "source": [
        "# Different Function for models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92kuqtl9yfap"
      },
      "source": [
        "def prediction_test(yhat,test_y_up):\n",
        "\n",
        "    test_o=test_y_up\n",
        "    yhat=yhat\n",
        "\n",
        "    y_1=yhat[:,0]\n",
        "    y_2=yhat[:,1]\n",
        "    y_3=yhat[:,2]\n",
        "    y_4=yhat[:,3]\n",
        "    y_5=yhat[:,4]\n",
        "    y_6=yhat[:,5]\n",
        "\n",
        "\n",
        "    y_test_1=test_o[:,0]\n",
        "    y_test_2=test_o[:,1]\n",
        "    y_test_3=test_o[:,2]\n",
        "    y_test_4=test_o[:,3]\n",
        "    y_test_5=test_o[:,4]\n",
        "    y_test_6=test_o[:,5]\n",
        "\n",
        "\n",
        "    ###calculate RMSE\n",
        "\n",
        "    rmse_1 =np.sqrt(mean_squared_error(y_test_1,y_1))\n",
        "    rmse_2 =np.sqrt(mean_squared_error(y_test_2,y_2))\n",
        "    rmse_3 =np.sqrt(mean_squared_error(y_test_3,y_3))\n",
        "    rmse_4 =np.sqrt(mean_squared_error(y_test_4,y_4))\n",
        "    rmse_5 =np.sqrt(mean_squared_error(y_test_5,y_5))\n",
        "    rmse_6 =np.sqrt(mean_squared_error(y_test_6,y_6))\n",
        "\n",
        "\n",
        "    p_1=np.corrcoef(y_1, y_test_1)[0, 1]\n",
        "    p_2=np.corrcoef(y_2, y_test_2)[0, 1]\n",
        "    p_3=np.corrcoef(y_3, y_test_3)[0, 1]\n",
        "    p_4=np.corrcoef(y_4, y_test_4)[0, 1]\n",
        "    p_5=np.corrcoef(y_5, y_test_5)[0, 1]\n",
        "    p_6=np.corrcoef(y_6, y_test_6)[0, 1]\n",
        "\n",
        "    ### Getiing single RMSE and PCC value for a joint\n",
        "    p=np.array([(p_1+p_4)/2,(p_2+p_5)/2,(p_3+p_6)/2])\n",
        "\n",
        "    rmse=np.array([(rmse_1+rmse_4)/2,(rmse_2+rmse_5)/2,(rmse_3+rmse_6)/2])\n",
        "\n",
        "\n",
        "\n",
        "    return rmse,p"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WAW6trUjyt8F"
      },
      "source": [
        "## Evaluting data with the original form without any overlaps\n",
        "\n",
        "# # convert an array of values into a dataset matrix\n",
        "def unpack_dataset_present(dataset_1):\n",
        "  dataX= []\n",
        "  k=1\n",
        "  l=0\n",
        "  shift=100\n",
        "  for i in range(int(len(dataset_1)/shift)-1):\n",
        "    j=shift*k\n",
        "    a = dataset_1[l:j,:]\n",
        "    l=0\n",
        "    l=j+50\n",
        "    dataX=np.append(dataX,a)\n",
        "    k=k+1\n",
        "    j=0\n",
        "  return np.array(dataX)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preparation"
      ],
      "metadata": {
        "id": "9jBrhY101ZR1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gjRHNe1ckRoy"
      },
      "outputs": [],
      "source": [
        "### IMUs- Chest, Waist, Right Foot, Right shank, Right thigh, Left Foot, Left shank, Left thigh, 2D-body coordinate\n",
        "### 0:48- IMU, 48:92-2D body coordinate, 92:97-- Target\n",
        "\n",
        "\n",
        "### Data Processing\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "train_features_1D = torch.Tensor(train_X_1D)\n",
        "val_features_1D = torch.Tensor(X_validation_1D)\n",
        "test_features_1D = torch.Tensor(test_X_1D)\n",
        "\n",
        "train_features_2D = torch.Tensor(train_X_2D)\n",
        "val_features_2D = torch.Tensor(X_validation_2D)\n",
        "test_features_2D = torch.Tensor(test_X_2D)\n",
        "\n",
        "train_targets = torch.Tensor(train_y_5)\n",
        "val_targets = torch.Tensor(Y_validation)\n",
        "test_targets = torch.Tensor(test_y)\n",
        "\n",
        "## all Modality Features\n",
        "train = TensorDataset(train_features_1D, train_features_2D, train_targets)\n",
        "val = TensorDataset(val_features_1D, val_features_2D,val_targets)\n",
        "test = TensorDataset(test_features_1D,test_features_2D,test_targets)\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train, batch_size=batch_size, shuffle=True, drop_last=False)\n",
        "val_loader = DataLoader(val, batch_size=batch_size, shuffle=True, drop_last=False)\n",
        "test_loader = DataLoader(test, batch_size=batch_size, shuffle=False, drop_last=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Important Functions"
      ],
      "metadata": {
        "id": "xKXUc_383Xqr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RMSELoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(RMSELoss, self).__init__()\n",
        "\n",
        "    def forward(self, pred, target):\n",
        "        mse = nn.MSELoss()(pred, target)\n",
        "        rmse = torch.sqrt(mse)\n",
        "        return rmse"
      ],
      "metadata": {
        "id": "YhQKyL6ph4hD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class correlation_coefficient_loss_joint_pytorch(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(correlation_coefficient_loss_joint_pytorch, self).__init__()\n",
        "\n",
        "  def forward(self, y_true, y_pred):\n",
        "\n",
        "    # Calculate mean values\n",
        "    mx = torch.mean(y_true)\n",
        "    my = torch.mean(y_pred)\n",
        "\n",
        "    # Calculate differences from mean\n",
        "    xm, ym = y_true - mx, y_pred - my\n",
        "\n",
        "    # Calculate numerator and denominator of Pearson correlation coefficient\n",
        "    r_num = torch.sum(torch.mul(xm, ym))\n",
        "    r_den = torch.sqrt(torch.mul(torch.sum(torch.square(xm)), torch.sum(torch.square(ym))))\n",
        "\n",
        "    # Calculate Pearson correlation coefficient\n",
        "    r = r_num / r_den\n",
        "\n",
        "    # Clamp r between -1 and 1\n",
        "    r = torch.clamp(r, -1.0, 1.0)\n",
        "\n",
        "    # Calculate l2 loss\n",
        "    l2 = 1 - torch.square(r)\n",
        "\n",
        "    # Calculate l1 loss\n",
        "    l1 = torch.sqrt(F.mse_loss(y_pred, y_true))\n",
        "\n",
        "    return l1 + l2"
      ],
      "metadata": {
        "id": "GYu5VDjK-qYL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s5Q-cJ2tK8Vb"
      },
      "outputs": [],
      "source": [
        "# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
        "is_cuda = torch.cuda.is_available()\n",
        "\n",
        "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
        "if is_cuda:\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Loop Function"
      ],
      "metadata": {
        "id": "h79K77Op1d3_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_kinematics(train_loader, learn_rate, EPOCHS, model,filename,k_1,k_2,k_3,k_4):\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "      model.cuda()\n",
        "    # Defining loss function and optimizer\n",
        "    criterion =RMSELoss()\n",
        "    # criterion =correlation_coefficient_loss_joint_pytorch()\n",
        "\n",
        "    # criterion=PearsonCorrLoss()\n",
        "    # optimizer = torch.optim.Adam(model.parameters(), lr=learn_rate)\n",
        "    optimizer_1 = torch.optim.Adam(model.model_1.parameters(), lr=learn_rate)\n",
        "    optimizer_2 = torch.optim.Adam(model.cnn_1D.parameters(), lr=learn_rate)\n",
        "    optimizer_3 = torch.optim.Adam(model.cnn_2D.parameters(), lr=learn_rate)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters())\n",
        "\n",
        "\n",
        "    running_loss=0\n",
        "    # Train the model\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Train the model with early stopping\n",
        "    best_val_loss = float('inf')\n",
        "    patience = 10\n",
        "\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        epoch_start_time = time.time()\n",
        "\n",
        "        # model.model_1.train()\n",
        "        # model.cnn_1D.train()\n",
        "        # model.cnn_2D.train()\n",
        "        model.train()\n",
        "\n",
        "        for i, (data_features_1D, data_features_2D, data_targets) in enumerate(train_loader):\n",
        "\n",
        "\n",
        "# ###################################################################################################################################\n",
        "\n",
        "            # optimizer_1.zero_grad()\n",
        "\n",
        "            # output_1, output_2, output_3, output= model(data_features_1D[:,:,k_1:k_2].to(device).float(),data_features_2D[:,:,:,k_3:k_4].to(device).float())\n",
        "            # loss_1=criterion(output_1, data_targets.to(device).float())\n",
        "\n",
        "            # loss_1.backward()\n",
        "            # optimizer_1.step()\n",
        "\n",
        "# ###################################################################################################################################\n",
        "\n",
        "            # optimizer_2.zero_grad()\n",
        "\n",
        "            # output_1, output_2, output_3, output= model(data_features_1D[:,:,k_1:k_2].to(device).float(),data_features_2D[:,:,:,k_3:k_4].to(device).float())\n",
        "            # loss_2=criterion(output_2, data_targets.to(device).float())\n",
        "\n",
        "            # loss_2.backward()\n",
        "            # optimizer_2.step()\n",
        "\n",
        "# ###################################################################################################################################\n",
        "\n",
        "\n",
        "            # optimizer_3.zero_grad()\n",
        "\n",
        "            # output_1, output_2, output_3, output= model(data_features_1D[:,:,k_1:k_2].to(device).float(),data_features_2D[:,:,:,k_3:k_4].to(device).float())\n",
        "            # loss_3=criterion(output_3, data_targets.to(device).float())\n",
        "\n",
        "            # loss_3.backward()\n",
        "            # optimizer_3.step()\n",
        "\n",
        "# ###################################################################################################################################\n",
        "\n",
        "\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            output_1, output_2, output_3, output= model(data_features_1D[:,:,k_1:k_2].to(device).float(),data_features_2D[:,:,:,k_3:k_4].to(device).float())\n",
        "            loss=criterion(output, data_targets.to(device).float())\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "\n",
        "###################################################################################################################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            # Compute the regularization loss for the custom linear layers\n",
        "            # regularization_loss = 0.0\n",
        "            # if hasattr(model.output_GRU, 'regularizer_loss'):\n",
        "            #     regularization_loss += model.output_GRU.regularizer_loss()\n",
        "            # if hasattr(model.output_C1, 'regularizer_loss'):\n",
        "            #     regularization_loss += model.output_C1.regularizer_loss()\n",
        "            # if hasattr(model.output_C2, 'regularizer_loss'):\n",
        "            #     regularization_loss += model.output_C2.regularizer_loss()\n",
        "\n",
        "            # loss=criterion(output_1, data_targets.to(device).float())+criterion(output_2, data_targets.to(device).float())\\\n",
        "            # +criterion(output_3, data_targets.to(device).float())+criterion(output, data_targets.to(device).float())\n",
        "\n",
        "            loss_1=criterion(output, data_targets.to(device).float())\n",
        "\n",
        "            # loss.backward()\n",
        "            # optimizer.step()\n",
        "\n",
        "            running_loss += loss_1.item()\n",
        "\n",
        "        train_loss=running_loss/len(train_loader)\n",
        "\n",
        "       # Validate\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for data_features_1D, data_features_2D, data_targets in val_loader:\n",
        "                output_1, output_2, output_3, output= model(data_features_1D[:,:,k_1:k_2].to(device).float(),data_features_2D[:,:,:,k_3:k_4].to(device).float())\n",
        "                val_loss += criterion(output, data_targets.to(device).float())\n",
        "\n",
        "        val_loss /= len(val_loader)\n",
        "\n",
        "        epoch_end_time = time.time()\n",
        "        epoch_training_time = epoch_end_time - epoch_start_time\n",
        "\n",
        "        print(f\"Epoch: {epoch+1}, time: {epoch_training_time:.4f}, Training Loss: {train_loss:.4f},  Validation loss: {val_loss:.4f}\")\n",
        "\n",
        "        running_loss=0\n",
        "\n",
        "        epoch_end_time = time.time()\n",
        "\n",
        "                # Check if the validation loss has improved\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            torch.save(model.state_dict(), filename)\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        # Early stopping if the validation loss hasn't improved for `patience` epochs\n",
        "        if patience_counter >= patience:\n",
        "            print(f\"Stopping early after {epoch+1} epochs\")\n",
        "            break\n",
        "\n",
        "\n",
        "\n",
        "    end_time = time.time()\n",
        "\n",
        "    training_time = end_time - start_time\n",
        "    print(f\"Training time: {training_time} seconds\")\n",
        "\n",
        "\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "a2wiLJ-0RyzC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "EnimIe173UB5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# class RegularizedLinear(nn.Module):\n",
        "#     def __init__(self, in_features, out_features, weight_decay=0.001):\n",
        "#         super(RegularizedLinear, self).__init__()\n",
        "#         self.linear = nn.Linear(in_features, out_features)\n",
        "#         self.weight_decay = weight_decay\n",
        "\n",
        "#     def forward(self, input):\n",
        "#         return self.linear(input)\n",
        "\n",
        "#     def regularizer_loss(self):\n",
        "#         return self.weight_decay * torch.sum(self.linear.bias**2)\n"
      ],
      "metadata": {
        "id": "qYIjEEsn9BrX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder_GRU(nn.Module):\n",
        "    def __init__(self, input_shape_1D, input_shape_2D, dropout):\n",
        "        super(Encoder_GRU, self).__init__()\n",
        "        self.lstm_1 = nn.GRU(input_shape_1D, 128, bidirectional=True, batch_first=True, dropout=0.0)\n",
        "        self.lstm_2 = nn.GRU(256, 64, bidirectional=True, batch_first=True, dropout=0.0)\n",
        "        self.flatten=nn.Flatten()\n",
        "        self.fc = nn.Linear(128, 32)\n",
        "        self.dropout_1=nn.Dropout(dropout)\n",
        "        self.dropout_2=nn.Dropout(dropout)\n",
        "        self.output_GRU = nn.Linear(w*128, 6 * w)\n",
        "\n",
        "        self.BN_1D= nn.BatchNorm1d(input_shape_1D, affine=False)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        out_1, _ = self.lstm_1(x)\n",
        "        out_1=self.dropout_1(out_1)\n",
        "        out_2, _ = self.lstm_2(out_1)\n",
        "        out_2=self.dropout_2(out_2)\n",
        "        out_2=self.flatten(out_2)\n",
        "\n",
        "        output_GRU=self.output_GRU(out_2).view(-1,w,6)\n",
        "\n",
        "        return output_GRU\n"
      ],
      "metadata": {
        "id": "cAa3QerVhHpJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder_GRU_cnn(nn.Module):\n",
        "    def __init__(self, input_dim, dropout):\n",
        "        super(Encoder_GRU_cnn, self).__init__()\n",
        "        self.lstm_1 = nn.GRU(input_dim, 128, bidirectional=True, batch_first=True, dropout=0.0)\n",
        "        self.lstm_2 = nn.GRU(256, 64, bidirectional=True, batch_first=True, dropout=0.0)\n",
        "        self.flatten=nn.Flatten()\n",
        "        self.fc = nn.Linear(128, 32)\n",
        "        self.dropout_1=nn.Dropout(dropout)\n",
        "        self.dropout_2=nn.Dropout(dropout)\n",
        "        self.output_GRU = nn.Linear(w*128, 6 * w)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        out_1, _ = self.lstm_1(x)\n",
        "        out_1=self.dropout_1(out_1)\n",
        "        out_2, _ = self.lstm_2(out_1)\n",
        "        out_2=self.dropout_2(out_2)\n",
        "        out_2=self.flatten(out_2)\n",
        "\n",
        "        return out_2\n"
      ],
      "metadata": {
        "id": "904lRi-9AA0q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder_CNN_1D(nn.Module):\n",
        "    def __init__(self, input_shape_1D, input_shape_2D, dropout, hidden_dim=64, output_size=128, kernel_size=3, stride=1, padding=1):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv1d(input_shape_1D, hidden_dim, kernel_size, stride, padding)\n",
        "        self.conv2 = nn.Conv1d(hidden_dim, hidden_dim, kernel_size, stride, padding)\n",
        "        self.conv3 = nn.Conv1d(hidden_dim, output_size, kernel_size, stride, padding)\n",
        "        self.conv4 = nn.Conv1d(output_size, output_size, kernel_size, stride, padding)\n",
        "        self.BN_2= nn.BatchNorm1d(hidden_dim)\n",
        "        self.BN_4= nn.BatchNorm1d(output_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.pool = nn.MaxPool1d(kernel_size=2)\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.fc1 = nn.Linear(128, 64)\n",
        "        self.dropout1 = nn.Dropout(0.2)\n",
        "        self.fc2 = nn.Linear(64, 32)\n",
        "        self.dropout2 = nn.Dropout(0.2)\n",
        "        self.flatten=nn.Flatten()\n",
        "\n",
        "        self.BN_1D= nn.BatchNorm1d(input_shape_1D, affine=False)\n",
        "        self.BN_2D= nn.BatchNorm2d(input_shape_2D, affine=False)\n",
        "\n",
        "        self.model_2=Encoder_GRU_cnn(input_shape_1D,0.40)\n",
        "\n",
        "        self.output_C2 = nn.Linear(25*32+w*128, 6 * w)\n",
        "\n",
        "    def forward(self, x, x_1):\n",
        "\n",
        "        input_1D_N_1=x.transpose(1,2)\n",
        "        input_1D_N_1=self.BN_1D(input_1D_N_1)\n",
        "        input_1D_N=input_1D_N_1.transpose(1,2)\n",
        "\n",
        "        # input_1D_N_1=x_1.transpose(1,2)\n",
        "        # input_1D_N_1=self.BN_1D(input_1D_N_1)\n",
        "        # input_1D_N=input_1D_N_1.transpose(1,2)\n",
        "\n",
        "\n",
        "\n",
        "        x = input_1D_N.transpose(1, 2)  # reshape from (batch_size, seq_len, input_size) to (batch_size, input_size, seq_len)\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.BN_2(x)\n",
        "        x = self.pool(x)\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = F.relu(self.conv4(x))\n",
        "        x = self.BN_4(x)\n",
        "        x = self.pool(x)\n",
        "        x = x.transpose(1, 2)  # reshape back to (batch_size, seq_len, output_size)\n",
        "\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout1(x)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.dropout2(x)\n",
        "        x = self.flatten(x)\n",
        "\n",
        "        model_2_output = self.model_2(input_1D_N)\n",
        "        model_2_output_1 = torch.cat([model_2_output, x], dim=-1)\n",
        "\n",
        "        output_C2=self.output_C2(model_2_output_1).view(-1,w,6)\n",
        "\n",
        "        return output_C2"
      ],
      "metadata": {
        "id": "4iK0oA10iA_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder_CNN_2D(nn.Module):\n",
        "    def __init__(self, input_shape_1D,input_shape_2D, dropout, hidden_dim=64, output_size=128, kernel_size=(3,3), stride=(1,1), padding=(1,1)):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(input_shape_2D, hidden_dim, kernel_size, stride, padding)\n",
        "        self.conv2 = nn.Conv2d(hidden_dim, output_size, kernel_size, stride, padding)\n",
        "        self.BN_2= nn.BatchNorm2d(hidden_dim)\n",
        "        self.BN_4= nn.BatchNorm2d(output_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=(2,2))\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.fc1 = nn.Linear(128, 64)\n",
        "        self.dropout1 = nn.Dropout(0.2)\n",
        "        self.fc2 = nn.Linear(64, 32)\n",
        "        self.dropout2 = nn.Dropout(0.2)\n",
        "        self.flatten=nn.Flatten()\n",
        "\n",
        "        self.BN_1D= nn.BatchNorm1d(input_shape_1D, affine=False)\n",
        "        self.BN_2D= nn.BatchNorm2d(input_shape_2D, affine=False)\n",
        "\n",
        "        self.model_3=Encoder_GRU_cnn(input_shape_1D,0.40)\n",
        "\n",
        "        self.output_C2 = nn.Linear(25*32+w*128, 6 * w)\n",
        "\n",
        "    def forward(self, x_1,x):\n",
        "\n",
        "        input_1D_N_1=x_1.transpose(1,2)\n",
        "        input_1D_N_1=self.BN_1D(input_1D_N_1)\n",
        "        input_1D_N=input_1D_N_1.transpose(1,2)\n",
        "\n",
        "        inputs_2D_N=x.transpose(1,3)\n",
        "        inputs_2D_N=self.BN_2D(inputs_2D_N)\n",
        "        inputs_2D_N=inputs_2D_N.transpose(1,3)\n",
        "\n",
        "        x = inputs_2D_N.transpose(1, 3)  # reshape from (batch_size, seq_len, input_size) to (batch_size, input_size, seq_len)\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.BN_2(x)\n",
        "        x = self.pool(x)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.BN_4(x)\n",
        "        x = self.pool(x)\n",
        "        x = x.transpose(1, 3)  # reshape back to (batch_size, seq_len, output_size)\n",
        "\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout1(x)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.dropout2(x)\n",
        "        x = self.flatten(x)\n",
        "\n",
        "        model_3_output = self.model_3(input_1D_N)\n",
        "        model_3_output = torch.cat([model_3_output, x], dim=-1)\n",
        "        # output_C2 = self.output_C2(model_3_output)\n",
        "\n",
        "        output_C1=self.output_C2(model_3_output).view(-1,w,6)\n",
        "\n",
        "        return output_C1"
      ],
      "metadata": {
        "id": "6K8RJJ2r0sC6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Teacher Model"
      ],
      "metadata": {
        "id": "MbISy0StPDZn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class teacher(nn.Module):\n",
        "    def __init__(self, input_shape_1D,input_shape_2D, w):\n",
        "        super(teacher, self).__init__()\n",
        "        self.w = w\n",
        "\n",
        "        # 1D Models\n",
        "\n",
        "        self.model_1=Encoder_GRU(input_shape_1D,input_shape_2D,0.40)\n",
        "        self.cnn_1D = Encoder_CNN_1D(input_shape_1D,input_shape_2D,0.40)\n",
        "        self.cnn_2D = Encoder_CNN_2D(input_shape_1D,input_shape_2D,0.40)\n",
        "\n",
        "        self.BN_1D= nn.BatchNorm1d(input_shape_1D, affine=False)\n",
        "        self.BN_2D= nn.BatchNorm2d(input_shape_2D, affine=False)\n",
        "\n",
        "\n",
        "    def forward(self, inputs_1D_N, inputs_2D_N):\n",
        "\n",
        "        # input_1D_N_1=inputs_1D_N.transpose(1,2)\n",
        "        # input_1D_N_1=self.BN_1D(input_1D_N_1)\n",
        "        # input_1D_N=input_1D_N_1.transpose(1,2)\n",
        "\n",
        "        # inputs_2D_N=inputs_2D_N.transpose(1,3)\n",
        "        # inputs_2D_N=self.BN_2D(inputs_2D_N)\n",
        "        # inputs_2D_N=inputs_2D_N.transpose(1,3)\n",
        "\n",
        "        output_GRU = self.model_1(inputs_1D_N)\n",
        "        output_C2 = self.cnn_1D(inputs_1D_N,inputs_1D_N)\n",
        "        output_C1 = self.cnn_2D(inputs_1D_N,inputs_2D_N)\n",
        "\n",
        "\n",
        "        output = (output_GRU +output_C2+output_C1)/3\n",
        "\n",
        "        return output_GRU, output_C1, output_C2, output"
      ],
      "metadata": {
        "id": "KgcMZBai3S69"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k_1=0\n",
        "k_2=48\n",
        "k_3=0\n",
        "k_4=8\n",
        "\n",
        "lr = 0.001\n",
        "model = teacher(k_2-k_1,k_4-k_3,100)\n",
        "\n",
        "gait_Net_teacher = train_kinematics(train_loader, lr,num_epoch,model,path+encoder+'_teacher.pth',k_1, k_2, k_3, k_4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "xxJiNOWSYnC-",
        "outputId": "ae5252b1-1397-4f6d-d502-f2db1248932b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, time: 15.4266, Training Loss: 4.5036,  Validation loss: 3.2493\n",
            "Epoch: 2, time: 15.3665, Training Loss: 2.7068,  Validation loss: 2.5061\n",
            "Epoch: 3, time: 15.0734, Training Loss: 2.4891,  Validation loss: 2.7108\n",
            "Epoch: 4, time: 14.9395, Training Loss: 2.3720,  Validation loss: 2.3077\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-77-e3f4bc1dfd70>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mteacher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk_2\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mk_1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk_4\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mk_3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mgait_Net_teacher\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_kinematics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_teacher.pth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-70-9ec0270d9373>\u001b[0m in \u001b[0;36mtrain_kinematics\u001b[0;34m(train_loader, learn_rate, EPOCHS, model, filename, k_1, k_2, k_3, k_4)\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m             \u001b[0moutput_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_features_1D\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk_1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mk_2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata_features_2D\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk_3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mk_4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_targets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gait_Net_teacher= teacher(k_2-k_1,k_4-k_3,100)\n",
        "gait_Net_teacher.load_state_dict(torch.load(path+encoder+'_teacher.pth'))\n",
        "gait_Net_teacher.to(device)\n",
        "\n",
        "gait_Net_teacher.eval()\n",
        "\n",
        "# iterate through batches of test data\n",
        "with torch.no_grad():\n",
        "    for i, (data_features_1D, data_features_2D, data_targets) in enumerate(test_loader):\n",
        "        output_1,output_2,output_3,output = gait_Net_teacher(data_features_1D[:,:,k_1:k_2].to(device).float(),data_features_2D[:,:,:,k_3:k_4].to(device).float())\n",
        "        if i==0:\n",
        "          yhat_5=output\n",
        "          test_target=data_targets\n",
        "\n",
        "        else:\n",
        "          yhat_5=torch.cat((yhat_5,output),dim=0)\n",
        "          test_target=torch.cat((test_target,data_targets),dim=0)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "yhat_4 = yhat_5.detach().cpu().numpy()\n",
        "test_y = test_target.detach().cpu().numpy()\n",
        "print(yhat_4.shape)\n",
        "\n",
        " ### Present ###\n",
        "yhat_5=yhat_4.reshape((yhat_4.shape[0]*w,6))\n",
        "test_y_r=test_y.reshape((test_y.shape[0]*w,6))\n",
        "\n",
        "print(yhat_4.shape)\n",
        "\n",
        "### Unpack ###\n",
        "yhat_up=unpack_dataset_present(np.array(yhat_5))\n",
        "test_y_up=unpack_dataset_present(np.array(test_y_r))\n",
        "\n",
        "print(yhat_up.shape,test_y_up.shape)\n",
        "\n",
        "### Present ###\n",
        "\n",
        "yhat_up=yhat_up.reshape(int(len(yhat_up)/6),6)\n",
        "test_y_up=test_y_up.reshape(int(len(test_y_up)/6),6)\n",
        "\n",
        "print(yhat_up.shape,test_y_up.shape)\n",
        "\n",
        "print(yhat_up.shape)\n",
        "\n",
        "\n",
        "### Present ###\n",
        "\n",
        "rmse,p= prediction_test(np.array(yhat_up),np.array(test_y_up))\n",
        "\n",
        "print(rmse[0])\n",
        "print(rmse[1])\n",
        "print(rmse[2])\n",
        "\n",
        "m=np.mean(rmse)\n",
        "\n",
        "print('\\n')\n",
        "print(m)\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "print(p[0])\n",
        "print(p[1])\n",
        "print(p[2])\n",
        "print('\\n')\n",
        "\n",
        "print(np.mean(p))\n",
        "\n",
        "ablation_1=np.hstack([rmse,p])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bk43RyYm9Qcb",
        "outputId": "4af13ea0-0b13-4dcb-ac85-a71999d600ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3495, 100, 6)\n",
            "(3495, 100, 6)\n",
            "(1048500,) (1048500,)\n",
            "(174750, 6) (174750, 6)\n",
            "(174750, 6)\n",
            "5.121346129264148\n",
            "3.9807326717106615\n",
            "3.6319999478995566\n",
            "\n",
            "\n",
            "4.244692916291455\n",
            "\n",
            "\n",
            "0.9657379606576907\n",
            "0.9871729013950058\n",
            "0.9427772321085144\n",
            "\n",
            "\n",
            "0.9652293647204037\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Student Model"
      ],
      "metadata": {
        "id": "Bdwh0N10PFV1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class student(nn.Module):\n",
        "    def __init__(self, input_shape_1D,input_shape_2D, w):\n",
        "        super(student, self).__init__()\n",
        "        self.w = w\n",
        "\n",
        "        # 1D Models\n",
        "\n",
        "        self.model_1=Encoder_GRU(input_shape_1D,input_shape_2D,0.40)\n",
        "        self.cnn_1D = Encoder_CNN_1D(input_shape_1D,input_shape_2D,0.40)\n",
        "        self.cnn_2D = Encoder_CNN_2D(input_shape_1D,input_shape_2D,0.40)\n",
        "\n",
        "        self.BN_1D= nn.BatchNorm1d(input_shape_1D, affine=False)\n",
        "        self.BN_2D= nn.BatchNorm2d(input_shape_2D, affine=False)\n",
        "\n",
        "\n",
        "    def forward(self, inputs_1D_N, inputs_2D_N):\n",
        "\n",
        "        # input_1D_N_1=inputs_1D_N.transpose(1,2)\n",
        "        # input_1D_N_1=self.BN_1D(input_1D_N_1)\n",
        "        # input_1D_N=input_1D_N_1.transpose(1,2)\n",
        "\n",
        "        # inputs_2D_N=inputs_2D_N.transpose(1,3)\n",
        "        # inputs_2D_N=self.BN_2D(inputs_2D_N)\n",
        "        # inputs_2D_N=inputs_2D_N.transpose(1,3)\n",
        "\n",
        "        output_GRU = self.model_1(inputs_1D_N)\n",
        "        output_C2 = self.cnn_1D(inputs_1D_N,inputs_1D_N)\n",
        "        output_C1 = self.cnn_2D(inputs_1D_N,inputs_2D_N)\n",
        "\n",
        "\n",
        "        output = (output_GRU +output_C2+output_C1)/3\n",
        "\n",
        "        return output_GRU, output_C1, output_C2, output"
      ],
      "metadata": {
        "id": "AjB8HTaPPFV8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k_1=36\n",
        "k_2=48\n",
        "k_3=6\n",
        "k_4=8\n",
        "\n",
        "lr = 0.001\n",
        "model = student(k_2-k_1,k_4-k_3,100)\n",
        "\n",
        "gait_Net_student = train_kinematics(train_loader, lr,num_epoch,model,path+encoder+'_student.pth',k_1, k_2, k_3, k_4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "7c95b5b8-65ec-4664-f698-a97c5c0c0e80",
        "id": "yeUePDsQPFV9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, time: 14.6632, Training Loss: 6.2742,  Validation loss: 4.6750\n",
            "Epoch: 2, time: 14.8435, Training Loss: 4.2918,  Validation loss: 3.8783\n",
            "Epoch: 3, time: 14.8381, Training Loss: 3.8508,  Validation loss: 3.6549\n",
            "Epoch: 4, time: 14.6781, Training Loss: 3.5658,  Validation loss: 3.4419\n",
            "Epoch: 5, time: 14.5090, Training Loss: 3.3882,  Validation loss: 3.3646\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-80-91b2f2f5a79a>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstudent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk_2\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mk_1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk_4\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mk_3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mgait_Net_student\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_kinematics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_student.pth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-70-9ec0270d9373>\u001b[0m in \u001b[0;36mtrain_kinematics\u001b[0;34m(train_loader, learn_rate, EPOCHS, model, filename, k_1, k_2, k_3, k_4)\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m             \u001b[0moutput_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_features_1D\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk_1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mk_2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata_features_2D\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk_3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mk_4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_targets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-79-b33ea4e51aa4>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs_1D_N, inputs_2D_N)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m# inputs_2D_N=inputs_2D_N.transpose(1,3)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0moutput_GRU\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs_1D_N\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0moutput_C2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcnn_1D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs_1D_N\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minputs_1D_N\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0moutput_C1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcnn_2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs_1D_N\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minputs_2D_N\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-72-4ff992309023>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mout_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mout_1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mout_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m   1098\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m             result = _VF.gru(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0m\u001b[1;32m   1101\u001b[0m                              self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[1;32m   1102\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gait_Net_student= student(k_2-k_1,k_4-k_3,100)\n",
        "gait_Net_student.load_state_dict(torch.load(path+encoder+'_student.pth'))\n",
        "gait_Net_student.to(device)\n",
        "\n",
        "gait_Net_student.eval()\n",
        "\n",
        "# iterate through batches of test data\n",
        "with torch.no_grad():\n",
        "    for i, (data_features_1D, data_features_2D, data_targets) in enumerate(test_loader):\n",
        "        output_1,output_2,output_3,output = gait_Net_student(data_features_1D[:,:,k_1:k_2].to(device).float(),data_features_2D[:,:,:,k_3:k_4].to(device).float())\n",
        "        if i==0:\n",
        "          yhat_5=output\n",
        "          test_target=data_targets\n",
        "\n",
        "        else:\n",
        "          yhat_5=torch.cat((yhat_5,output),dim=0)\n",
        "          test_target=torch.cat((test_target,data_targets),dim=0)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "yhat_4 = yhat_5.detach().cpu().numpy()\n",
        "test_y = test_target.detach().cpu().numpy()\n",
        "print(yhat_4.shape)\n",
        "\n",
        " ### Present ###\n",
        "yhat_5=yhat_4.reshape((yhat_4.shape[0]*w,6))\n",
        "test_y_r=test_y.reshape((test_y.shape[0]*w,6))\n",
        "\n",
        "print(yhat_4.shape)\n",
        "\n",
        "### Unpack ###\n",
        "yhat_up=unpack_dataset_present(np.array(yhat_5))\n",
        "test_y_up=unpack_dataset_present(np.array(test_y_r))\n",
        "\n",
        "print(yhat_up.shape,test_y_up.shape)\n",
        "\n",
        "### Present ###\n",
        "\n",
        "yhat_up=yhat_up.reshape(int(len(yhat_up)/6),6)\n",
        "test_y_up=test_y_up.reshape(int(len(test_y_up)/6),6)\n",
        "\n",
        "print(yhat_up.shape,test_y_up.shape)\n",
        "\n",
        "print(yhat_up.shape)\n",
        "\n",
        "\n",
        "### Present ###\n",
        "\n",
        "rmse,p= prediction_test(np.array(yhat_up),np.array(test_y_up))\n",
        "\n",
        "print(rmse[0])\n",
        "print(rmse[1])\n",
        "print(rmse[2])\n",
        "\n",
        "m=np.mean(rmse)\n",
        "\n",
        "print('\\n')\n",
        "print(m)\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "print(p[0])\n",
        "print(p[1])\n",
        "print(p[2])\n",
        "print('\\n')\n",
        "\n",
        "print(np.mean(p))\n",
        "\n",
        "ablation_2=np.hstack([rmse,p])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4d1e2b1-e0d3-4fad-b46a-9aa4889aa8bc",
        "id": "3RTst751PFV9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3495, 100, 6)\n",
            "(3495, 100, 6)\n",
            "(1048500,) (1048500,)\n",
            "(174750, 6) (174750, 6)\n",
            "(174750, 6)\n",
            "4.653032151343236\n",
            "6.302119645429729\n",
            "4.806327062916224\n",
            "\n",
            "\n",
            "5.253826286563062\n",
            "\n",
            "\n",
            "0.9696954730219682\n",
            "0.978915997112876\n",
            "0.9359312360503449\n",
            "\n",
            "\n",
            "0.9615142353950631\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sensor Distillation"
      ],
      "metadata": {
        "id": "L7ywgdr68tj4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DiO2hPE7RtPH"
      },
      "outputs": [],
      "source": [
        "def train_SD(train_loader, learn_rate, EPOCHS, student, teacher, filename, k_1s, k_2s, k_3s, k_4s, k_1t, k_2t, k_3t, k_4t):\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "      student.cuda()\n",
        "    # Defining loss function and optimizer\n",
        "    criterion =RMSELoss()\n",
        "    # criterion =correlation_coefficient_loss_joint_pytorch()\n",
        "\n",
        "    # criterion=PearsonCorrLoss()\n",
        "    optimizer = torch.optim.Adam(student.parameters(), lr=learn_rate)\n",
        "    # optimizer_1 = torch.optim.Adam(student.model_1.parameters(), lr=learn_rate)\n",
        "    # optimizer_2 = torch.optim.Adam(student.cnn_1D.parameters(), lr=learn_rate)\n",
        "    # optimizer_3 = torch.optim.Adam(student.cnn_2D.parameters(), lr=learn_rate)\n",
        "\n",
        "    # optimizer_t = torch.optim.Adam(teacher.parameters(), lr=learn_rate)\n",
        "\n",
        "    # optimizer = torch.optim.Adam(model.parameters())\n",
        "\n",
        "\n",
        "    running_loss=0\n",
        "    # Train the model\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Train the model with early stopping\n",
        "    best_val_loss = float('inf')\n",
        "    patience = 10\n",
        "\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        epoch_start_time = time.time()\n",
        "\n",
        "        # student.model_1.train()\n",
        "        # student.cnn_1D.train()\n",
        "        # student.cnn_2D.train()\n",
        "        student.train()\n",
        "\n",
        "        for i, (data_features_1D, data_features_2D, data_targets) in enumerate(train_loader):\n",
        "\n",
        "\n",
        "\n",
        "            # with torch.no_grad():\n",
        "            #  output_1t, output_2t, output_3t, outputt= teacher(data_features_1D[:,:,k_1t:k_2t].to(device).float(),data_features_2D[:,:,:,k_3t:k_4t].to(device).float())\n",
        "\n",
        "\n",
        "\n",
        "# ###################################################################################################################################\n",
        "\n",
        "            # optimizer_1.zero_grad()\n",
        "\n",
        "            # output_1, output_2, output_3, output= student(data_features_1D[:,:,k_1s:k_2s].to(device).float(),data_features_2D[:,:,:,k_3s:k_4s].to(device).float())\n",
        "\n",
        "            # loss_1=criterion(output_1, data_targets.to(device).float())+alpha*criterion(output_1, output_1t)\n",
        "\n",
        "            # loss_1.backward()\n",
        "            # optimizer_1.step()\n",
        "\n",
        "# ###################################################################################################################################\n",
        "\n",
        "            # optimizer_2.zero_grad()\n",
        "\n",
        "            # output_1, output_2, output_3, output= student(data_features_1D[:,:,k_1s:k_2s].to(device).float(),data_features_2D[:,:,:,k_3s:k_4s].to(device).float())\n",
        "\n",
        "            # # with torch.no_grad():\n",
        "            # #  output_1t, output_2t, output_3t, outputt= teacher(data_features_1D[:,:,k_1t:k_2t].to(device).float(),data_features_2D[:,:,:,k_3t:k_4t].to(device).float())\n",
        "\n",
        "            # loss_2=criterion(output_2, data_targets.to(device).float())+alpha*criterion(output_2, output_2t)\n",
        "\n",
        "            # loss_2.backward()\n",
        "            # optimizer_2.step()\n",
        "\n",
        "# ###################################################################################################################################\n",
        "\n",
        "\n",
        "            # optimizer_3.zero_grad()\n",
        "\n",
        "            # output_1, output_2, output_3, output= student(data_features_1D[:,:,k_1s:k_2s].to(device).float(),data_features_2D[:,:,:,k_3s:k_4s].to(device).float())\n",
        "\n",
        "            # # with torch.no_grad():\n",
        "            # #  output_1t, output_2t, output_3t, outputt= teacher(data_features_1D[:,:,k_1t:k_2t].to(device).float(),data_features_2D[:,:,:,k_3t:k_4t].to(device).float())\n",
        "\n",
        "            # loss_3=criterion(output_3, data_targets.to(device).float())+alpha*criterion(output_3, output_3t)\n",
        "\n",
        "            # loss_3.backward()\n",
        "            # optimizer_3.step()\n",
        "\n",
        "# ###################################################################################################################################\n",
        "\n",
        "\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            output_1, output_2, output_3, output= student(data_features_1D[:,:,k_1s:k_2s].to(device).float(),data_features_2D[:,:,:,k_3s:k_4s].to(device).float())\n",
        "\n",
        "            with torch.no_grad():\n",
        "             output_1t, output_2t, output_3t, outputt= teacher(data_features_1D[:,:,k_1t:k_2t].to(device).float(),data_features_2D[:,:,:,k_3t:k_4t].to(device).float())\n",
        "\n",
        "            loss=criterion(output, data_targets.to(device).float())+alpha*criterion(output, outputt)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "\n",
        "###################################################################################################################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            # Compute the regularization loss for the custom linear layers\n",
        "            # regularization_loss = 0.0\n",
        "            # if hasattr(model.output_GRU, 'regularizer_loss'):\n",
        "            #     regularization_loss += model.output_GRU.regularizer_loss()\n",
        "            # if hasattr(model.output_C1, 'regularizer_loss'):\n",
        "            #     regularization_loss += model.output_C1.regularizer_loss()\n",
        "            # if hasattr(model.output_C2, 'regularizer_loss'):\n",
        "            #     regularization_loss += model.output_C2.regularizer_loss()\n",
        "\n",
        "            # loss=criterion(output_1, data_targets.to(device).float())+criterion(output_2, data_targets.to(device).float())\\\n",
        "            # +criterion(output_3, data_targets.to(device).float())+criterion(output, data_targets.to(device).float())\n",
        "\n",
        "            loss_1=criterion(output, data_targets.to(device).float())\n",
        "\n",
        "            # loss.backward()\n",
        "            # optimizer.step()\n",
        "\n",
        "            running_loss += loss_1.item()\n",
        "\n",
        "        train_loss=running_loss/len(train_loader)\n",
        "\n",
        "       # Validate\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for data_features_1D, data_features_2D, data_targets in val_loader:\n",
        "                output_1, output_2, output_3, output= student(data_features_1D[:,:,k_1s:k_2s].to(device).float(),data_features_2D[:,:,:,k_3s:k_4s].to(device).float())\n",
        "                val_loss += criterion(output, data_targets.to(device).float())\n",
        "\n",
        "        val_loss /= len(val_loader)\n",
        "\n",
        "        epoch_end_time = time.time()\n",
        "        epoch_training_time = epoch_end_time - epoch_start_time\n",
        "\n",
        "        print(f\"Epoch: {epoch+1}, time: {epoch_training_time:.4f}, Training Loss: {train_loss:.4f},  Validation loss: {val_loss:.4f}\")\n",
        "\n",
        "        running_loss=0\n",
        "\n",
        "        epoch_end_time = time.time()\n",
        "\n",
        "                # Check if the validation loss has improved\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            torch.save(student.state_dict(), filename)\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        # Early stopping if the validation loss hasn't improved for `patience` epochs\n",
        "        if patience_counter >= patience:\n",
        "            print(f\"Stopping early after {epoch+1} epochs\")\n",
        "            break\n",
        "\n",
        "\n",
        "\n",
        "    end_time = time.time()\n",
        "\n",
        "    training_time = end_time - start_time\n",
        "    print(f\"Training time: {training_time} seconds\")\n",
        "\n",
        "\n",
        "\n",
        "    return student"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "k_1s=36\n",
        "k_2s=48\n",
        "k_3s=6\n",
        "k_4s=8\n",
        "\n",
        "lr = 0.001\n",
        "Student = student(k_2s-k_1s,k_4s-k_3s,100)\n",
        "\n",
        "k_1t=0\n",
        "k_2t=48\n",
        "k_3t=0\n",
        "k_4t=8\n",
        "\n",
        "lr = 0.001\n",
        "Teacher = teacher(k_2t-k_1t,k_4t-k_3t,100)\n",
        "Teacher.load_state_dict(torch.load(path+encoder+'_teacher.pth'))\n",
        "Teacher.to(device)\n",
        "\n",
        "\n",
        "\n",
        "student_KD= train_SD(train_loader, lr,num_epoch, Student,Teacher, path+'student_kd.pth', k_1s, k_2s, k_3s, k_4s, k_1t, k_2t, k_3t, k_4t)"
      ],
      "metadata": {
        "id": "VeZh5DcW6WfE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "b7687aa1-6b98-49ee-f943-1645f93e31d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, time: 18.9053, Training Loss: 6.2649,  Validation loss: 4.6685\n",
            "Epoch: 2, time: 19.2937, Training Loss: 4.2752,  Validation loss: 4.1224\n",
            "Epoch: 3, time: 19.0542, Training Loss: 3.8267,  Validation loss: 3.7421\n",
            "Epoch: 4, time: 18.8027, Training Loss: 3.5529,  Validation loss: 3.5792\n",
            "Epoch: 5, time: 18.8208, Training Loss: 3.3819,  Validation loss: 3.4723\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-83-ae1abea94bea>\u001b[0m in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mstudent_KD\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mtrain_SD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStudent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mTeacher\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'student_kd.pth'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_1s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_2s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_3s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_4s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_1t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_2t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_3t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_4t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-82-4baa5673b822>\u001b[0m in \u001b[0;36mtrain_SD\u001b[0;34m(train_loader, learn_rate, EPOCHS, student, teacher, filename, k_1s, k_2s, k_3s, k_4s, k_1t, k_2t, k_3t, k_4t)\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_targets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m             )\n\u001b[0;32m--> 522\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    523\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    267\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gait_Net_student_SD= student(k_2s-k_1s,k_4s-k_3s,100)\n",
        "gait_Net_student_SD.load_state_dict(torch.load(path+'student_kd.pth'))\n",
        "gait_Net_student_SD.to(device)\n",
        "\n",
        "gait_Net_student_SD.eval()\n",
        "\n",
        "# iterate through batches of test data\n",
        "with torch.no_grad():\n",
        "    for i, (data_features_1D, data_features_2D, data_targets) in enumerate(test_loader):\n",
        "        output_1,output_2,output_3,output = gait_Net_student_SD(data_features_1D[:,:,k_1s:k_2s].to(device).float(),data_features_2D[:,:,:,k_3s:k_4s].to(device).float())\n",
        "        if i==0:\n",
        "          yhat_5=output\n",
        "          test_target=data_targets\n",
        "\n",
        "        else:\n",
        "          yhat_5=torch.cat((yhat_5,output),dim=0)\n",
        "          test_target=torch.cat((test_target,data_targets),dim=0)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "yhat_4 = yhat_5.detach().cpu().numpy()\n",
        "test_y = test_target.detach().cpu().numpy()\n",
        "print(yhat_4.shape)\n",
        "\n",
        " ### Present ###\n",
        "yhat_5=yhat_4.reshape((yhat_4.shape[0]*w,6))\n",
        "test_y_r=test_y.reshape((test_y.shape[0]*w,6))\n",
        "\n",
        "print(yhat_4.shape)\n",
        "\n",
        "### Unpack ###\n",
        "yhat_up=unpack_dataset_present(np.array(yhat_5))\n",
        "test_y_up=unpack_dataset_present(np.array(test_y_r))\n",
        "\n",
        "print(yhat_up.shape,test_y_up.shape)\n",
        "\n",
        "### Present ###\n",
        "\n",
        "yhat_up=yhat_up.reshape(int(len(yhat_up)/6),6)\n",
        "test_y_up=test_y_up.reshape(int(len(test_y_up)/6),6)\n",
        "\n",
        "print(yhat_up.shape,test_y_up.shape)\n",
        "\n",
        "print(yhat_up.shape)\n",
        "\n",
        "\n",
        "### Present ###\n",
        "\n",
        "rmse,p= prediction_test(np.array(yhat_up),np.array(test_y_up))\n",
        "\n",
        "print(rmse[0])\n",
        "print(rmse[1])\n",
        "print(rmse[2])\n",
        "\n",
        "m=np.mean(rmse)\n",
        "\n",
        "print('\\n')\n",
        "print(m)\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "print(p[0])\n",
        "print(p[1])\n",
        "print(p[2])\n",
        "print('\\n')\n",
        "\n",
        "print(np.mean(p))\n",
        "\n",
        "ablation_3=np.hstack([rmse,p])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vaPjEUmsCYRY",
        "outputId": "dfa54503-87a4-4a3b-ee2b-ed93c08c2cd5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3495, 100, 6)\n",
            "(3495, 100, 6)\n",
            "(1048500,) (1048500,)\n",
            "(174750, 6) (174750, 6)\n",
            "(174750, 6)\n",
            "4.827186433819915\n",
            "4.675814397924784\n",
            "4.238344828960212\n",
            "\n",
            "\n",
            "4.580448553568304\n",
            "\n",
            "\n",
            "0.9718093795720301\n",
            "0.9817342736687817\n",
            "0.9371520588138704\n",
            "\n",
            "\n",
            "0.9635652373515607\n"
          ]
        }
      ]
    }
  ]
}